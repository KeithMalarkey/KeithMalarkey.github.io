<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Applied Multivariate Statistical Analysis(2)</title>
    <url>/2023/05/15/Applied-Multivariate-Statistical-Analysis-2/</url>
    <content><![CDATA[<h1 id="多元正态分布">多元正态分布</h1>
<h2 id="基本形式">基本形式</h2>
<ul>
<li>一元正态分布的密度函数：<span
class="math display">\[f(x)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})\]</span></li>
<li>多元正态分布的密度函数：<span
class="math display">\[f(\bf{x})=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}exp(-\frac{(\bf{x-\mu})^T\Sigma^{-1}(\bf{x-\mu})}{2})\]</span>
其中<span class="math inline">\(\bf\mu\)</span>为均值，<span
class="math inline">\(\bf\Sigma\)</span>为协方差矩阵;
对概率密度作等高线图（同条等高线上各处概率密度值相等），即<span
class="math display">\[(\bf{x-\mu})^T\Sigma^{-1}(\bf{x-\mu})=c^2\]</span>
其中<span class="math inline">\(\bf
c\)</span>为常数。这是个高维椭圆，中心在<span
class="math inline">\(\bf\mu\)</span>，拥有<span
class="math inline">\(p\)</span>个轴，第<span
class="math inline">\(i\)</span>个轴为<span class="math inline">\(\pm
c\sqrt{\lambda_i\textbf{e}_i}\)</span>，其中<span
class="math inline">\(\Sigma\textbf{e}_i=\lambda_i\textbf{e}_i\)</span>，即<span
class="math inline">\(\lambda_i\)</span>和<span
class="math inline">\(\textbf{e}_i\)</span>为<span
class="math inline">\(\Sigma\)</span>的特征值和特征向量。</li>
</ul>
<h2 id="线性变换">线性变换</h2>
<p><span class="math inline">\(X~\)</span></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>统计学习</category>
      </categories>
      <tags>
        <tag>统计分析</tag>
        <tag>数据降维</tag>
      </tags>
  </entry>
  <entry>
    <title>Applied Multivariate Statistical Analysis(1)</title>
    <url>/2023/05/13/Applied-Multivariate-Statistical-Analysis/</url>
    <content><![CDATA[<h1 id="前言">前言</h1>
<p>本贴参考书《Applied Multivariate Statistical Analysis, 4th ed.》 by
Richard A.Johnson and Dean
W.Wichern，主要是学习统计分析的基础，并且在数据预处理时常常会涉及到数据压缩和数据降维，在本贴中也会给出很多相关的方法。</p>
<h1 id="预备知识">预备知识</h1>
<h2 id="多元数据的组织">多元数据的组织</h2>
<p>多元数据的基本表示:n行p列的数据矩阵，每一行表示一个样本(sample)，每一列代表一个特征(feature)或属性(attribute)。即<span
class="math display">\[X=\begin{bmatrix}
    x_{11} &amp; x_{12} &amp; \dotsb &amp; x_{1p}\\
    x_{21} &amp; x_{22} &amp; \dotsb &amp; x_{2p}\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    x_{n1} &amp; x_{n2} &amp; \dotsb &amp; x_{np}
\end{bmatrix}=\begin{bmatrix}
    X_1^T\\
    \vdots\\
    X_n^T
\end{bmatrix}\]</span> 其中，不同样本之间相互独立。每个样本为<span
class="math inline">\(X_j=[x_{j1},x_{j2},\dotsb,x_{jp}]^T,j=\{1,2,\dotsb,n\}\)</span>；</p>
<h2 id="描述性统计量">描述性统计量</h2>
<h3 id="对于单个特征">对于单个特征：</h3>
<ul>
<li>样本均值： <span class="math display">\[\begin{equation}
  \bar{x}_k=\frac{1}{n}\sum\limits^n_{j=1} x_{jk},k=1,2,\dotsb,p
\end{equation}\]</span></li>
<li>样本方差：<span class="math display">\[\begin{equation}
  s_k^2=s_{kk}=\frac{1}{n}\sum\limits^n_{j=1} (x_{jk}-\bar{x}_k
)^2,k=1,2,\dotsb,p
\end{equation}\]</span></li>
</ul>
<h3 id="对于两个特征">对于两个特征：</h3>
<ul>
<li>样本协方差：<span class="math display">\[\begin{equation}
  s_{ik}=\frac{1}{n}\sum\limits^n_{j=1}
(x_{ji}-\bar{x}_i)(x_{jk}-\bar{x}_k),i,k=1,2,\dotsb,p
\end{equation}\]</span></li>
<li>样本相关系数：<span class="math display">\[\begin{equation}
  r_{ik}=\frac{s_{ik}}{\sqrt{s_{ii}}\sqrt{s_{kk}}}
\end{equation}\]</span>，其中<span
class="math inline">\(\sqrt{s_{ii}}\)</span>表示样本标准差；
协方差和相关系数刻画两个特征之间的的线性相关性。</li>
</ul>
<h3 id="对于数据集x的特征空间">对于数据集<span
class="math inline">\(X\)</span>的特征空间：</h3>
<ul>
<li>样本均值：<span
class="math inline">\(\bar{x}=\begin{bmatrix}  \bar{x}_1 \\ \vdots \\
\bar{x}_p \end{bmatrix}\)</span></li>
<li>样本协方差矩阵：<span
class="math inline">\(S_n=\begin{bmatrix}  s_{11} &amp; \dotsb &amp;
s_{1p}\\  \vdots &amp; \ddots &amp; \vdots\\  s_{p1} &amp; \dotsb &amp;
s_{pp} \end{bmatrix}=\mathbb{E}[(X-\bar{X}) (X-\bar{X})^T]\)</span></li>
<li>样本相关系数矩阵：<span class="math inline">\(R_n=\begin{bmatrix}  1
&amp; \dotsb &amp; r_{1p}\\  \vdots &amp; \ddots &amp; \vdots\\  r_{p1}
&amp; \dotsb &amp; 1 \end{bmatrix}\)</span></li>
</ul>
<p>且有结论：协方差矩阵和相关系数矩阵都对称且半正定。
从公式(1)可以看出，样本均值<span
class="math inline">\(\bar{x}\)</span>中的每个元素都是某个特征的均值，即<span
class="math inline">\(\bar{x}_i\)</span>表示的是第i列（第i个）特征的均值，所以可以将<span
class="math inline">\(\bar{x}\)</span>拓展为<span
class="math inline">\(\bar{X}\)</span>，且<span
class="math display">\[\bar{X}=\begin{bmatrix}
    \bar{x}_1 &amp; \bar{x}_2 &amp; \bar{x}_3 &amp; \dotsb &amp;
\bar{x}_p\\
    \bar{x}_1 &amp; \bar{x}_2 &amp; \bar{x}_3 &amp; \dotsb &amp;
\bar{x}_p\\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    \bar{x}_1 &amp; \bar{x}_2 &amp; \bar{x}_3 &amp; \dotsb &amp;
\bar{x}_p
\end{bmatrix}\]</span>，用以和数据集<span
class="math inline">\(X\)</span>进行比较，使得每列的特征和其均值进行直接比较。所以<span
class="math display">\[X-\bar{X}=\begin{bmatrix}
    x_{11}-\bar{x}_1 &amp; x_{12}-\bar{x}_2 &amp; \dotsb &amp;
x_{1p}-\bar{x}_p\\
    x_{21}-\bar{x}_1 &amp; x_{22}-\bar{x}_2 &amp; \dotsb &amp;
x_{2p}-\bar{x}_p\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    x_{n1}-\bar{x}_1 &amp; x_{n2}-\bar{x}_2 &amp; \dotsb &amp;
x_{np}-\bar{x}_p
\end{bmatrix}\]</span>，不难发现样本协方差矩阵<span
class="math inline">\(S_n=\mathbb{E}[(X-\bar{X})
(X-\bar{X})^T]\)</span>，其中<span
class="math inline">\(\mathbb{E}\)</span>表示数学期望。</p>
<h3
id="证明协方差矩阵和相关系数矩阵都对称且半正定">证明：协方差矩阵和相关系数矩阵都对称且半正定</h3>
<ol type="1">
<li>证明协方差矩阵对称且半正定：
由公式（3）知对于两个特征，其样本协方差为<span
class="math display">\[s_{ik}=\frac{1}{n}\sum\limits^n_{j=1}
(x_{ji}-\bar{x}_i)(x_{jk}-\bar{x}_k),i,k=1,2,\dotsb,p.\]</span>同样地,<span
class="math display">\[s_{ki}=\frac{1}{n}\sum\limits^n_{j=1}
(x_{ji}-\bar{x}_i)(x_{jk}-\bar{x}_k),i,k=1,2,\dotsb,p.\]</span>所以得到$s_{ik}=s_{ki}
$ 协方差矩阵对称；对于<span
class="math inline">\(S_n\)</span>的任意特征向量<span
class="math inline">\(\eta \in \mathcal{R^p}\)</span>和相应的特征值<span
class="math inline">\(\lambda\)</span>，即<span
class="math inline">\(S_n\eta=\lambda\eta(\eta \neq
0)\)</span>。又有<span class="math display">\[\begin{split}
\eta^T S_n \eta &amp;= \eta^T\mathbb{E}[(X-\bar{X}) (X-\bar{X})^T]\eta
\\
&amp; = \mathbb{E}[\eta^T (X-\bar{X}) (X-\bar{X})^T \eta]\\
&amp; \xlongequal{Q=(X-\bar{X})^T \eta
, Q \in \mathcal{R^p}} \mathbb{E}(Q^TQ) \geq 0.
\end{split}\]</span> 得证。</li>
<li>证相关系数矩阵对称且半正定：
这个就比较简单了。首先由公式(4)知，样本相关系数矩阵<span
class="math inline">\(R_n\)</span>是对称的，其次，由于<span
class="math inline">\(R_n=\begin{bmatrix}  1 &amp; \dotsb &amp;
r_{1p}\\  \vdots &amp; \ddots &amp; \vdots\\  r_{p1} &amp; \dotsb &amp;
1 \end{bmatrix}\)</span>中非对角元素<span class="math inline">\(r_{ij}
\leq
1\)</span>，所以任何一个主对角子式的行列式值都大于等于零，得证。</li>
</ol>
<h2 id="距离">距离</h2>
<h3 id="欧氏距离">欧氏距离</h3>
<p>设两点<span class="math inline">\(P、Q \in \mathcal{R^p},P=(x_1,x_2,
\dotsb, x_p)^T, Q=(y_1,y_2, \dotsb,
y_p)^T\)</span>,两点之间的距离为<span
class="math inline">\(d(P,Q)\)</span>;则<span
class="math display">\[\begin{split}
    d(P,Q)&amp;=\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+ \dotsb +(x_p-y_p)^2}\\
    &amp;=\sqrt{\sum\limits^p_{i=1} (x_i - y_i)^2}\\
    &amp;=\sqrt{P^TQ}
\end{split}\]</span></p>
<h3 id="标准化距离">标准化距离</h3>
<p>当p个变量全部相互独立，标准化距离：<span
class="math display">\[d(P,Q)=\sqrt{\sum\limits^p_{i=1}
\frac{(x_i-y_i)^2}{s_{ii}}}.\]</span></p>
<h3 id="统计距离">统计距离</h3>
<p>考虑到变差的不同以及相关性的存在，我们引入统计距离的概念。统计距离是多元分析的基础。比如，从原点<span
class="math inline">\(O(0,0)\)</span>到<span
class="math inline">\(P(x_1,x_2)\)</span>的统计距离可以由标准化坐标<span
class="math inline">\(x_1^*=\frac{x_1}{\sqrt{s_{11}}},x_2^*=\frac{x_2}{\sqrt{s_{22}}}\)</span>计算出来。<span
class="math display">\[\begin{split}
    d(O,P)&amp;=\sqrt{(x_1^*)^2+(x_2^*)^2}\\
    &amp;=\sqrt{(\frac{x_1}{\sqrt{s_{11}}})^2+(\frac{x_2}{\sqrt{s_{22}}})^2}\\
    &amp;=\sqrt{\frac{x_1^2}{s_{11}}+\frac{x_2^2}{s_{22}}}
\end{split}\]</span> 较之于欧氏距离，<span
class="math display">\[d(O,P)=\sqrt{x_1^2+x_2^2}\]</span>
我们可以发现增加了权重系数<span
class="math inline">\(k_1=1/s_{11}\)</span>和<span
class="math inline">\(k_2=1/s_{22}\)</span>。如果样本方差相同，即<span
class="math inline">\(k_1=k_2\)</span>，则<span
class="math inline">\(x_1^2\)</span>和<span
class="math inline">\(x_2^2\)</span>将得到相同的权重系数。当权重系数相同时，忽略公因子，使用普通的欧几里得距离仍然是适用的。<font color="red">换言之，如果<span
class="math inline">\(x_1\)</span>方向的变异性与<span
class="math inline">\(x_2\)</span>方向的相同，并且<span
class="math inline">\(x_1\)</span>值的变化不依赖于<span
class="math inline">\(x_2\)</span>的值，欧氏距离是适用的。</font></p>
<h3
id="最小化特征相关度最大化特征间的方差">最小化特征相关度/最大化特征间的方差</h3>
<p>当变量之间有相关性时，相当于先旋转坐标轴到变量之间相互独立的角度，再计算标准化距离。旋转的过程是二次型变换。<img data-src="/images/Applied%20Multivariate%20Statistical%20Analysis/p1.png"
alt="坐标基旋转" /> 从上图可以看出，原来的坐标系空间内，<span
class="math inline">\(x_1\)</span>和<span
class="math inline">\(x_2\)</span>方向的可变性（方差/协方差）不同，为了减少变量的相关性，在保持散布不变的基础上，将原始的坐标系旋转<span
class="math inline">\(\theta\)</span>角并标出旋转过的坐标轴<span
class="math inline">\(\tilde{x}_1\)</span>和<span
class="math inline">\(\tilde{x}_2\)</span>。原始坐标<span
class="math inline">\((x_1,x_2)\)</span>与旋转后所得坐标<span
class="math inline">\((\tilde{x}_1,\tilde{x}_2)\)</span>之间的关系如下<span
class="math display">\[\begin{gather*}
    \tilde{x}_1=x_1\cos(\theta) + x_2\sin(\theta) \\
    \tilde{x}_2=-x_1\sin(\theta) + x_2\cos(\theta)
\end{gather*}\]</span> 推导过程： 初始坐标<span
class="math inline">\((x_1,x_2)\)</span>的极坐标表示为 <span
class="math display">\[\begin{cases}
    x_1=r\cos(\alpha)\\
    x_2=r\sin(\alpha)
\end{cases}\]</span> 由于坐标轴逆时针旋转<span
class="math inline">\(\theta^\circ\)</span>，所以角度由初始的<span
class="math inline">\(\alpha\)</span>变为<span
class="math inline">\(\alpha -
\theta\)</span>,所以极坐标表示发生如下变化 <span
class="math display">\[\begin{cases}
    \tilde{x}_1=r\cos(\alpha - \theta)\\
    \tilde{x}_2=r\sin(\alpha - \theta)\\
\end{cases}\]</span> <span class="math display">\[
\Rightarrow
\begin{cases}
    \tilde{x}_1=r\cos(\alpha)\cos(\theta) + r\sin(\alpha)\sin(\theta)\\
    \tilde{x}_2=r\sin(\alpha)\cos(\theta)-r\cos(\alpha)\sin(\theta)\\
\end{cases}\]</span> <span class="math display">\[
\Rightarrow
\begin{cases}
    \tilde{x}_1=x_1\cos(\theta) + x_2\sin(\theta)\\
    \tilde{x}_2=-x_1\sin(\theta) + x_2\cos(\theta)
\end{cases}
\]</span></p>
<h3 id="马氏距离">马氏距离</h3>
<h4 id="定义">定义</h4>
<p>设<span class="math inline">\(x,y \in
\mathcal{R^n}\)</span>为样本空间中的两个样本点，<span
class="math inline">\(S\)</span>为<span
class="math inline">\(x,y\)</span>所服从分布的协方差矩阵，则马氏距离为
<span class="math display">\[\begin{gather*}
    d(x,y)=\sqrt{(x-y)^TS^{-1}(x-y)}\\
    d(x,0)=\sqrt{(x-\mu)^TS^{-1}(x-\mu)}
\end{gather*}\]</span> 计算欧氏距离和马氏距离的期望。
<font color="red">引理（lemma）</font>：若<span
class="math inline">\(X\)</span>为时间序列，有均值<span
class="math inline">\(\mu\)</span>和协方差矩阵<span
class="math inline">\(\Sigma\)</span>(即<span
class="math inline">\(\mathbb{E}(X)=\mu,\mathbb{E}[(X-\mu)(X-\mu)^T]=\Sigma\)</span>)，则<span
class="math display">\[\begin{equation}
\mathbb{E}(X^TAX)=tr(A\Sigma)+\mu^TA\mu
\end{equation}\]</span> <span
class="math inline">\(\bf{Proof}\)</span>:<span
class="math display">\[\begin{equation}\begin{split}
    X^TAX&amp;=(X-\mu)^TAX + \mu^TAX\\
    &amp;=(x-\mu)^TA(X-\mu)+\mu^TAX+(X-\mu)^TA\mu
\end{split}\end{equation}\]</span>
然后对公式(6)左右两侧同时取期望值，则有 <span
class="math display">\[\begin{equation}\begin{split}
    \mathbb{E}(X^TAX)&amp;=\mathbb{E}[(x-\mu)^TA(X-\mu)]+\mathbb{E}(\mu^TAX)+\mathbb{E}[(X-\mu)^TA\mu]\\
    &amp;=\mathbb{E}[(x-\mu)^TA(X-\mu)]+\mu^TA\mu+\mathbb{E}(X^TA\mu)-\mathbb{E}(\mu^TA\mu)\\
    &amp;=\mathbb{E}[(x-\mu)^TA(X-\mu)]+\mu^TA\mu+\mu^TA\mu-\mu^TA\mu\\
    &amp;=\mathbb{E}[(x-\mu)^TA(X-\mu)]+\mu^TA\mu
\end{split}\end{equation}\]</span> 此外， <span class="math display">\[
\begin{equation}
\begin{split}
    \mathbb{E}[(x-\mu)^TA(X-\mu)]&amp;=\mathbb{E}(Y^TAY)\\
    &amp;=\sum\limits^n_{i=1}
\sum\limits^n_{j=1}\mathbb{E}(Y_iA_{i,j}Y_j)=\sum\limits^n_{i=1}
\sum\limits^n_{j=1} A_{i,j}[Var(Y)]_{i,j}\\
    &amp;=\sum\limits^n_{i=1} \sum\limits^n_{j=1}
A_{i,j}[Var(X-\mu)]_{i,j}=\sum\limits^n_{i=1} \sum\limits^n_{j=1}
A_{i,j}[Var(X)]_{i,j}\\
    &amp;=\sum\limits^n_{i=1} \sum\limits^n_{j=1}
A_{i,j}\Sigma_{i,j}=\sum\limits^n_{i=1} \sum\limits^n_{j=1}
A_{i,j}\Sigma_{j,i}\\
    &amp;=\sum\limits^n_{i=1} [A\Sigma]_{i,i}=tr(A\Sigma)
\end{split}
\end{equation}
\]</span> 故，依据(6)、(7)和(8),易得到公式(5)。</p>
<h4 id="欧氏距离期望">欧氏距离期望</h4>
<p><span class="math display">\[
\begin{equation}
    \mathbb{E}[(X-Y)^TI(X-Y)]=2tr(\Sigma)=2\sum\limits^n_{i=1}\sigma_{ii}
\end{equation}
\]</span></p>
<h4 id="马氏距离期望">马氏距离期望</h4>
<p><span class="math display">\[
\begin{equation}
    \mathbb{E}[(X-Y)^T\Sigma^{-1}(X-Y)]=2tr(\Sigma^{-1}\Sigma)=2n
\end{equation}
\]</span></p>
<h2 id="马氏距离推导">马氏距离推导</h2>
<p>设原始数据为<span class="math inline">\(x \in
\mathcal{R^p}\)</span>，经过可逆线性变换后为数据<span
class="math inline">\(y\)</span>，且假设<span
class="math inline">\(x=\hat{P}\hat{\Lambda}y\)</span>,其中<span
class="math inline">\(\hat{P}\)</span>为正交阵，负责旋转，<span
class="math inline">\(\hat{\Lambda}\)</span>为对角阵，负责伸缩。目的是将变换后的数据变成各维度独立且尺度统一，即<span
class="math inline">\(Var(y)=I\)</span>，对y计算欧氏距离就是<span
class="math inline">\(x\)</span>计算马氏距离。其实，就是和上述讲到的最小化特征相关度再进行标准化一致。</p>
<ol type="1">
<li>原数据的协方差矩阵：<span
class="math display">\[\Sigma=Var(x)=Var(\hat{P}\hat{\Lambda}y)=\hat{P}\hat{\Lambda}Var(y)\hat{\Lambda}\hat{P}=\hat{P}\hat{\Lambda}\hat{P}^T\]</span></li>
<li>转换后数据的欧氏距离： <span
class="math display">\[y^Ty=(x^T\hat{P}\hat{\Lambda}^{-1})(\hat{\Lambda}^{-1}\hat{P}^Tx)=x^T\Sigma^{-1}x\]</span></li>
</ol>
<p>这样，就得到了马氏距离的表达式。</p>
<h2 id="矩阵相关不等式">矩阵相关不等式</h2>
<h3 id="柯西-施瓦茨不等式三角不等式">柯西-施瓦茨不等式(三角不等式)</h3>
<ul>
<li>已知<span class="math inline">\(b,d \in
\mathcal{R^p}\)</span>(两个样本) <span
class="math display">\[\begin{equation}
  (b^Td)^2 \leq (b^Tb)(d^Td)
\end{equation}\]</span> 等号成立当且仅当<span
class="math inline">\(b=cd\)</span>，其中c为常数。 <span
class="math inline">\(\bf{Proof}\)</span>: <span class="math display">\[
\begin{gather*}
  b^Td=|b| \cdot |d| \cdot \cos&lt;b,d&gt; \\
  (b^Td)^2=(|b| \cdot |d| \cdot \cos&lt;b,d&gt;)^2\\
  =|b|^2 \cdot |d|^2 \cdot \cos^2 &lt;b,d&gt;\\
  \leq (b^Tb)(d^Td), \cos&lt;b,d&gt; \in [0,1]
\end{gather*}
\]</span></li>
</ul>
<h3 id="扩展的柯西-施瓦茨不等式">扩展的柯西-施瓦茨不等式</h3>
<ul>
<li><span class="math inline">\(B\)</span>为正定矩阵<span
class="math display">\[(b^Td)^2 \leq
(b^TBb)(d^TB^{-1}d)\]</span>等号成立当且仅当<span
class="math inline">\(b=cB^{-1}d\)</span>，其中c为常数。</li>
</ul>
<h3 id="极大化引理">极大化引理</h3>
<ul>
<li><span class="math inline">\(B \in \mathcal{R^{p \times p}},d \in
\mathcal{R^p}\)</span>,<span
class="math inline">\(B\)</span>为正定矩阵，<span
class="math inline">\(d\)</span>为给定向量，则对任何非零向量<span
class="math inline">\(x\)</span>,有<span class="math display">\[
\mathop{max}\limits_{x \neq 0}
\frac{(x^Td)^2}{x^TBx}=d^TB^{-1}d\]</span> 其中最大值取在<span
class="math inline">\(x=cB^{-1}d\)</span>，其中c为常数。</li>
</ul>
<h3 id="单位球上二次型最大化瑞丽商">单位球上二次型最大化(瑞丽商)</h3>
<p><span class="math inline">\(B\)</span>为正定矩阵，特征值为<span
class="math inline">\(\lambda_1 \geq \lambda_2 \geq \dotsb \geq
\lambda_p \geq 0\)</span>，对应特征向量（单位化后）为<span
class="math inline">\(e_1,e_2,\dotsb,e_p\)</span>,则<span
class="math display">\[\begin{gather*}
    \mathop{max}\limits_{x \neq 0} \frac{x^TBx}{x^Tx} =
\lambda_1(x=e_1)\\
    \mathop{min}\limits_{x \neq 0} \frac{x^TBx}{x^Tx} =
\lambda_p(x=e_p)\\
    \mathop{max}\limits_{x \bot e_1,\dotsb,e_k} \frac{x^TBx}{x^Tx} =
\lambda_{k+1}(x=e_{k+1})\\
\end{gather*}\]</span></p>
<figure>
<img data-src="/images/Applied%20Multivariate%20Statistical%20Analysis/p2.png"
alt="超平面和向量投影" />
<figcaption aria-hidden="true">超平面和向量投影</figcaption>
</figure>
<h2 id="向量投影">向量投影</h2>
<p>设<span class="math inline">\(x,y \in
\mathcal{R^n}\)</span>,向量<span class="math inline">\(x\)</span>到<span
class="math inline">\(y\)</span>的投影：<span
class="math inline">\(\frac{x^Ty}{y^Ty}y\)</span>。证明可以看MIT的《Introduction
to linear algebra》。</p>
<h2 id="矩阵求导">矩阵求导</h2>
<p><span class="math inline">\(A\)</span>为矩阵，<span
class="math inline">\(x\)</span>为向量，则有 <span
class="math display">\[
\begin{gather*}
    \frac{\partial}{\partial x}(Ax)=A^T\\
    \frac{\partial}{\partial x}(x^TA)=A\\
    \frac{\partial}{\partial x}(x^Tx)=2x\\
    \frac{\partial}{\partial x}(x^TAx)=Ax+A^Tx
\end{gather*}
\]</span> <span class="math inline">\(A\)</span>为矩阵，<span
class="math inline">\(x\)</span>为方阵，则有<span
class="math display">\[
\begin{gather*}
    \frac{\partial |A|}{\partial A}=|A|A^{-1}\\
    \frac{\partial tr(AB)}{\partial A}=B^T\\
    \frac{\partial tr(A^{-1}B)}{\partial A}=-A^{-1}B^TA^{-1}
\end{gather*}\]</span></p>
<h2 id="统计量的更多运算">统计量的更多运算</h2>
<p><span class="math display">\[
\begin{gather*}
    \mathbb{E}(c^Tx)=x^T\mu\\
    Var(c^Tx)=c^T\Sigma c\\
    \mathbb{E}(CX)=C\mu_X\\
    Cov(CX)=C\Sigma_X C\\
    Cov(b^Tx,c^Tx)=b^TCov(x)c
\end{gather*}\]</span></p>
<h1 id="其他章节">其他章节</h1>
<p><a
href="https://keithmalarkey.github.io/2023/05/15/Applied-Multivariate-Statistical-Analysis-2/">Applied
Multivariate Statistical Analysis(2)</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>统计学习</category>
      </categories>
      <tags>
        <tag>统计分析</tag>
        <tag>数据降维</tag>
      </tags>
  </entry>
  <entry>
    <title>创建一个hexoBlog</title>
    <url>/2023/05/08/Create-a-hexoBlog/</url>
    <content><![CDATA[<h1 id="使用hexo框架创建个人博客">使用hexo框架创建个人博客</h1>
<p>创建个人博客，主要用于机器学习及其相关数学基础，包括线性降维、非线性降维（流形学习）、统计分析、凸分析、信息论基础等。除此之外，会做一些算法笔记，包括一些state-of-art
methods。也可能写一点和算法分析、操作系统、Linux、vim(neovim)的学术/技术贴。在此，本贴给出hexo博客的创建流程。</p>
<hr />
<h1 id="安装">安装</h1>
<h2 id="安装需求">安装需求</h2>
<ul>
<li>Node.js (version &gt;= 10.13)</li>
<li>Git</li>
<li>npm</li>
<li>github账户</li>
</ul>
<p>可以使用下面的cmd命令查看是否安装成功及版本 <figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line">node --version</span><br><span class="line">git --version</span><br><span class="line">npm --verison</span><br></pre></td></tr></table></figure> 或者</p>
<figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line">node -V</span><br><span class="line">git -V</span><br><span class="line">npm -V</span><br></pre></td></tr></table></figure>
<h2 id="安装hexo">安装hexo</h2>
<p>创建本地仓库/路径，用以编辑blog，比如 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /d</span><br><span class="line"><span class="built_in">mkdir</span> KeithBlog</span><br><span class="line"><span class="built_in">cd</span> KeithBlog/</span><br></pre></td></tr></table></figure> &gt;
其上的代码块中，KeithBlog即本地仓库所在文件夹，并且在下面的代码块执行需要翻墙或者配置国内镜像源（cnpm）(建议面向知乎或搜索引擎，比如<a
href="https://zhuanlan.zhihu.com/p/426952333">zhihu</a>)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<h2 id="初始化blog">初始化blog</h2>
<p>准备好环境之后，就可以初始化blog了，在本地仓下使用下面bash命令，初始化一个名为blog的目录结构，会生成基础的构建
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo init blog</span><br><span class="line"><span class="built_in">cd</span> blog</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure> &gt; hexo
s是关于hexo的命令，通常用来在本地对编辑的网页博客进行预览，可以参考<a
href="https://hexo.io/zh-cn/docs/commands">hexoCmds</a></p>
<h2 id="创建新的blog">创建新的blog</h2>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">hexo n [layout] <span class="tag">&lt;<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 如：hexo new &quot;keith&#x27;s first Blog&quot; --&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>layout表示文章布局，可以选择post/page/draft，需要在_config.yml配置文件中对default_layout的参数进行修改。</p>
</blockquote>
<h2 id="生成和部署blog">生成和部署blog</h2>
<p>使用下面的cmd就可以生成网页（当前还没有部署到github项目中）
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- clean清楚缓存文件和静态文件,g指的是generate,d指的是deploy --&gt;</span></span><br><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></p>
<h1
id="将blog项目部署到github以chic主题为例">将blog项目部署到github(以Chic主题为例)</h1>
<h2 id="为blog创建github的repo">为blog创建github的repo</h2>
<p><code>cd /KeithBlog/blog</code>进入博客的本地仓，输入<code>git init</code>初始化git的main分支，在github上创建一个新的repo,注意一定要是<code>username.github.io</code>,这里的<code>username</code>指的是github名称，当然也可以在bash环境下使用
<code>git config --global --list</code>
查看user.name确认<code>username</code>，当然还需要配置SSH密钥</p>
<h2 id="生成ssh密钥">生成SSH密钥</h2>
<p>使用<code>git config --global --list</code>
查看user.email确认<code>&lt;email&gt;</code>，之后在bash环境下使用<code>ssh-keygen -t rsa -C &lt;email&gt;</code>,然后一直回车，当出现
下图时，说明密钥已经生成 <img data-src="/images/Create-a-hexoBlog/p1.png"
alt="密钥" /> 之后以下bash代码 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/.ssh</span><br><span class="line">vi id_rsa.pub</span><br></pre></td></tr></table></figure>
将内容全部复制后，在键盘输入：q，退出vi编辑。之后，来到<a
href="https://github.com/">github</a>中,点击右上角的用户头像，选择Settings,然后选择左侧的<code>SSH and GPG keys</code>，选择New
SSH
key,随便取个title(比如key4blog)，将刚刚复制的公钥粘贴到下方的Key中，然后执行bash命令<code>ssh -T git@github.com</code>确认主机和github网站之间的ssh通信是否连接成功
<img data-src="/images/Create-a-hexoBlog/p2.png" alt="连接判断" /></p>
<h2
id="配置blog主题建议安装next主题">配置blog主题(建议安装Next主题)</h2>
<p><code>cd /KeithBlog/blog</code>回到本地仓,在<a
href="https://hexo.io/themes/"
class="uri">https://hexo.io/themes/</a>中选择心仪的主题，我选择了Chic,所以在bash中执行<code>git clone git@github.com:Siricee/hexo-theme-Chic.git themes/Chic</code>,打开到路径<code>themes\Chic</code>,编辑<code>_config.yml</code>配置文件（注意yml也是键值对，但是冒号之后需要有空格，并且由空格控制嵌套结构），更改或编辑基本信息（navname、nickname、description、links等），然后<code>cd /KeithBlog/blog</code>回到仓库地址，编辑<code>_config.yml</code>配置文件，主要包括#Site信息和url(一定要是<code>https://&lt;username&gt;.github.io</code>),并且将theme变成Chic，deploy编辑为
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@github.com:<span class="tag">&lt;<span class="name">username</span>&gt;</span>/<span class="tag">&lt;<span class="name">username</span>&gt;</span>.github.io.git</span><br><span class="line">  <span class="comment">&lt;!-- 注意branch不是旧版本的master --&gt;</span></span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure></p>
<p>之后<code>hexo g &amp;&amp; hexo d</code>并输入<code>https://&lt;username&gt;.github.io</code>查看是否部署成功</p>
<h2 id="生成tag和categories页面">生成tag和categories页面</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new page tag</span><br><span class="line">hexo new page category</span><br></pre></td></tr></table></figure>
<p>之后,执行bash指令<code>cd source/tag/</code>和<code>vi index.md</code>,编辑成
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">title: Tag</span><br><span class="line">layout: tag</span><br></pre></td></tr></table></figure>
同样地，执行bash命令<code>cd source/category/</code>和<code>vi index.md</code>,编辑成
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">title: Category</span><br><span class="line">layout: category</span><br></pre></td></tr></table></figure></p>
<h2 id="生成自定义theme的blog">生成自定义theme的blog</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意hexo d可以将本地仓上传到github repo中，和git remote add后push功能一致</span></span><br><span class="line">hexo cl</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
<h1
id="博客中图片无法同步以chic主题为例">博客中图片无法同步(以Chic主题为例)</h1>
<ol type="1">
<li>本地仓路径为<code>/d/KeithBlog/blog</code>(根目录)，在根目录使用<code>git clone https://github.com/CodeFalling/hexo-asset-image</code>下载hexo-asset-image;</li>
<li>在博客的（不是主题的）<code>_config.yml</code>中，将<code>post_asset_folder</code>设置为<code>true</code>;</li>
<li>来到<code>/blog/source/_posts/</code>下，建立当前博客(当前编辑的markdown文件名)同名的文件夹（folder），然后放置博客插图到该文件夹(folder)下,在<code>.md</code>格式下的图片地址为<code>./&lt;folder&gt;/&lt;picname&gt;</code>；
&gt;注意：如果你使用typora编辑markdown,一定要是<code>./&lt;folder&gt;/&lt;picname&gt;</code>的地址格式，而不是<code>/&lt;folder&gt;/&lt;picname&gt;</code>，具体可以参考<a
href="https://www.bilibili.com/video/BV1D7411U7Yk/?spm_id_from=333.880.my_history.page.click&amp;vd_source=03a1d15d9e2ed541e2d422d799de6c41">bilibili</a>这位up的教程,这主要是由于typora的默认地址格式设置导致，如果是vscode则没有该问题</li>
</ol>
<h1 id="更改页面字体以chic主题为例">更改页面字体(以Chic主题为例)</h1>
<p>初始状态的字体真的很丑，所以有必要换一下字体 1.
选择自己喜欢的字体（.ttf格式，我选择的是Arial来自JBmono）; 2.
在<code>themes\Chic(&lt;your-chosen-theme&gt;)\source\fonts\</code>下新建字体文件夹(costom-font)，导入上述自定义字体;
3.
来到<code>themes\Chic(&lt;your-chosen-theme&gt;)\source\css\</code>下，编辑font.styl风格文档，将<code>font-family</code>改为文件夹名<code>&lt;costom-font&gt;</code>，且将该字体设置为首选。
4. .styl具体如下：<img data-src="/images/Create-a-hexoBlog/p3.png"
alt="styl设置" /></p>
<h1 id="latex支持以chic主题为例">latex支持(以Chic主题为例)</h1>
<p><span
class="math inline">\(\LaTeX\)</span>中的公式编辑是十分重要的，而且在论文/笔记编辑中由于强大的排版优势，经常涉及到。但是hexo自带的markdown渲染不支持<span
class="math inline">\(\LaTeX\)</span>，所以如果使用的是next主题还需要很多额外配置（网上的教程也很多），幸运的是Chic主题不需要，可以参考<a
href="https://nathaniel.blog/tutorials/make-hexo-support-math-again/">latex编辑</a>，具体操作流程：
1. 在<code>Chic\_config.yml</code>配置文件末添加 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># plugin functions</span></span><br><span class="line"><span class="comment">## Mathjax: Math Formula Support</span></span><br><span class="line"><span class="comment">## https://www.mathjax.org</span></span><br><span class="line"><span class="attr">mathjax:</span></span><br><span class="line"><span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">import:</span> <span class="string">demand</span> <span class="comment"># global or demand</span></span><br><span class="line"><span class="comment">## global: all pages will load mathjax,this will degrade performance  and some grammers may be parsed wrong.</span></span><br><span class="line"><span class="comment">## demand: Recommend option,if your post need fomula, you can declare &#x27;mathjax: true&#x27; in Front-matter</span></span><br></pre></td></tr></table></figure> 2.
执行bash命令<code>npm uninstall hexo-renderer-marked --save</code>和<code>npm install hexo-renderer-kramed --save</code>，然后去到<code>&lt;your-project-dir&gt;/node_modules/hexo-renderer-kramed/lib/renderer.js</code>，将代码块
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">formatText</span>(<span class="params">text</span>) &#123;</span><br><span class="line">    <span class="comment">// Fit kramed&#x27;s rule: $$ + \1 + $$</span></span><br><span class="line">    <span class="keyword">return</span> text.<span class="title function_">replace</span>(<span class="regexp">/`\$(.*?)\$`/g</span>, <span class="string">&#x27;$$$$$1$$$$&#x27;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 修改为： <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">formatText</span>(<span class="params">text</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 3.
由于在1中我们将import设置为demand类型，所以在需要使用<span
class="math inline">\(\LaTeX\)</span>编辑的博客文章头部设定<code>mathjax: true</code>,比如此文章<img data-src="/images/Create-a-hexoBlog/p4.png"
alt="latex渲染" />，这就是我转Next主题的文章。</p>
<blockquote>
<p>注意，这样做只能满足单行latex编辑，所以还存在问题</p>
</blockquote>
<h1 id="next主题配置">Next主题配置</h1>
<p>相似于section 2.3-section 5，参考<a
href="https://blog.csdn.net/Bennnnnnn/article/details/128000842">Next主题配置</a>。至于<span
class="math inline">\(\LaTeX\)</span>配置，可以参考<a
href="https://blog.csdn.net/qq_52466006/article/details/126924064">hexo-next
latex配置</a>，但是很多帖子反应pandoc下载的版本过低，我这里直接给出下载pandoc的<a
href="https://github.com/jgm/pandoc/blob/main/INSTALL.md">pandoc
download</a>，可以依据自己的OS在cmd下载到系统，并且使用<code>pandoc --version</code>查看是否安装成功或版本，插图刷新的问题不同于Chic主题，需要在<code>blog\source\images</code>下创建对应博客名称的文件夹<code>&lt;folder&gt;</code>，在<code>&lt;folder&gt;</code>下放置图片，还有一个值得注意的是，在编辑博客的markdown文件时，地址格式为<code>/images/&lt;folder&gt;/&lt;pic&gt;.&lt;format&gt;</code>，可以利用<code>hexo s</code>在本地预览并同步修改刷新，查看是否成功。</p>
<h1 id="注意事项">注意事项</h1>
<ol type="1">
<li>在同步到github-page时，一定要注意branch是否正确（master/main是有区别的），否则很容易出现位同步刷新网页博客的问题;</li>
<li>hexo官网：<a href="https://hexo.io/zh-cn/docs/"
class="uri">https://hexo.io/zh-cn/docs/</a></li>
</ol>
<h1 id="参考文章">参考文章</h1>
<ul>
<li><a
href="https://blog.csdn.net/qq_52466006/article/details/126924064"
class="uri">https://blog.csdn.net/qq_52466006/article/details/126924064</a></li>
<li><a href="https://blog.csdn.net/Bennnnnnn/article/details/128000842"
class="uri">https://blog.csdn.net/Bennnnnnn/article/details/128000842</a></li>
<li><a href="https://www.issey.top/"
class="uri">https://www.issey.top/</a></li>
</ul>
]]></content>
      <categories>
        <category>Instruction</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Residual Networks for Image Recognition</title>
    <url>/2023/05/16/Deep-Residual-Networks-for-Image-Recognition/</url>
    <content><![CDATA[<h1 id="前言">前言</h1>
<p>残差网络(ResNet)已经成为了SOTA的方法，在图像识别、文本处理等领域有着举足轻重的地位。本贴以何凯明的<a
href="https://arxiv.org/pdf/1512.03385.pdf">Deep Residual Learning for
Image Recognition</a>作为开篇。<br />
主要目的：<br />
1. 什么是ResNet？其基本结构是什么？<br />
2. 为什么提出ResNet?或者说它的优势是什么？<br />
3. 为何会有其优势，从数理层面的解释是什么？<br />
4. 适用的场景是什么？它与卷积网的不同点是什么？</p>
<hr />
<h1 id="论文解析">论文解析</h1>
<h2 id="概要">概要</h2>
<p>一般而言，深度神经网络难以训练。ResNet给出了较之于传统深度网络更加deeper的结构，且由于给出了更加平和的优化地形，缓解了网络训练的压力。</p>
<h2 id="引言">引言</h2>
<p>深度网络有更好的图像分类性能。深度对于非平凡的图像识别任务是至关重要的因素。问题是：（通过训练所得）更好的网络是否就是简单地堆加更多的网络层呢？现实是，如果要对此作出回答，必须先要解决梯度弥散和梯度爆炸的问题。目前(2022/2023)，常见的为了处理梯度消失和梯度爆炸的常见手段是对网络参数进行weight
normlization,或者对间歇地在网络中增加normlization layers(BN, Batch
Normalization)。<br />
然而，在解决了梯度问题使得loss开始收敛之后，新的问题出现了：网络深度增加了，准确率一度饱和甚至出现了下降的现象。如下图所示：<img data-src="/images/Deep-Residual-Networks-for-Image-Recognition/p1.png"
alt="深层网络准确率饱和现象" />
从Fig.1可以看出，不是说层数越多，训练（测试）的错误率就越小，况且也没有出现过拟合。此外还发现，在一定的迭代之后出现明显的饱和现象。训练精度的降级意味着复杂的模型难以优化。
<font color="red">
设计思路：考虑一个可以在深层增加更多layers的浅层结构。存在一种设计，就是将深层的layers设计成恒等映射(identity
mapping)，这种设计(相比于只用浅层网络)应该不会产生更高的训练误差(training
error)。
</font>。换言之，浅层网络本就可以构造出比较好的特征空间，即便是在后面加了恒等映射，其性能也不会变得比浅层网络更差。<br />
<img data-src="/images/Deep-Residual-Networks-for-Image-Recognition/p2.png"
alt="Shortcut connection" /> <font color="red">Deep residual learning
framework</font>:假设我们希望得到的一个潜在映射为<span
class="math inline">\(\mathcal{H}(\bf
x)\)</span>,当然这也是整个框架希望得到的(设计思路那里所说的可以在深层增加更多layers的浅层结构，然后深层+浅层作为整个framework)。在深层layers中，目标学习映射将不再是<span
class="math inline">\(\mathcal{H}(\textbf{x})\)</span>，而是<span
class="math inline">\(\mathcal{F}(\textbf{x}) :=
\mathcal{H}(\textbf{x})-\textbf{x}\)</span>，并且对来自浅层网络的输入<span
class="math inline">\(\bf x\)</span>进行恒等映射之后，进行shortcut
connection，将深层网络学到的<span class="math inline">\(\mathcal{F}({\bf
x})\)</span>直接同恒等映射后的<span class="math inline">\(\bf
x\)</span>相加，我们从Fig.2中看到这样的流程。在下图中给出了残差网络的全局设计。
<img data-src="/images/Deep-Residual-Networks-for-Image-Recognition/p3.jpg"
alt="全局设计" /></p>
<h2 id="实验结果">实验结果</h2>
<p><img data-src="/images/Deep-Residual-Networks-for-Image-Recognition/p4.png"
alt="ImageNet上的残差结构" /> 从Tab.1看到，每个残差块(residual
block)以及残差块的数量都是超参数(回到了炼丹玄学)，所以这些都要在验证集上不断调试，我。。</p>
<p><img data-src="/images/Deep-Residual-Networks-for-Image-Recognition/p5.png"
alt="Resnet性能同plain CNN的对比" />
上图给出了ImageNet数据集上的对比，首先对比左右图，可以看到ResNet的性能优势；其次我们看到ResNet收敛的速度更快；最后，我们看得到两个错误率断崖式降低，原因是学习率的调整(<span
class="math inline">\(\times 0.1\)</span>)，所以这又是一个超参数。</p>
<h1 id="数理解释">数理解释</h1>
<ol type="1">
<li>假设plain CNN需要学得潜在的mapping是<span
class="math inline">\(\mathcal{G}(\theta)\)</span>(<span
class="math inline">\(\theta\)</span>是需要学到的网络参数)，它可以抽象成两个部分：(1)浅层部分的映射<span
class="math inline">\(g\)</span>;(2)深层部分的映射<span
class="math inline">\(f\)</span>，即<span
class="math inline">\(\mathcal{G}(\theta)=f(g(\theta))\)</span>。所以在参数优化时，实际上做的就是<span
class="math inline">\(\frac{\partial\mathcal{G}(\theta)}{\partial\theta}=\frac{\partial
f(g(\theta))}{\partial g(\theta)} \cdot \frac{\partial
g(\theta)}{\partial
\theta}\)</span>。所以，由于这样的多次迭代(上百甚至百万次)，容易出现梯度弥散(梯度消失)的现象；</li>
<li>同样的，ResNet需要学习的潜在mapping也是<span
class="math inline">\(\mathcal{G}(\theta)\)</span>(<span
class="math inline">\(\mathcal{G}(\theta)=f(g(\theta))+g(\theta)\)</span>)，但是由于residual
connection的原因，每次迭代训练参数时需要做的是<span
class="math inline">\(\underbrace{\frac{\partial f(g(\theta))}{\partial
g(\theta)} \cdot \frac{\partial g(\theta)}{\partial \theta}}_{(1)} +
\underbrace{\frac{\partial g(\theta)}{\partial
\theta}}_{(2)}\)</span>，在(1)中还会存在plain
CNN中情况，但是由于(2)的存在，经过多次迭代之后，完全不会出现梯度弥散的情形。所以这就是较之于CNN的优势所在。</li>
</ol>
<h1 id="总结">总结</h1>
<p>ResNet就是使用了shortcut(residual)
connection的CNN变种，其基本结构就是残差块(residual
block)和残差连接(residual
connection)。提出ResNet的背景就是深度网络可以有效地提高图像识别的性能，但是当到了一定的深度时，出行了性能饱和甚至降级，原因也不是饱和，况且由于深度网络由于庞大复杂的参数优化任务，难以优化，所以提出了残差网络用以解决深度网络的参数学习问题，有效的解决了梯度弥散问题。</p>
<h1 id="遗留问题">遗留问题</h1>
<ol type="1">
<li>ResNet有效地解决了深度网络参数优化过程中的梯度弥散现象，那么如何缓和/解决梯度爆炸呢？</li>
<li>由于ResNet在CNN的基础之上，多了残差结构，那么如何设计残差块和学习率衰变都是额外的超参数设定任务。</li>
<li>提到Residual，在机器学习中，比如Gradient
boosting中的residual和这里的resodual有什么区别？(这部分目前我还是小白，哎)。</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>图像识别</tag>
        <tag>ResNet</tag>
      </tags>
  </entry>
  <entry>
    <title>Information Theory</title>
    <url>/2023/05/18/Information-Theory/</url>
    <content><![CDATA[<h1 id="参考资料">参考资料</h1>
<ol type="1">
<li>CS 258(2020 Spring), Information Theory, Fan Cheng from ShangHai
Jiao Tong University;</li>
<li>Statistics 311(2021 Fall), Information Theory, John Duchi from
Stanford University.</li>
<li>Thomas Cover, Elements of Information theory, 2<span
class="math inline">\(^{nd}\)</span>.</li>
</ol>
<h1 id="introduction-theory-and-ai">Introduction Theory and AI</h1>
<h2 id="深度学习中的信息论概念">深度学习中的信息论概念</h2>
<ol type="1">
<li>普遍使用的交叉熵损失函数；</li>
<li>最大信息获取基础之上的决策树构建；</li>
<li>广泛应用在NLP和Speech中的维特比算法(Viterbi algorithm)；</li>
<li>广泛应用在机翻RNNs以及其他模型结构中的encoder-decoder(编码器)。</li>
</ol>
<h2 id="前备知识">前备知识</h2>
<ol type="1">
<li>概率论基础
<ol type="1">
<li>均匀分布，高斯分布</li>
<li>期望值，方差/协方差</li>
</ol></li>
<li>优化理论基础(凹凸函数):凸(凹)函数的最小(大)值</li>
<li>Reasoning in analysis
<ol type="1">
<li>微积分</li>
<li>矩阵论基础</li>
</ol></li>
</ol>
<h1 id="chapter-1--basic-concepts-in-information-theory">Chapter
1--Basic concepts in information theory</h1>
<p>在本章节，主要引入一些信息论中的基本概念，内容相对琐碎。主要包括香农熵，KL散度、互信息和相应的条件版本。在本贴中所有的<span
class="math inline">\(\log\)</span>都默认以<span
class="math inline">\(e\)</span>为底。在本节的开始，假设所有的分布离散的。</p>
<h2 id="definitions">Definitions</h2>
<p>依据香农的回忆，信息熵可以简单地理解为"uncertainty"(不确定性)。</p>
<h3 id="entropy">Entropy</h3>
<p>假设<span class="math inline">\(P\)</span>是有限集<span
class="math inline">\(\mathcal{X}\)</span>上的一个分布，<span
class="math inline">\(p\)</span>用以标记和<span
class="math inline">\(P\)</span>相关的概率质量函数。也就是说，如果随机变量<span
class="math inline">\(X\)</span>服从<span
class="math inline">\(P\)</span>分布，那么<span
class="math inline">\(P\{X=x\}=p(x)\)</span>。那么，<span
class="math inline">\(X\)</span>(或者<span
class="math inline">\(P\)</span>)的熵可以定义为：<span
class="math display">\[H(X):=-\sum\limits_{x \in \mathcal{X}} p(x)\log
p(x)\]</span>。 由于，对于所有的<span
class="math inline">\(x\)</span>都满足<span class="math inline">\(p(x)
\leq 1\)</span>，<span
class="math inline">\(H(X)\)</span>必然是负数。</p>
<blockquote>
<p>几个要点：<br />
1. <span class="math inline">\(0\log 0 \rightarrow 0.(x \rightarrow0,
x\log x \rightarrow 0)\)</span>;<br />
2. <span class="math inline">\(H(x)\)</span>仅仅依赖于/服从于<span
class="math inline">\(p(x)\)</span>，也可以将<span
class="math inline">\(H(X)\)</span>写成<span
class="math inline">\(H(p)\)</span>;<br />
3. <span class="math inline">\(H(x) \geq 0\)</span>;<br />
4. 当<span class="math inline">\(X\)</span>是数据集<span
class="math inline">\(\mathcal{X}\)</span>上的均匀分布，那么<span
class="math inline">\(H(X)=\log |\mathcal{X}|\)</span>;<br />
5. <span class="math inline">\(H_b(X)=\log_b a H_a(X)\)</span><br />
(1)若底数为2，则信息熵的单位为bits;<br />
(2)若底数为<span
class="math inline">\(e\)</span>,信息熵的单位为nats.</p>
</blockquote>
<h3 id="entropyexamples">Entropy:examples</h3>
<ul>
<li>Binary entropy function <span class="math inline">\(H(p)\)</span>
<span class="math display">\[Let \quad X=\begin{cases}
  1 \qquad with \quad probability \quad p,\\
  0 \qquad with \quad probability \quad 1-p
\end{cases}\]</span> <span class="math inline">\(\Rightarrow H(X)=-p\log
p - (1-p)\log (1-p)\)</span></li>
<li>Let <span class="math display">\[X=
\begin{cases}
  a \qquad with \quad prob. \quad \frac{1}{2}\\
  b \qquad with \quad prob. \quad \frac{1}{4}\\
  c \qquad with \quad prob. \quad \frac{1}{8}\\
  d \qquad with \quad prob. \quad \frac{1}{8}
\end{cases}
\]</span> <span class="math inline">\(\Rightarrow
H(X)=\frac{1}{2}\log2+\frac{1}{4}\log4+\frac{1}{8}\log8+\frac{1}{8}\log8=\log2+\frac{3}{4}\log2=\frac{7}{4}\log2\)</span><br />
</li>
<li>用<span class="math inline">\(E\)</span>标记期望。若<span
class="math inline">\(X \sim p(x)\)</span>,则随机变量的<span
class="math inline">\(g(X)\)</span>的期望值可以写作<span
class="math display">\[E_p g(X)=\sum_{x \in \mathcal{X}}
g(x)p(x)\]</span><br />
<span class="math inline">\(\Rightarrow
H(X)=E_p\log\frac{1}{p(X)}\)</span></li>
</ul>
<h3 id="hx的性质范围"><span
class="math inline">\(H(X)\)</span>的性质范围</h3>
<p>对于定义在数据集<span
class="math inline">\(\mathcal{X}\)</span>上的离散型随机变量<span
class="math inline">\(X\)</span>，满足：<span class="math display">\[0
\leq H(X) \leq \log |\mathcal{X}|\]</span><br />
证明：<br />
（1）由于<span class="math inline">\(p(x) \in [0,1]\)</span>，所以<span
class="math inline">\(H(X)=-E_p[\log p(x)] \geq 0\)</span>；
（2）给出证明<span class="math inline">\(H(X) \leq \log
|\mathcal{X}|\)</span>的两个方法：<br />
1. Jensen不等式<br />
<span class="math inline">\(f(x)=-x\log x, x \in (0,1].
f^{&#39;}(x)=-1-\log x, f^{&#39;&#39;}(x)=-\frac{1}{x} &lt;
0\)</span>。所以，由凸函数的判定条件，知道<span
class="math inline">\(f(x)\)</span>为<span
class="math inline">\((0,1]\)</span>上的凹函数。由Jensen不等式，有<span
class="math inline">\(f[E(X)] \ge E[f(X)]\)</span>，故 <span
class="math display">\[\frac{1}{|\mathcal{X}|}H(X)=E[f(X)] \le
-(E(X))\log (E(X))=-\frac{\sum\limits_\mathcal{X}p(x)}{|\mathcal{X}|}
\log
\frac{\sum\limits_{\mathcal{X}}p(x)}{|\mathcal{X}|}=\frac{1}{|\mathcal{X}|}\log
|\mathcal{X}|\]</span> 2. 函数最值</p>
]]></content>
      <categories>
        <category>信息论</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Information Theory</tag>
        <tag>信息熵</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Ordinary Differential Equation(NODE)</title>
    <url>/2023/05/16/Neural-Ordinary-Differential-Equation-NODE/</url>
    <content><![CDATA[
]]></content>
  </entry>
</search>
