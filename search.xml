<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Combinatorics</title>
    <url>/2023/06/24/Combinatorial-Mathematics/</url>
    <content><![CDATA[<h1 id="introduction">Introduction</h1>
<p>主要记录一些组合数学和离散数学的知识框架用以回顾,
参考书：（美）R.A.Brualdi. 组合数学（第五版）. 机械工业出版社，2012.</p>
<h1 id="什么是组合数学">什么是组合数学</h1>
<p>组合数学也称为组合学和组合分析，是一门研究离散对象的科学。随着计算机科学的日益发展，组合数学的重要性也日渐凸显，因为计算机科学的核心内容是使用算法处理离散数据。组合数学主要研究满足一定条件的组态（也称组合模型）的存在、计数以及构造等方面的问题。组合数学的主要内容有组合计数、组合设计、组合矩阵、组合优化（最佳组合）等。组合数学不仅在软件技术中有重要的应用价值，在企业管理，交通规划，试验设计，战争指挥，金融分析等领域都有重要的应用。</p>
<h2 id="切块问题">切块问题</h2>
<p>把一个<span class="math inline">\(3 \times 3 \times
3\)</span>的立方体木块切成27个<span class="math inline">\(1 \times 1
\times
1\)</span>的小立方体木块，如果切割过程中允许重新排列已切割木块的位置，求完成整个切割的最少次数。<br />
1. 测量实心长方体的对角线长度？<br />
2. 如果把上述问题中的立方体换成<span class="math inline">\(3 \times 6
\times 9\)</span>的长方体结果如何？</p>
<p>思路：对于<span class="math inline">\(1 \times 1 \times
5\)</span>的立方体，至少需要切<span class="math inline">\(\lceil \log_2
5 \rceil\)</span>；相似的，对于<span class="math inline">\(1 \times 3
\times 5\)</span>的立方体，至少需要切<span class="math inline">\(\lceil
\log_2 3 \rceil + \lceil \log_2 5 \rceil\)</span></p>
<p>解答：由上分析，对于任意<span class="math inline">\(w_1 \times w_2
\times w_3\)</span>的立方体，分割成单位立方体至少需要<span
class="math inline">\(\sum\limits_{i=1}^3 \lceil \log_2 w_i
\rceil\)</span>,故此时的最小切割代价为<span class="math inline">\(3
\times \lceil \log_2 3 \rceil = 6\)</span></p>
<h2 id="nim问题">Nim问题</h2>
<p>有五堆石子，各堆石子的个数分别是10、29和48。甲乙两人玩取石子游戏，要求：两个人轮流取石子，每个人取石子时，只能从同一堆中取，最少取一枚，最多可以把整堆石子取走。谁取到最后的石子，谁获胜。如果你是其中的一个游戏人，你选择先取子还是后取子，如何取？</p>
<p>思路：按位异或，非零，则先手必胜；否则，先手必输。</p>
<p>解答：10D=1010B,29D=11101B,48D=110000B,由<span
class="math inline">\(001010 \oplus 011101 \oplus 110000=100111 \neq
0\)</span>，所以先手必赢。</p>
<p>取子方案：<br />
<span class="math inline">\(110000(48D) \rightarrow
010111(23D)\)</span>,即从第三堆48块中取走25块，并保证每次自己取子之后，每堆石子数按位异或为0.，这样最终就会获胜。</p>
<h1 id="鸽笼原理">鸽笼原理</h1>
<h2 id="简单形式">简单形式</h2>
<p>若把n+1个物体放到n
（n≥1）个盒子中去，则至少有一个盒子放有至少2个物体。</p>
<p>证明：用反证法证明。如果n个盒子中每个盒子至多放入一个物体，则放入n个盒子中的物体总数至多为n个。这与假设有n+1个物体矛盾。从而定理得证。</p>
<h2 id="例题">例题</h2>
<p><img data-src="/images/Combinatorial-Mathematics/p1.jpg" alt="p1" /> <img data-src="/images/Combinatorial-Mathematics/p2.png" alt="p2" /> <img data-src="/images/Combinatorial-Mathematics/p3.png" alt="p3" /> <img data-src="/images/Combinatorial-Mathematics/p4.png" alt="p4-1" /> <img data-src="/images/Combinatorial-Mathematics/p4-1.png" alt="p4-2" /></p>
<h2 id="一般形式">一般形式</h2>
]]></content>
  </entry>
  <entry>
    <title>Fourier Series and Fourier Transform</title>
    <url>/2023/06/13/Fourier-Serie-and-Fourier-Transformation/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>Pattern Recognition and Machine Learning(1)</title>
    <url>/2023/06/01/PRML-1/</url>
    <content><![CDATA[<h1 id="前言">前言</h1>
<ol type="1">
<li>学习模式识别和机器学习的基础</li>
<li>贝叶斯学派日常学习记录</li>
<li>不断夯实基础，包括统计学习的相关概念、图网络和经典的生成模型等</li>
<li>面向以下几个优秀UP学习：
<ol type="1">
<li><a
href="https://space.bilibili.com/497536319?spm_id_from=333.999.0.0">徐芝兰</a></li>
<li><a
href="https://space.bilibili.com/6293151?spm_id_from=333.999.0.0">派派大星星</a></li>
<li><a
href="https://space.bilibili.com/552439878?spm_id_from=333.999.0.0">Poros踵</a></li>
</ol></li>
<li><a
href="https://space.bilibili.com/6293151?spm_id_from=333.999.0.0">UP派派大星星</a>
给出了相关书籍的<a
href="https://github.com/jiangyiqun233/PRML_learning">github-repo</a>,里面会有简略中文版书籍和公式推导</li>
<li><a
href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">英文原版书籍</a></li>
<li>因为一些推理或者proof不够完整，自己整理和修改了<a
href="https://github.com/jiangyiqun233/PRML_learning">github-repo</a>，之后会上传到github(尚未完成：TODO)
<!-- #TODO --></li>
</ol>
<h1 id="prerequisites">Prerequisites</h1>
<p>需要一些前备知识： 1. 概率论(够用)或随机过程(更好) 2.
线性代数/高等代数/矩阵论 3. 微积分基础</p>
<h1 id="频率学派和贝叶斯学派">频率学派和贝叶斯学派</h1>
<p>在学习大数定理时，在样本观测值已知情形下，我们可以确定该样本的似然函数，通过求最大似然确定未知参数。即，假设对独立同分布(i.i.d,
independent and identical distribution)的样本集<span
class="math inline">\(X_1,X_2,\dotsb,X_n\)</span>进行观测，其观测值依次为<span
class="math inline">\(x_1,x_2,\dotsb,x_n\)</span>，其样本的p.d.f(概率密度函数)为<span
class="math display">\[p(x_i|\omega)(i=1,2,\dots,n)\]</span>
则相应似然函数为<span
class="math inline">\(f(\omega)=\prod\limits_{i=1}^n
p(x_i|\omega)\)</span>,然后求得对参数<span
class="math inline">\(\omega\)</span>的极大参数估计<span
class="math inline">\(\hat{\omega}_{MLE}\)</span>，其推导如下： <span
class="math display">\[
\begin{equation*}
    \begin{split}
    \hat{\omega}_{MLE}&amp;= argmax \ln f(\omega)\\
    &amp;= argmax \sum\limits_{i=1}^{n} \ln p(x_i|\omega)\\
    &amp;= argmin - \sum\limits_{i=1}^{n} \ln p(x_i|\omega)\\
    \end{split}
\end{equation*}
\]</span> 参考<a
href="https://zhuanlan.zhihu.com/p/337637096">频率学派和贝叶斯学派</a>,<font color="purple">频率学派和贝叶斯学派的主要区别：是否允许先验概率分布的使用</font>。</p>
<ul>
<li>频率学派不假设任何的先验知识，不参照过去的经验，只按照当前已有的数据进行概率推断</li>
<li>贝叶斯学派会假设先验知识的存在，然后再用采样逐渐修改先验知识并逼近真实知识(一个不断更新先验知识的过程，也就是一个不断学习的过程)</li>
</ul>
<p>因此贝叶斯推论中前一次得到的后验概率分布可以作为后一次的先验概率。但实际上，在数据量趋近无穷时，频率学派和贝叶斯学派得到的结果是一样的，也就是说频率方法是贝叶斯方法的极限。当考虑的试验次数非常少的时候，贝叶斯方法的解释非常有用。此外，贝叶斯理论将我们对于随机过程的先验知识纳入考虑，当我们获得的数据越来越多的时候，这个先验的概率分布就会被更新到后验分布中。</p>
<h2 id="贝叶斯公式">贝叶斯公式</h2>
<p><span class="math display">\[
\begin{equation}
\begin{split}
    p(\theta | x)&amp;= \frac{p(x|\theta) \cdot \pi(\theta)}{p(x)}\\
    &amp;=\frac{p(x|\theta) \cdot \pi(\theta)}{\sum\limits_{\Theta}
p(x|\theta) \cdot \pi(\theta)}\\
    \text{或} &amp;=\frac{p(x|\theta) \cdot \pi(\theta)}{\int_{\Theta}
p(x|\theta) \cdot \pi(\theta)d\theta}
\end{split}
\end{equation}
\]</span> 其中，<span
class="math inline">\(\pi(\theta)\)</span>称为先验概率(the prior)，<span
class="math inline">\(p(x|\theta)\)</span>称为似然函数(likelihood)，<span
class="math inline">\(p(x)\)</span>称为边缘分布(margin
distribution)，<span class="math inline">\(p(\theta |
x)\)</span>称为后验分布(the
posterior)。事实上，分母可以看成是对后验分布的正则化或归一化</p>
<h2 id="先验概率分布">先验概率分布</h2>
<blockquote>
<p><a
href="https://en.wikipedia.org/wiki/Prior_probability">维基百科</a>:A
prior probability distribution of an uncertain quantity, often simply
called the prior, is its assumed probability distribution before some
evidence is taken into account. For example, the prior could be the
probability distribution representing the relative proportions of voters
who will vote for a particular politician in a future election. The
unknown quantity may be a parameter of the model or a latent variable
rather than an observable variable.
不确定量的先验概率分布，通常简称为先验，是在考虑某些证据之前假定的概率分布。例如，先验可以是代表在未来选举中投票给特定政治家的选民的相对比例的概率分布。未知量可能是模型的参数或潜在变量，而不是可观察变量。</p>
</blockquote>
<h2 id="共轭分布">共轭分布</h2>
<p>在贝叶斯统计中，如果先验分布和后验分布属于同类（分布形式相同），则先验分布与后验分布被称为<font color="red">共轭分布</font>，而先验分布被称为似然函数的<font color="red">共轭先验</font>。如果我们需要验证共轭分布，由于<span
class="math inline">\(posterior \propto likelihood \times
prior\)</span>，所以带入似然函数和先验分布子式之后，若后验分布和先验分布的形式相同，那么就说明两者是共轭分布。</p>
<h2 id="共轭分布举例">共轭分布举例</h2>
<ol type="1">
<li>二项分布 <span class="math display">\[Bin(m|N,\mu)=\binom{N}{m}\mu^m
(1-\mu)^{N-m} \label{eq3}\]</span></li>
<li>Beta分布 <span
class="math display">\[Bata(\mu|a,b)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}
\mu^{a-1}(1-\mu)^{b-1}(a,b&gt;0,\mu \in (0,1)) \label{eq4}\]</span>
其中，<span class="math inline">\(\Gamma(x) \equiv \int_0^\infty
u^{x-1}e^{-u}du(x&gt;0)\)</span></li>
<li>由分布<span class="math inline">\(\ref{eq3}\)</span> <span
class="math inline">\(\times\)</span> 分布 <span
class="math inline">\(\ref{eq4}\)</span>，归一化之后得到： <span
class="math display">\[p(\mu|m,l,a,b) \propto
\mu^{m+a-1}(1-\mu)^{l+b-1}\]</span>，其中<span
class="math inline">\(l=N-m\)</span>。和先验分布的形式相同。所以说,二项分布和Beta分布互为共轭先验，分布<span
class="math inline">\(p(\mu|m,l,a,b)\)</span>和二项分布以及beta分布都是共轭分布</li>
</ol>
<h1 id="概率分布">概率分布</h1>
<h2 id="二元分布0-1分布binary-distribution">二元分布/0-1分布(Binary
distribution)</h2>
<p><span
class="math display">\[Bern(x|\mu)=\mu^x(1-\mu)^{1-x}\]</span>，可以看到这是特殊的伯努利分布</p>
<h2 id="二项分布binomial-distribution">二项分布(Binomial
distribution)</h2>
<p><span class="math display">\[Bin(m|N, \mu) = \binom{N}{m}\mu^m(1 -
\mu)^{N - m}\]</span></p>
<h2 id="beta分布">Beta分布</h2>
<p><span class="math display">\[Beta(\mu|a, b) = \frac{\Gamma(a +
b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}(a,b &gt; 0, \mu \in
(0,1))\]</span>，其中<span class="math inline">\(\Gamma(x) \equiv
\int_0^\infty u^{x-1}e^{-u}du (x &gt; 0)\)</span></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>贝叶斯学习</category>
        <category>统计学习</category>
      </categories>
      <tags>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title>Principal component analysis</title>
    <url>/2023/05/30/Principal-component-analysis/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>Applied Multivariate Statistical Analysis(2)</title>
    <url>/2023/05/15/Applied-Multivariate-Statistical-Analysis-2/</url>
    <content><![CDATA[<h1 id="多元正态分布">多元正态分布</h1>
<h2 id="基本形式">基本形式</h2>
<ul>
<li>一元正态分布的密度函数：<span
class="math display">\[f(x)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})\]</span></li>
<li>多元正态分布的密度函数：<span
class="math display">\[f(\bf{x})=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}exp(-\frac{(\bf{x-\mu})^T\Sigma^{-1}(\bf{x-\mu})}{2})\]</span>
其中<span class="math inline">\(\bf\mu\)</span>为均值，<span
class="math inline">\(\bf\Sigma\)</span>为协方差矩阵;
对概率密度作等高线图（同条等高线上各处概率密度值相等），即<span
class="math display">\[(\bf{x-\mu})^T\Sigma^{-1}(\bf{x-\mu})=c^2\]</span>
其中<span class="math inline">\(\bf
c\)</span>为常数。这是个高维椭圆，中心在<span
class="math inline">\(\bf\mu\)</span>，拥有<span
class="math inline">\(p\)</span>个轴，第<span
class="math inline">\(i\)</span>个轴为<span class="math inline">\(\pm
c\sqrt{\lambda_i\textbf{e}_i}\)</span>，其中<span
class="math inline">\(\Sigma\textbf{e}_i=\lambda_i\textbf{e}_i\)</span>，即<span
class="math inline">\(\lambda_i\)</span>和<span
class="math inline">\(\textbf{e}_i\)</span>为<span
class="math inline">\(\Sigma\)</span>的特征值和特征向量。</li>
</ul>
<h2 id="线性变换">线性变换</h2>
<p><span class="math inline">\(X~\)</span></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>统计学习</category>
      </categories>
      <tags>
        <tag>统计分析</tag>
        <tag>数据降维</tag>
      </tags>
  </entry>
  <entry>
    <title>Applied Multivariate Statistical Analysis(1)</title>
    <url>/2023/05/13/Applied-Multivariate-Statistical-Analysis/</url>
    <content><![CDATA[<h1 id="前言">前言</h1>
<p>本贴参考书《Applied Multivariate Statistical Analysis, 4th ed.》 by
Richard A.Johnson and Dean
W.Wichern，主要是学习统计分析的基础，并且在数据预处理时常常会涉及到数据压缩和数据降维，在本贴中也会给出很多相关的方法。</p>
<h1 id="预备知识">预备知识</h1>
<h2 id="多元数据的组织">多元数据的组织</h2>
<p>多元数据的基本表示:n行p列的数据矩阵，每一行表示一个样本(sample)，每一列代表一个特征(feature)或属性(attribute)。即<span
class="math display">\[X=\begin{bmatrix}
    x_{11} &amp; x_{12} &amp; \dotsb &amp; x_{1p}\\
    x_{21} &amp; x_{22} &amp; \dotsb &amp; x_{2p}\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    x_{n1} &amp; x_{n2} &amp; \dotsb &amp; x_{np}
\end{bmatrix}=\begin{bmatrix}
    X_1^T\\
    \vdots\\
    X_n^T
\end{bmatrix}\]</span> 其中，不同样本之间相互独立。每个样本为<span
class="math inline">\(X_j=[x_{j1},x_{j2},\dotsb,x_{jp}]^T,j=\{1,2,\dotsb,n\}\)</span>；</p>
<h2 id="描述性统计量">描述性统计量</h2>
<h3 id="对于单个特征">对于单个特征：</h3>
<ul>
<li>样本均值： <span class="math display">\[\begin{equation}
  \bar{x}_k=\frac{1}{n}\sum\limits^n_{j=1} x_{jk},k=1,2,\dotsb,p
\end{equation}\]</span></li>
<li>样本方差：<span class="math display">\[\begin{equation}
  s_k^2=s_{kk}=\frac{1}{n}\sum\limits^n_{j=1} (x_{jk}-\bar{x}_k
)^2,k=1,2,\dotsb,p
\end{equation}\]</span></li>
</ul>
<h3 id="对于两个特征">对于两个特征：</h3>
<ul>
<li>样本协方差：<span class="math display">\[\begin{equation}
  s_{ik}=\frac{1}{n}\sum\limits^n_{j=1}
(x_{ji}-\bar{x}_i)(x_{jk}-\bar{x}_k),i,k=1,2,\dotsb,p
\end{equation}\]</span></li>
<li>样本相关系数：<span class="math display">\[\begin{equation}
  r_{ik}=\frac{s_{ik}}{\sqrt{s_{ii}}\sqrt{s_{kk}}}
\end{equation}\]</span>，其中<span
class="math inline">\(\sqrt{s_{ii}}\)</span>表示样本标准差；
协方差和相关系数刻画两个特征之间的的线性相关性。</li>
</ul>
<h3 id="对于数据集x的特征空间">对于数据集<span
class="math inline">\(X\)</span>的特征空间：</h3>
<ul>
<li>样本均值：<span
class="math inline">\(\bar{x}=\begin{bmatrix}  \bar{x}_1 \\ \vdots \\
\bar{x}_p \end{bmatrix}\)</span></li>
<li>样本协方差矩阵：<span
class="math inline">\(S_n=\begin{bmatrix}  s_{11} &amp; \dotsb &amp;
s_{1p}\\  \vdots &amp; \ddots &amp; \vdots\\  s_{p1} &amp; \dotsb &amp;
s_{pp} \end{bmatrix}=\mathbb{E}[(X-\bar{X}) (X-\bar{X})^T]\)</span></li>
<li>样本相关系数矩阵：<span class="math inline">\(R_n=\begin{bmatrix}  1
&amp; \dotsb &amp; r_{1p}\\  \vdots &amp; \ddots &amp; \vdots\\  r_{p1}
&amp; \dotsb &amp; 1 \end{bmatrix}\)</span></li>
</ul>
<p>且有结论：协方差矩阵和相关系数矩阵都对称且半正定。
从公式(1)可以看出，样本均值<span
class="math inline">\(\bar{x}\)</span>中的每个元素都是某个特征的均值，即<span
class="math inline">\(\bar{x}_i\)</span>表示的是第i列（第i个）特征的均值，所以可以将<span
class="math inline">\(\bar{x}\)</span>拓展为<span
class="math inline">\(\bar{X}\)</span>，且<span
class="math display">\[\bar{X}=\begin{bmatrix}
    \bar{x}_1 &amp; \bar{x}_2 &amp; \bar{x}_3 &amp; \dotsb &amp;
\bar{x}_p\\
    \bar{x}_1 &amp; \bar{x}_2 &amp; \bar{x}_3 &amp; \dotsb &amp;
\bar{x}_p\\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    \bar{x}_1 &amp; \bar{x}_2 &amp; \bar{x}_3 &amp; \dotsb &amp;
\bar{x}_p
\end{bmatrix}\]</span>，用以和数据集<span
class="math inline">\(X\)</span>进行比较，使得每列的特征和其均值进行直接比较。所以<span
class="math display">\[X-\bar{X}=\begin{bmatrix}
    x_{11}-\bar{x}_1 &amp; x_{12}-\bar{x}_2 &amp; \dotsb &amp;
x_{1p}-\bar{x}_p\\
    x_{21}-\bar{x}_1 &amp; x_{22}-\bar{x}_2 &amp; \dotsb &amp;
x_{2p}-\bar{x}_p\\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    x_{n1}-\bar{x}_1 &amp; x_{n2}-\bar{x}_2 &amp; \dotsb &amp;
x_{np}-\bar{x}_p
\end{bmatrix}\]</span>，不难发现样本协方差矩阵<span
class="math inline">\(S_n=\mathbb{E}[(X-\bar{X})
(X-\bar{X})^T]\)</span>，其中<span
class="math inline">\(\mathbb{E}\)</span>表示数学期望。</p>
<h3
id="证明协方差矩阵和相关系数矩阵都对称且半正定">证明：协方差矩阵和相关系数矩阵都对称且半正定</h3>
<ol type="1">
<li>证明协方差矩阵对称且半正定：
由公式（3）知对于两个特征，其样本协方差为<span
class="math display">\[s_{ik}=\frac{1}{n}\sum\limits^n_{j=1}
(x_{ji}-\bar{x}_i)(x_{jk}-\bar{x}_k),i,k=1,2,\dotsb,p.\]</span>同样地,<span
class="math display">\[s_{ki}=\frac{1}{n}\sum\limits^n_{j=1}
(x_{ji}-\bar{x}_i)(x_{jk}-\bar{x}_k),i,k=1,2,\dotsb,p.\]</span>所以得到$s_{ik}=s_{ki}
$ 协方差矩阵对称；对于<span
class="math inline">\(S_n\)</span>的任意特征向量<span
class="math inline">\(\eta \in \mathcal{R^p}\)</span>和相应的特征值<span
class="math inline">\(\lambda\)</span>，即<span
class="math inline">\(S_n\eta=\lambda\eta(\eta \neq
0)\)</span>。又有<span class="math display">\[\begin{split}
\eta^T S_n \eta &amp;= \eta^T\mathbb{E}[(X-\bar{X}) (X-\bar{X})^T]\eta
\\
&amp; = \mathbb{E}[\eta^T (X-\bar{X}) (X-\bar{X})^T \eta]\\
&amp; \xlongequal{Q=(X-\bar{X})^T \eta
, Q \in \mathcal{R^p}} \mathbb{E}(Q^TQ) \geq 0.
\end{split}\]</span> 得证。</li>
<li>证相关系数矩阵对称且半正定：
这个就比较简单了。首先由公式(4)知，样本相关系数矩阵<span
class="math inline">\(R_n\)</span>是对称的，其次，由于<span
class="math inline">\(R_n=\begin{bmatrix}  1 &amp; \dotsb &amp;
r_{1p}\\  \vdots &amp; \ddots &amp; \vdots\\  r_{p1} &amp; \dotsb &amp;
1 \end{bmatrix}\)</span>中非对角元素<span class="math inline">\(r_{ij}
\leq
1\)</span>，所以任何一个主对角子式的行列式值都大于等于零，得证。</li>
</ol>
<h2 id="距离">距离</h2>
<h3 id="欧氏距离">欧氏距离</h3>
<p>设两点<span class="math inline">\(P、Q \in \mathcal{R^p},P=(x_1,x_2,
\dotsb, x_p)^T, Q=(y_1,y_2, \dotsb,
y_p)^T\)</span>,两点之间的距离为<span
class="math inline">\(d(P,Q)\)</span>;则<span
class="math display">\[\begin{split}
    d(P,Q)&amp;=\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+ \dotsb +(x_p-y_p)^2}\\
    &amp;=\sqrt{\sum\limits^p_{i=1} (x_i - y_i)^2}\\
    &amp;=\sqrt{P^TQ}
\end{split}\]</span></p>
<h3 id="标准化距离">标准化距离</h3>
<p>当p个变量全部相互独立，标准化距离：<span
class="math display">\[d(P,Q)=\sqrt{\sum\limits^p_{i=1}
\frac{(x_i-y_i)^2}{s_{ii}}}.\]</span></p>
<h3 id="统计距离">统计距离</h3>
<p>考虑到变差的不同以及相关性的存在，我们引入统计距离的概念。统计距离是多元分析的基础。比如，从原点<span
class="math inline">\(O(0,0)\)</span>到<span
class="math inline">\(P(x_1,x_2)\)</span>的统计距离可以由标准化坐标<span
class="math inline">\(x_1^*=\frac{x_1}{\sqrt{s_{11}}},x_2^*=\frac{x_2}{\sqrt{s_{22}}}\)</span>计算出来。<span
class="math display">\[\begin{split}
    d(O,P)&amp;=\sqrt{(x_1^*)^2+(x_2^*)^2}\\
    &amp;=\sqrt{(\frac{x_1}{\sqrt{s_{11}}})^2+(\frac{x_2}{\sqrt{s_{22}}})^2}\\
    &amp;=\sqrt{\frac{x_1^2}{s_{11}}+\frac{x_2^2}{s_{22}}}
\end{split}\]</span> 较之于欧氏距离，<span
class="math display">\[d(O,P)=\sqrt{x_1^2+x_2^2}\]</span>
我们可以发现增加了权重系数<span
class="math inline">\(k_1=1/s_{11}\)</span>和<span
class="math inline">\(k_2=1/s_{22}\)</span>。如果样本方差相同，即<span
class="math inline">\(k_1=k_2\)</span>，则<span
class="math inline">\(x_1^2\)</span>和<span
class="math inline">\(x_2^2\)</span>将得到相同的权重系数。当权重系数相同时，忽略公因子，使用普通的欧几里得距离仍然是适用的。<font color="red">换言之，如果<span
class="math inline">\(x_1\)</span>方向的变异性与<span
class="math inline">\(x_2\)</span>方向的相同，并且<span
class="math inline">\(x_1\)</span>值的变化不依赖于<span
class="math inline">\(x_2\)</span>的值，欧氏距离是适用的。</font></p>
<h3
id="最小化特征相关度最大化特征间的方差">最小化特征相关度/最大化特征间的方差</h3>
<p>当变量之间有相关性时，相当于先旋转坐标轴到变量之间相互独立的角度，再计算标准化距离。旋转的过程是二次型变换。<img data-src="/images/Applied%20Multivariate%20Statistical%20Analysis/p1.png"
alt="坐标基旋转" /> 从上图可以看出，原来的坐标系空间内，<span
class="math inline">\(x_1\)</span>和<span
class="math inline">\(x_2\)</span>方向的可变性（方差/协方差）不同，为了减少变量的相关性，在保持散布不变的基础上，将原始的坐标系旋转<span
class="math inline">\(\theta\)</span>角并标出旋转过的坐标轴<span
class="math inline">\(\tilde{x}_1\)</span>和<span
class="math inline">\(\tilde{x}_2\)</span>。原始坐标<span
class="math inline">\((x_1,x_2)\)</span>与旋转后所得坐标<span
class="math inline">\((\tilde{x}_1,\tilde{x}_2)\)</span>之间的关系如下<span
class="math display">\[\begin{gather*}
    \tilde{x}_1=x_1\cos(\theta) + x_2\sin(\theta) \\
    \tilde{x}_2=-x_1\sin(\theta) + x_2\cos(\theta)
\end{gather*}\]</span> 推导过程： 初始坐标<span
class="math inline">\((x_1,x_2)\)</span>的极坐标表示为 <span
class="math display">\[\begin{cases}
    x_1=r\cos(\alpha)\\
    x_2=r\sin(\alpha)
\end{cases}\]</span> 由于坐标轴逆时针旋转<span
class="math inline">\(\theta^\circ\)</span>，所以角度由初始的<span
class="math inline">\(\alpha\)</span>变为<span
class="math inline">\(\alpha -
\theta\)</span>,所以极坐标表示发生如下变化 <span
class="math display">\[\begin{cases}
    \tilde{x}_1=r\cos(\alpha - \theta)\\
    \tilde{x}_2=r\sin(\alpha - \theta)\\
\end{cases}\]</span> <span class="math display">\[
\Rightarrow
\begin{cases}
    \tilde{x}_1=r\cos(\alpha)\cos(\theta) + r\sin(\alpha)\sin(\theta)\\
    \tilde{x}_2=r\sin(\alpha)\cos(\theta)-r\cos(\alpha)\sin(\theta)\\
\end{cases}\]</span> <span class="math display">\[
\Rightarrow
\begin{cases}
    \tilde{x}_1=x_1\cos(\theta) + x_2\sin(\theta)\\
    \tilde{x}_2=-x_1\sin(\theta) + x_2\cos(\theta)
\end{cases}
\]</span></p>
<h3 id="马氏距离">马氏距离</h3>
<h4 id="定义">定义</h4>
<p>设<span class="math inline">\(x,y \in
\mathcal{R^n}\)</span>为样本空间中的两个样本点，<span
class="math inline">\(S\)</span>为<span
class="math inline">\(x,y\)</span>所服从分布的协方差矩阵，则马氏距离为
<span class="math display">\[\begin{gather*}
    d(x,y)=\sqrt{(x-y)^TS^{-1}(x-y)}\\
    d(x,0)=\sqrt{(x-\mu)^TS^{-1}(x-\mu)}
\end{gather*}\]</span> 计算欧氏距离和马氏距离的期望。
<font color="red">引理（lemma）</font>：若<span
class="math inline">\(X\)</span>为时间序列，有均值<span
class="math inline">\(\mu\)</span>和协方差矩阵<span
class="math inline">\(\Sigma\)</span>(即<span
class="math inline">\(\mathbb{E}(X)=\mu,\mathbb{E}[(X-\mu)(X-\mu)^T]=\Sigma\)</span>)，则<span
class="math display">\[\begin{equation}
\mathbb{E}(X^TAX)=tr(A\Sigma)+\mu^TA\mu
\end{equation}\]</span> <span
class="math inline">\(\bf{Proof}\)</span>:<span
class="math display">\[\begin{equation}\begin{split}
    X^TAX&amp;=(X-\mu)^TAX + \mu^TAX\\
    &amp;=(x-\mu)^TA(X-\mu)+\mu^TAX+(X-\mu)^TA\mu
\end{split}\end{equation}\]</span>
然后对公式(6)左右两侧同时取期望值，则有 <span
class="math display">\[\begin{equation}\begin{split}
    \mathbb{E}(X^TAX)&amp;=\mathbb{E}[(x-\mu)^TA(X-\mu)]+\mathbb{E}(\mu^TAX)+\mathbb{E}[(X-\mu)^TA\mu]\\
    &amp;=\mathbb{E}[(x-\mu)^TA(X-\mu)]+\mu^TA\mu+\mathbb{E}(X^TA\mu)-\mathbb{E}(\mu^TA\mu)\\
    &amp;=\mathbb{E}[(x-\mu)^TA(X-\mu)]+\mu^TA\mu+\mu^TA\mu-\mu^TA\mu\\
    &amp;=\mathbb{E}[(x-\mu)^TA(X-\mu)]+\mu^TA\mu
\end{split}\end{equation}\]</span> 此外， <span class="math display">\[
\begin{equation}
\begin{split}
    \mathbb{E}[(x-\mu)^TA(X-\mu)]&amp;=\mathbb{E}(Y^TAY)\\
    &amp;=\sum\limits^n_{i=1}
\sum\limits^n_{j=1}\mathbb{E}(Y_iA_{i,j}Y_j)=\sum\limits^n_{i=1}
\sum\limits^n_{j=1} A_{i,j}[Var(Y)]_{i,j}\\
    &amp;=\sum\limits^n_{i=1} \sum\limits^n_{j=1}
A_{i,j}[Var(X-\mu)]_{i,j}=\sum\limits^n_{i=1} \sum\limits^n_{j=1}
A_{i,j}[Var(X)]_{i,j}\\
    &amp;=\sum\limits^n_{i=1} \sum\limits^n_{j=1}
A_{i,j}\Sigma_{i,j}=\sum\limits^n_{i=1} \sum\limits^n_{j=1}
A_{i,j}\Sigma_{j,i}\\
    &amp;=\sum\limits^n_{i=1} [A\Sigma]_{i,i}=tr(A\Sigma)
\end{split}
\end{equation}
\]</span> 故，依据(6)、(7)和(8),易得到公式(5)。</p>
<h4 id="欧氏距离期望">欧氏距离期望</h4>
<p><span class="math display">\[
\begin{equation}
    \mathbb{E}[(X-Y)^TI(X-Y)]=2tr(\Sigma)=2\sum\limits^n_{i=1}\sigma_{ii}
\end{equation}
\]</span></p>
<h4 id="马氏距离期望">马氏距离期望</h4>
<p><span class="math display">\[
\begin{equation}
    \mathbb{E}[(X-Y)^T\Sigma^{-1}(X-Y)]=2tr(\Sigma^{-1}\Sigma)=2n
\end{equation}
\]</span></p>
<h2 id="马氏距离推导">马氏距离推导</h2>
<p>设原始数据为<span class="math inline">\(x \in
\mathcal{R^p}\)</span>，经过可逆线性变换后为数据<span
class="math inline">\(y\)</span>，且假设<span
class="math inline">\(x=\hat{P}\hat{\Lambda}y\)</span>,其中<span
class="math inline">\(\hat{P}\)</span>为正交阵，负责旋转，<span
class="math inline">\(\hat{\Lambda}\)</span>为对角阵，负责伸缩。目的是将变换后的数据变成各维度独立且尺度统一，即<span
class="math inline">\(Var(y)=I\)</span>，对y计算欧氏距离就是<span
class="math inline">\(x\)</span>计算马氏距离。其实，就是和上述讲到的最小化特征相关度再进行标准化一致。</p>
<ol type="1">
<li>原数据的协方差矩阵：<span
class="math display">\[\Sigma=Var(x)=Var(\hat{P}\hat{\Lambda}y)=\hat{P}\hat{\Lambda}Var(y)\hat{\Lambda}\hat{P}=\hat{P}\hat{\Lambda}\hat{P}^T\]</span></li>
<li>转换后数据的欧氏距离： <span
class="math display">\[y^Ty=(x^T\hat{P}\hat{\Lambda}^{-1})(\hat{\Lambda}^{-1}\hat{P}^Tx)=x^T\Sigma^{-1}x\]</span></li>
</ol>
<p>这样，就得到了马氏距离的表达式。</p>
<h2 id="矩阵相关不等式">矩阵相关不等式</h2>
<h3 id="柯西-施瓦茨不等式三角不等式">柯西-施瓦茨不等式(三角不等式)</h3>
<ul>
<li>已知<span class="math inline">\(b,d \in
\mathcal{R^p}\)</span>(两个样本) <span
class="math display">\[\begin{equation}
  (b^Td)^2 \leq (b^Tb)(d^Td)
\end{equation}\]</span> 等号成立当且仅当<span
class="math inline">\(b=cd\)</span>，其中c为常数。 <span
class="math inline">\(\bf{Proof}\)</span>: <span class="math display">\[
\begin{gather*}
  b^Td=|b| \cdot |d| \cdot \cos&lt;b,d&gt; \\
  (b^Td)^2=(|b| \cdot |d| \cdot \cos&lt;b,d&gt;)^2\\
  =|b|^2 \cdot |d|^2 \cdot \cos^2 &lt;b,d&gt;\\
  \leq (b^Tb)(d^Td), \cos&lt;b,d&gt; \in [0,1]
\end{gather*}
\]</span></li>
</ul>
<h3 id="扩展的柯西-施瓦茨不等式">扩展的柯西-施瓦茨不等式</h3>
<ul>
<li><span class="math inline">\(B\)</span>为正定矩阵<span
class="math display">\[(b^Td)^2 \leq
(b^TBb)(d^TB^{-1}d)\]</span>等号成立当且仅当<span
class="math inline">\(b=cB^{-1}d\)</span>，其中c为常数。</li>
</ul>
<h3 id="极大化引理">极大化引理</h3>
<ul>
<li><span class="math inline">\(B \in \mathcal{R^{p \times p}},d \in
\mathcal{R^p}\)</span>,<span
class="math inline">\(B\)</span>为正定矩阵，<span
class="math inline">\(d\)</span>为给定向量，则对任何非零向量<span
class="math inline">\(x\)</span>,有<span class="math display">\[
\mathop{max}\limits_{x \neq 0}
\frac{(x^Td)^2}{x^TBx}=d^TB^{-1}d\]</span> 其中最大值取在<span
class="math inline">\(x=cB^{-1}d\)</span>，其中c为常数。</li>
</ul>
<h3 id="单位球上二次型最大化瑞丽商">单位球上二次型最大化(瑞丽商)</h3>
<p><span class="math inline">\(B\)</span>为正定矩阵，特征值为<span
class="math inline">\(\lambda_1 \geq \lambda_2 \geq \dotsb \geq
\lambda_p \geq 0\)</span>，对应特征向量（单位化后）为<span
class="math inline">\(e_1,e_2,\dotsb,e_p\)</span>,则<span
class="math display">\[\begin{gather*}
    \mathop{max}\limits_{x \neq 0} \frac{x^TBx}{x^Tx} =
\lambda_1(x=e_1)\\
    \mathop{min}\limits_{x \neq 0} \frac{x^TBx}{x^Tx} =
\lambda_p(x=e_p)\\
    \mathop{max}\limits_{x \bot e_1,\dotsb,e_k} \frac{x^TBx}{x^Tx} =
\lambda_{k+1}(x=e_{k+1})\\
\end{gather*}\]</span></p>
<figure>
<img data-src="/images/Applied%20Multivariate%20Statistical%20Analysis/p2.png"
alt="超平面和向量投影" />
<figcaption aria-hidden="true">超平面和向量投影</figcaption>
</figure>
<h2 id="向量投影">向量投影</h2>
<p>设<span class="math inline">\(x,y \in
\mathcal{R^n}\)</span>,向量<span class="math inline">\(x\)</span>到<span
class="math inline">\(y\)</span>的投影：<span
class="math inline">\(\frac{x^Ty}{y^Ty}y\)</span>。证明可以看MIT的《Introduction
to linear algebra》。</p>
<h2 id="矩阵求导">矩阵求导</h2>
<p><span class="math inline">\(A\)</span>为矩阵，<span
class="math inline">\(x\)</span>为向量，则有 <span
class="math display">\[
\begin{gather*}
    \frac{\partial}{\partial x}(Ax)=A^T\\
    \frac{\partial}{\partial x}(x^TA)=A\\
    \frac{\partial}{\partial x}(x^Tx)=2x\\
    \frac{\partial}{\partial x}(x^TAx)=Ax+A^Tx
\end{gather*}
\]</span> <span class="math inline">\(A\)</span>为矩阵，<span
class="math inline">\(x\)</span>为方阵，则有<span
class="math display">\[
\begin{gather*}
    \frac{\partial |A|}{\partial A}=|A|A^{-1}\\
    \frac{\partial tr(AB)}{\partial A}=B^T\\
    \frac{\partial tr(A^{-1}B)}{\partial A}=-A^{-1}B^TA^{-1}
\end{gather*}\]</span></p>
<h2 id="统计量的更多运算">统计量的更多运算</h2>
<p><span class="math display">\[
\begin{gather*}
    \mathbb{E}(c^Tx)=x^T\mu\\
    Var(c^Tx)=c^T\Sigma c\\
    \mathbb{E}(CX)=C\mu_X\\
    Cov(CX)=C\Sigma_X C\\
    Cov(b^Tx,c^Tx)=b^TCov(x)c
\end{gather*}\]</span></p>
<h1 id="其他章节">其他章节</h1>
<p><a
href="https://keithmalarkey.github.io/2023/05/15/Applied-Multivariate-Statistical-Analysis-2/">Applied
Multivariate Statistical Analysis(2)</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>统计学习</category>
      </categories>
      <tags>
        <tag>统计分析</tag>
        <tag>数据降维</tag>
      </tags>
  </entry>
  <entry>
    <title>创建一个hexoBlog</title>
    <url>/2023/05/08/Create-a-hexoBlog/</url>
    <content><![CDATA[<h1 id="使用hexo框架创建个人博客">使用hexo框架创建个人博客</h1>
<p>创建个人博客，主要用于机器学习及其相关数学基础，包括线性降维、非线性降维（流形学习）、统计分析、凸分析、信息论基础等。除此之外，会做一些算法笔记，包括一些state-of-art
methods。也可能写一点和算法分析、操作系统、Linux、vim(neovim)的学术/技术贴。在此，本贴给出hexo博客的创建流程。</p>
<hr />
<h1 id="安装">安装</h1>
<h2 id="安装需求">安装需求</h2>
<ul>
<li>Node.js (version &gt;= 10.13)</li>
<li>Git</li>
<li>npm</li>
<li>github账户</li>
</ul>
<p>可以使用下面的cmd命令查看是否安装成功及版本 <figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line">node --version</span><br><span class="line">git --version</span><br><span class="line">npm --verison</span><br></pre></td></tr></table></figure> 或者</p>
<figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line">node -V</span><br><span class="line">git -V</span><br><span class="line">npm -V</span><br></pre></td></tr></table></figure>
<h2 id="安装hexo">安装hexo</h2>
<p>创建本地仓库/路径，用以编辑blog，比如 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /d</span><br><span class="line"><span class="built_in">mkdir</span> KeithBlog</span><br><span class="line"><span class="built_in">cd</span> KeithBlog/</span><br></pre></td></tr></table></figure> &gt;
其上的代码块中，KeithBlog即本地仓库所在文件夹，并且在下面的代码块执行需要翻墙或者配置国内镜像源（cnpm）(建议面向知乎或搜索引擎，比如<a
href="https://zhuanlan.zhihu.com/p/426952333">zhihu</a>)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<h2 id="初始化blog">初始化blog</h2>
<p>准备好环境之后，就可以初始化blog了，在本地仓下使用下面bash命令，初始化一个名为blog的目录结构，会生成基础的构建
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo init blog</span><br><span class="line"><span class="built_in">cd</span> blog</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure> &gt; hexo
s是关于hexo的命令，通常用来在本地对编辑的网页博客进行预览，可以参考<a
href="https://hexo.io/zh-cn/docs/commands">hexoCmds</a></p>
<h2 id="创建新的blog">创建新的blog</h2>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">hexo n [layout] <span class="tag">&lt;<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 如：hexo new &quot;keith&#x27;s first Blog&quot; --&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>layout表示文章布局，可以选择post/page/draft，需要在_config.yml配置文件中对default_layout的参数进行修改。</p>
</blockquote>
<h2 id="生成和部署blog">生成和部署blog</h2>
<p>使用下面的cmd就可以生成网页（当前还没有部署到github项目中）
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- clean清楚缓存文件和静态文件,g指的是generate,d指的是deploy --&gt;</span></span><br><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></p>
<h1
id="将blog项目部署到github以chic主题为例">将blog项目部署到github(以Chic主题为例)</h1>
<h2 id="为blog创建github的repo">为blog创建github的repo</h2>
<p><code>cd /KeithBlog/blog</code>进入博客的本地仓，输入<code>git init</code>初始化git的main分支，在github上创建一个新的repo,注意一定要是<code>username.github.io</code>,这里的<code>username</code>指的是github名称，当然也可以在bash环境下使用
<code>git config --global --list</code>
查看user.name确认<code>username</code>，当然还需要配置SSH密钥</p>
<h2 id="生成ssh密钥">生成SSH密钥</h2>
<p>使用<code>git config --global --list</code>
查看user.email确认<code>&lt;email&gt;</code>，之后在bash环境下使用<code>ssh-keygen -t rsa -C &lt;email&gt;</code>,然后一直回车，当出现
下图时，说明密钥已经生成 <img data-src="/images/Create-a-hexoBlog/p1.png"
alt="密钥" /> 之后以下bash代码 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/.ssh</span><br><span class="line">vi id_rsa.pub</span><br></pre></td></tr></table></figure>
将内容全部复制后，在键盘输入：q，退出vi编辑。之后，来到<a
href="https://github.com/">github</a>中,点击右上角的用户头像，选择Settings,然后选择左侧的<code>SSH and GPG keys</code>，选择New
SSH
key,随便取个title(比如key4blog)，将刚刚复制的公钥粘贴到下方的Key中，然后执行bash命令<code>ssh -T git@github.com</code>确认主机和github网站之间的ssh通信是否连接成功
<img data-src="/images/Create-a-hexoBlog/p2.png" alt="连接判断" /></p>
<h2
id="配置blog主题建议安装next主题">配置blog主题(建议安装Next主题)</h2>
<p><code>cd /KeithBlog/blog</code>回到本地仓,在<a
href="https://hexo.io/themes/"
class="uri">https://hexo.io/themes/</a>中选择心仪的主题，我选择了Chic,所以在bash中执行<code>git clone git@github.com:Siricee/hexo-theme-Chic.git themes/Chic</code>,打开到路径<code>themes\Chic</code>,编辑<code>_config.yml</code>配置文件（注意yml也是键值对，但是冒号之后需要有空格，并且由空格控制嵌套结构），更改或编辑基本信息（navname、nickname、description、links等），然后<code>cd /KeithBlog/blog</code>回到仓库地址，编辑<code>_config.yml</code>配置文件，主要包括#Site信息和url(一定要是<code>https://&lt;username&gt;.github.io</code>),并且将theme变成Chic，deploy编辑为
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@github.com:<span class="tag">&lt;<span class="name">username</span>&gt;</span>/<span class="tag">&lt;<span class="name">username</span>&gt;</span>.github.io.git</span><br><span class="line">  <span class="comment">&lt;!-- 注意branch不是旧版本的master --&gt;</span></span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure></p>
<p>之后<code>hexo g &amp;&amp; hexo d</code>并输入<code>https://&lt;username&gt;.github.io</code>查看是否部署成功</p>
<h2 id="生成tag和categories页面">生成tag和categories页面</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new page tag</span><br><span class="line">hexo new page category</span><br></pre></td></tr></table></figure>
<p>之后,执行bash指令<code>cd source/tag/</code>和<code>vi index.md</code>,编辑成
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">title: Tag</span><br><span class="line">layout: tag</span><br></pre></td></tr></table></figure>
同样地，执行bash命令<code>cd source/category/</code>和<code>vi index.md</code>,编辑成
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">title: Category</span><br><span class="line">layout: category</span><br></pre></td></tr></table></figure></p>
<h2 id="生成自定义theme的blog">生成自定义theme的blog</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意hexo d可以将本地仓上传到github repo中，和git remote add后push功能一致</span></span><br><span class="line">hexo cl</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
<h1
id="博客中图片无法同步以chic主题为例">博客中图片无法同步(以Chic主题为例)</h1>
<ol type="1">
<li>本地仓路径为<code>/d/KeithBlog/blog</code>(根目录)，在根目录使用<code>git clone https://github.com/CodeFalling/hexo-asset-image</code>下载hexo-asset-image;</li>
<li>在博客的（不是主题的）<code>_config.yml</code>中，将<code>post_asset_folder</code>设置为<code>true</code>;</li>
<li>来到<code>/blog/source/_posts/</code>下，建立当前博客(当前编辑的markdown文件名)同名的文件夹（folder），然后放置博客插图到该文件夹(folder)下,在<code>.md</code>格式下的图片地址为<code>./&lt;folder&gt;/&lt;picname&gt;</code>；
&gt;注意：如果你使用typora编辑markdown,一定要是<code>./&lt;folder&gt;/&lt;picname&gt;</code>的地址格式，而不是<code>/&lt;folder&gt;/&lt;picname&gt;</code>，具体可以参考<a
href="https://www.bilibili.com/video/BV1D7411U7Yk/?spm_id_from=333.880.my_history.page.click&amp;vd_source=03a1d15d9e2ed541e2d422d799de6c41">bilibili</a>这位up的教程,这主要是由于typora的默认地址格式设置导致，如果是vscode则没有该问题</li>
</ol>
<h1 id="更改页面字体以chic主题为例">更改页面字体(以Chic主题为例)</h1>
<p>初始状态的字体真的很丑，所以有必要换一下字体 1.
选择自己喜欢的字体（.ttf格式，我选择的是Arial来自JBmono）; 2.
在<code>themes\Chic(&lt;your-chosen-theme&gt;)\source\fonts\</code>下新建字体文件夹(costom-font)，导入上述自定义字体;
3.
来到<code>themes\Chic(&lt;your-chosen-theme&gt;)\source\css\</code>下，编辑font.styl风格文档，将<code>font-family</code>改为文件夹名<code>&lt;costom-font&gt;</code>，且将该字体设置为首选。
4. .styl具体如下：<img data-src="/images/Create-a-hexoBlog/p3.png"
alt="styl设置" /></p>
<h1 id="latex支持以chic主题为例">latex支持(以Chic主题为例)</h1>
<p><span
class="math inline">\(\LaTeX\)</span>中的公式编辑是十分重要的，而且在论文/笔记编辑中由于强大的排版优势，经常涉及到。但是hexo自带的markdown渲染不支持<span
class="math inline">\(\LaTeX\)</span>，所以如果使用的是next主题还需要很多额外配置（网上的教程也很多），幸运的是Chic主题不需要，可以参考<a
href="https://nathaniel.blog/tutorials/make-hexo-support-math-again/">latex编辑</a>，具体操作流程：
1. 在<code>Chic\_config.yml</code>配置文件末添加 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># plugin functions</span></span><br><span class="line"><span class="comment">## Mathjax: Math Formula Support</span></span><br><span class="line"><span class="comment">## https://www.mathjax.org</span></span><br><span class="line"><span class="attr">mathjax:</span></span><br><span class="line"><span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">import:</span> <span class="string">demand</span> <span class="comment"># global or demand</span></span><br><span class="line"><span class="comment">## global: all pages will load mathjax,this will degrade performance  and some grammers may be parsed wrong.</span></span><br><span class="line"><span class="comment">## demand: Recommend option,if your post need fomula, you can declare &#x27;mathjax: true&#x27; in Front-matter</span></span><br></pre></td></tr></table></figure> 2.
执行bash命令<code>npm uninstall hexo-renderer-marked --save</code>和<code>npm install hexo-renderer-kramed --save</code>，然后去到<code>&lt;your-project-dir&gt;/node_modules/hexo-renderer-kramed/lib/renderer.js</code>，将代码块
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">formatText</span>(<span class="params">text</span>) &#123;</span><br><span class="line">    <span class="comment">// Fit kramed&#x27;s rule: $$ + \1 + $$</span></span><br><span class="line">    <span class="keyword">return</span> text.<span class="title function_">replace</span>(<span class="regexp">/`\$(.*?)\$`/g</span>, <span class="string">&#x27;$$$$$1$$$$&#x27;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 修改为： <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">formatText</span>(<span class="params">text</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 3.
由于在1中我们将import设置为demand类型，所以在需要使用<span
class="math inline">\(\LaTeX\)</span>编辑的博客文章头部设定<code>mathjax: true</code>,比如此文章<img data-src="/images/Create-a-hexoBlog/p4.png"
alt="latex渲染" />，这就是我转Next主题的文章。</p>
<blockquote>
<p>注意，这样做只能满足单行latex编辑，所以还存在问题</p>
</blockquote>
<h1 id="next主题配置">Next主题配置</h1>
<p>相似于section 2.3-section 5，参考<a
href="https://blog.csdn.net/Bennnnnnn/article/details/128000842">Next主题配置</a>。至于<span
class="math inline">\(\LaTeX\)</span>配置，可以参考<a
href="https://blog.csdn.net/qq_52466006/article/details/126924064">hexo-next
latex配置</a>，但是很多帖子反应pandoc下载的版本过低，我这里直接给出下载pandoc的<a
href="https://github.com/jgm/pandoc/blob/main/INSTALL.md">pandoc
download</a>，可以依据自己的OS在cmd下载到系统，并且使用<code>pandoc --version</code>查看是否安装成功或版本，插图刷新的问题不同于Chic主题，需要在<code>blog\source\images</code>下创建对应博客名称的文件夹<code>&lt;folder&gt;</code>，在<code>&lt;folder&gt;</code>下放置图片，还有一个值得注意的是，在编辑博客的markdown文件时，地址格式为<code>/images/&lt;folder&gt;/&lt;pic&gt;.&lt;format&gt;</code>，可以利用<code>hexo s</code>在本地预览并同步修改刷新，查看是否成功。</p>
<h1 id="注意事项">注意事项</h1>
<ol type="1">
<li>在同步到github-page时，一定要注意branch是否正确（master/main是有区别的），否则很容易出现位同步刷新网页博客的问题;</li>
<li>hexo官网：<a href="https://hexo.io/zh-cn/docs/"
class="uri">https://hexo.io/zh-cn/docs/</a></li>
</ol>
<h1 id="参考文章">参考文章</h1>
<ul>
<li><a
href="https://blog.csdn.net/qq_52466006/article/details/126924064"
class="uri">https://blog.csdn.net/qq_52466006/article/details/126924064</a></li>
<li><a href="https://blog.csdn.net/Bennnnnnn/article/details/128000842"
class="uri">https://blog.csdn.net/Bennnnnnn/article/details/128000842</a></li>
<li><a href="https://www.issey.top/"
class="uri">https://www.issey.top/</a></li>
</ul>
]]></content>
      <categories>
        <category>Instruction</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Residual Networks for Image Recognition</title>
    <url>/2023/05/16/Deep-Residual-Networks-for-Image-Recognition/</url>
    <content><![CDATA[<h1 id="前言">前言</h1>
<p>残差网络(ResNet)已经成为了SOTA的方法，在图像识别、文本处理等领域有着举足轻重的地位。本贴以何凯明的<a
href="https://arxiv.org/pdf/1512.03385.pdf">Deep Residual Learning for
Image Recognition</a>作为开篇。<br />
主要目的：<br />
1. 什么是ResNet？其基本结构是什么？<br />
2. 为什么提出ResNet?或者说它的优势是什么？<br />
3. 为何会有其优势，从数理层面的解释是什么？<br />
4. 适用的场景是什么？它与卷积网的不同点是什么？</p>
<hr />
<h1 id="论文解析">论文解析</h1>
<h2 id="概要">概要</h2>
<p>一般而言，深度神经网络难以训练。ResNet给出了较之于传统深度网络更加deeper的结构，且由于给出了更加平和的优化地形，缓解了网络训练的压力。</p>
<h2 id="引言">引言</h2>
<p>深度网络有更好的图像分类性能。深度对于非平凡的图像识别任务是至关重要的因素。问题是：（通过训练所得）更好的网络是否就是简单地堆加更多的网络层呢？现实是，如果要对此作出回答，必须先要解决梯度弥散和梯度爆炸的问题。目前(2022/2023)，常见的为了处理梯度消失和梯度爆炸的常见手段是对网络参数进行weight
normlization,或者对间歇地在网络中增加normlization layers(BN, Batch
Normalization)。<br />
然而，在解决了梯度问题使得loss开始收敛之后，新的问题出现了：网络深度增加了，准确率一度饱和甚至出现了下降的现象。如下图所示：<img data-src="/images/Deep-Residual-Networks-for-Image-Recognition/p1.png"
alt="深层网络准确率饱和现象" />
从Fig.1可以看出，不是说层数越多，训练（测试）的错误率就越小，况且也没有出现过拟合。此外还发现，在一定的迭代之后出现明显的饱和现象。训练精度的降级意味着复杂的模型难以优化。
<font color="red">
设计思路：考虑一个可以在深层增加更多layers的浅层结构。存在一种设计，就是将深层的layers设计成恒等映射(identity
mapping)，这种设计(相比于只用浅层网络)应该不会产生更高的训练误差(training
error)。
</font>。换言之，浅层网络本就可以构造出比较好的特征空间，即便是在后面加了恒等映射，其性能也不会变得比浅层网络更差。<br />
<img data-src="/images/Deep-Residual-Networks-for-Image-Recognition/p2.png"
alt="Shortcut connection" /> <font color="red">Deep residual learning
framework</font>:假设我们希望得到的一个潜在映射为<span
class="math inline">\(\mathcal{H}(\bf
x)\)</span>,当然这也是整个框架希望得到的(设计思路那里所说的可以在深层增加更多layers的浅层结构，然后深层+浅层作为整个framework)。在深层layers中，目标学习映射将不再是<span
class="math inline">\(\mathcal{H}(\textbf{x})\)</span>，而是<span
class="math inline">\(\mathcal{F}(\textbf{x}) :=
\mathcal{H}(\textbf{x})-\textbf{x}\)</span>，并且对来自浅层网络的输入<span
class="math inline">\(\bf x\)</span>进行恒等映射之后，进行shortcut
connection，将深层网络学到的<span class="math inline">\(\mathcal{F}({\bf
x})\)</span>直接同恒等映射后的<span class="math inline">\(\bf
x\)</span>相加，我们从Fig.2中看到这样的流程。在下图中给出了残差网络的全局设计。
<img data-src="/images/Deep-Residual-Networks-for-Image-Recognition/p3.jpg"
alt="全局设计" /></p>
<h2 id="实验结果">实验结果</h2>
<p><img data-src="/images/Deep-Residual-Networks-for-Image-Recognition/p4.png"
alt="ImageNet上的残差结构" /> 从Tab.1看到，每个残差块(residual
block)以及残差块的数量都是超参数(回到了炼丹玄学)，所以这些都要在验证集上不断调试，我。。</p>
<p><img data-src="/images/Deep-Residual-Networks-for-Image-Recognition/p5.png"
alt="Resnet性能同plain CNN的对比" />
上图给出了ImageNet数据集上的对比，首先对比左右图，可以看到ResNet的性能优势；其次我们看到ResNet收敛的速度更快；最后，我们看得到两个错误率断崖式降低，原因是学习率的调整(<span
class="math inline">\(\times 0.1\)</span>)，所以这又是一个超参数。</p>
<h1 id="数理解释">数理解释</h1>
<ol type="1">
<li>假设plain CNN需要学得潜在的mapping是<span
class="math inline">\(\mathcal{G}(\theta)\)</span>(<span
class="math inline">\(\theta\)</span>是需要学到的网络参数)，它可以抽象成两个部分：(1)浅层部分的映射<span
class="math inline">\(g\)</span>;(2)深层部分的映射<span
class="math inline">\(f\)</span>，即<span
class="math inline">\(\mathcal{G}(\theta)=f(g(\theta))\)</span>。所以在参数优化时，实际上做的就是<span
class="math inline">\(\frac{\partial\mathcal{G}(\theta)}{\partial\theta}=\frac{\partial
f(g(\theta))}{\partial g(\theta)} \cdot \frac{\partial
g(\theta)}{\partial
\theta}\)</span>。所以，由于这样的多次迭代(上百甚至百万次)，容易出现梯度弥散(梯度消失)的现象；</li>
<li>同样的，ResNet需要学习的潜在mapping也是<span
class="math inline">\(\mathcal{G}(\theta)\)</span>(<span
class="math inline">\(\mathcal{G}(\theta)=f(g(\theta))+g(\theta)\)</span>)，但是由于residual
connection的原因，每次迭代训练参数时需要做的是<span
class="math inline">\(\underbrace{\frac{\partial f(g(\theta))}{\partial
g(\theta)} \cdot \frac{\partial g(\theta)}{\partial \theta}}_{(1)} +
\underbrace{\frac{\partial g(\theta)}{\partial
\theta}}_{(2)}\)</span>，在(1)中还会存在plain
CNN中情况，但是由于(2)的存在，经过多次迭代之后，完全不会出现梯度弥散的情形。所以这就是较之于CNN的优势所在。</li>
</ol>
<h1 id="总结">总结</h1>
<p>ResNet就是使用了shortcut(residual)
connection的CNN变种，其基本结构就是残差块(residual
block)和残差连接(residual
connection)。提出ResNet的背景就是深度网络可以有效地提高图像识别的性能，但是当到了一定的深度时，出行了性能饱和甚至降级，原因也不是饱和，况且由于深度网络由于庞大复杂的参数优化任务，难以优化，所以提出了残差网络用以解决深度网络的参数学习问题，有效的解决了梯度弥散问题。</p>
<h1 id="遗留问题">遗留问题</h1>
<ol type="1">
<li>ResNet有效地解决了深度网络参数优化过程中的梯度弥散现象，那么如何缓和/解决梯度爆炸呢？</li>
<li>由于ResNet在CNN的基础之上，多了残差结构，那么如何设计残差块和学习率衰变都是额外的超参数设定任务。</li>
<li>提到Residual，在机器学习中，比如Gradient
boosting中的residual和这里的resodual有什么区别？(这部分目前我还是小白，哎)。</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>图像识别</tag>
        <tag>ResNet</tag>
      </tags>
  </entry>
  <entry>
    <title>Hyperspectral or Multispectral Image</title>
    <url>/2023/07/14/Hyperspectral-or-Multispectral-Image/</url>
    <content><![CDATA[<h1 id="什么是高多光谱图像">什么是高(多)光谱图像</h1>
<p>一般常见的图像： + RGB图像（3 channels） + 纯色图像(1 channel)</p>
<p>高光谱图像： + 大量不同波段的图像(N channels, gennerally, N <span
class="math inline">\(\geq\)</span> 100) + 波段可以连续或离散 +
每个波段捕捉不同波长范围内的光谱信息</p>
<p>高光谱图像通常由高光谱成像设备（如高光谱遥感仪或高光谱相机）获取。这些设备能够在数百个或数千个窄波段范围内获取数据，并生成具有多个波段的图像。每个波段对应着不同的光谱范围，例如可见光、红外线和近红外线等。通过组合这些波段的数据，我们可以获得一个包含丰富光谱信息的多波段图像。</p>
<p>多光谱图像: + N channels(generally, 1 <span
class="math inline">\(\leq\)</span> N <span
class="math inline">\(\leq\)</span> 100)</p>
<p>多光谱图像和高光谱图像高度相似，区别在于波段较少</p>
<h1 id="高多光谱b-times-h-times-w图象识别">高(多)光谱(B <span
class="math inline">\(\times\)</span> H <span
class="math inline">\(\times\)</span> W)图象识别</h1>
<h2 id="光谱信息和空间信息">光谱信息和空间信息</h2>
<p>机器学习任务主要是学习特征表示，但是在接触高光谱之初，其实由于没有相关知识的学习，对于学习什么特征表示，是一头雾水。</p>
<p>通过些许论文的阅读，了解到主要是来自spectral-spatial的特征。如果一张图片是由多个channels组成，那么在高光谱图像中的channels也可以称为波段(spectral
bands, B)。</p>
<h3 id="spatial-information">spatial information</h3>
<p>spatial(空间)信息其实很好理解，就是将所有的波段分离开来，每张子图(<span
class="math inline">\(H \times
W\)</span>)内表现出的特征，就叫做spatial特征。</p>
<p>通过利用图像的空间信息，例如像素的邻域关系、纹理、形状等，可以进一步提高分类的准确性。常见的方法包括基于纹理特征的滤波器方法、基于空间像素关系的邻域统计方法等。</p>
<h3 id="spectral-information">spectral information</h3>
<p>其实光谱信息(spectral
information)是很抽象的。因为空间信息中像素的邻域关系、纹理、形状都可以理解并抛给深度网络进行学习特征表示，但是spectral,
what is it?</p>
<p>以下是面向GPT的结果：<br />
1.
光谱特征提取：对于每个像素，可以提取每个波段的光谱特征，例如波段反射率或辐射强度。通过计算波段的统计特征，如平均值、标准差、最大值、最小值等，可以获取波段的特征表示。<br />
2.
光谱指数：通过计算不同波段之间的比值或差异，可以提取地物的特定属性。常用的光谱指数包括NDVI（归一化植被指数）、NDWI（归一化水体指数）、NDSI（归一化积雪指数）等。这些指数可以用于提取植被、水体、积雪等地物信息。<br />
3.
光谱曲线形状：每个地物类别在光谱上具有特定的形状和特征。通过对比不同地物类别在光谱曲线上的差异，可以区分不同地物类别。例如，某个地物类别可能在某个波段上有较高的反射率，而其他类别则相对较低。<br />
4.
波段组合：将多个波段进行组合，形成新的特征表示。可以通过使用波段的比值、差异或组合算法（如主成分分析）来创建新的波段组合。这些波段组合可以提供更具区分度的特征，用于地物分类。</p>
]]></content>
  </entry>
  <entry>
    <title>Information Theory</title>
    <url>/2023/05/18/Information-Theory/</url>
    <content><![CDATA[<h1 id="参考资料">参考资料</h1>
<ol type="1">
<li>CS 258(2020 Spring), Information Theory, Fan Cheng from ShangHai
Jiao Tong University;</li>
<li>Statistics 311(2021 Fall), Information Theory, John Duchi from
Stanford University.</li>
<li>Thomas Cover, Elements of Information theory, 2<span
class="math inline">\(^{nd}\)</span>.</li>
</ol>
<h1 id="introduction-theory-and-ai">Introduction Theory and AI</h1>
<h2 id="深度学习中的信息论概念">深度学习中的信息论概念</h2>
<ol type="1">
<li>普遍使用的交叉熵损失函数；</li>
<li>最大信息获取基础之上的决策树构建；</li>
<li>广泛应用在NLP和Speech中的维特比算法(Viterbi algorithm)；</li>
<li>广泛应用在机翻RNNs以及其他模型结构中的encoder-decoder(编码器)。</li>
</ol>
<h2 id="前备知识">前备知识</h2>
<ol type="1">
<li>概率论基础
<ol type="1">
<li>均匀分布，高斯分布</li>
<li>期望值，方差/协方差</li>
</ol></li>
<li>优化理论基础(凹凸函数):凸(凹)函数的最小(大)值</li>
<li>Reasoning in analysis
<ol type="1">
<li>微积分</li>
<li>矩阵论基础</li>
</ol></li>
</ol>
<h1 id="chapter-1--basic-concepts-in-information-theory">Chapter
1--Basic concepts in information theory</h1>
<p>在本章节，主要引入一些信息论中的基本概念，内容相对琐碎。主要包括香农熵，KL散度、互信息和相应的条件版本。在本节的开始，假设所有的分布离散的。</p>
<h2 id="definitions-conclusion">Definitions &amp; Conclusion</h2>
<p>依据香农的回忆，信息熵可以简单地理解为"uncertainty"(不确定性)。</p>
<h3 id="entropy">Entropy</h3>
<p>假设<span class="math inline">\(P\)</span>是有限集<span
class="math inline">\(\mathcal{X}\)</span>上的一个分布，<span
class="math inline">\(p\)</span>用以标记和<span
class="math inline">\(P\)</span>相关的概率质量函数。也就是说，如果随机变量<span
class="math inline">\(X\)</span>服从<span
class="math inline">\(P\)</span>分布，那么<span
class="math inline">\(P\{X=x\}=p(x)\)</span>。那么，<span
class="math inline">\(X\)</span>(或者<span
class="math inline">\(P\)</span>)的熵可以定义为：<span
class="math display">\[\begin{equation}
    H(X):=-\sum\limits_{x \in \mathcal{X}} p(x)\log p(x)=-E_p \log p(X)
\end{equation}\]</span>由于，对于所有的<span
class="math inline">\(x\)</span>都满足<span class="math inline">\(p(x)
\leq 1\)</span>，<span
class="math inline">\(H(X)\)</span>必然是负数。</p>
<blockquote>
<p>几个要点：<br />
1. <span class="math inline">\(0\log 0 \rightarrow 0.(x \rightarrow0,
x\log x \rightarrow 0)\)</span>;<br />
2. <span class="math inline">\(H(x)\)</span>仅仅依赖于/服从于<span
class="math inline">\(p(x)\)</span>，也可以将<span
class="math inline">\(H(X)\)</span>写成<span
class="math inline">\(H(p)\)</span>;<br />
3. <span class="math inline">\(H(x) \geq 0\)</span>;<br />
4. 当<span class="math inline">\(X\)</span>是数据集<span
class="math inline">\(\mathcal{X}\)</span>上的均匀分布，那么<span
class="math inline">\(H(X)=\log |\mathcal{X}|\)</span>;<br />
5. <span class="math inline">\(H_b(X)=\log_b a H_a(X)\)</span><br />
(1)若底数为2，则信息熵的单位为bits;<br />
(2)若底数为<span
class="math inline">\(e\)</span>,信息熵的单位为nats.</p>
</blockquote>
<h3 id="entropyexamples">Entropy:examples</h3>
<ul>
<li><p>Binary entropy function <span class="math inline">\(H(p)\)</span>
<span class="math display">\[Let \quad X=\begin{cases}
  1 \qquad with \quad probability \quad p,\\
  0 \qquad with \quad probability \quad 1-p
\end{cases}\]</span> <span class="math inline">\(\Rightarrow H(X)=-p\log
p - (1-p)\log (1-p)\)</span></p></li>
<li><p>Let <span class="math display">\[X=
\begin{cases}
  a \qquad with \quad prob. \quad \frac{1}{2}\\
  b \qquad with \quad prob. \quad \frac{1}{4}\\
  c \qquad with \quad prob. \quad \frac{1}{8}\\
  d \qquad with \quad prob. \quad \frac{1}{8}
\end{cases}
\]</span> <span class="math inline">\(\Rightarrow
H(X)=\frac{1}{2}\log2+\frac{1}{4}\log4+\frac{1}{8}\log8+\frac{1}{8}\log8=\log2+\frac{3}{4}\log2=\frac{7}{4}\log2\)</span><br />
</p></li>
<li><p>用<span class="math inline">\(E\)</span>标记期望。若<span
class="math inline">\(X \sim p(x)\)</span>,则随机变量的<span
class="math inline">\(g(X)\)</span>的期望值可以写作<span
class="math display">\[E_p g(X)=\sum_{x \in \mathcal{X}}
g(x)p(x)\]</span><br />
<span class="math inline">\(\Rightarrow
H(X)=E_p\log\frac{1}{p(X)}\)</span></p></li>
<li><p>几何随机分布。随机变量<span class="math inline">\(X \sim
Geometric(p)\)</span>，如果<span class="math inline">\(p \in
[0,1]\)</span>，如果其支撑集为<span class="math inline">\(\{1,2, \dots
\}\)</span>，且<span
class="math inline">\(P(X=k)=(1-p)^{k-1}p\)</span>。则其离散香农熵为<span
class="math display">\[\begin{split}
  H(X)&amp;=-\sum\limits_{k=1}^{\infty} (1-p)^{k-1}p [(k-1)\log (1-p) +
\log p] \\
  &amp;=-p\log (1-p) \sum\limits_{k=1}^{\infty} k(1-p)^k-p\log
p\sum\limits_{k=1}
^{\infty}(1-p)^{k-1}\\
&amp;=-p(1-p)\log (1-p) \sum\limits_{k=1}^{\infty} k(1-p)^{k-1}-p\log
p\sum\limits_{k=0}
^{\infty}(1-p)^k\\
&amp;=-p(1-p)\log (1-p) \big (\sum\limits_{k=1}^{\infty}
(1-p)^k\big)^{&#39;}-p \cdot \frac{1}{p} \cdot \log p \\
&amp;=-p(1-p)\log (1-p) \big ( \frac{1-p}{p} \big)^{&#39;}-\log p \\
&amp;=\frac{1-p}{p}\log (1-p)-\log p
\end{split}\]</span> 其中，几何级数<span
class="math inline">\(\sum\limits_{k=a}^{\infty} p^k=\frac{p^a}{1-p}(a
\geq 0)\)</span></p></li>
</ul>
<h3 id="kl-divergencekl散度相对熵">KL-divergence(KL散度/相对熵)</h3>
<p>KL散度用来描述两个分布之间的距离/差异性。<span
class="math inline">\(P\)</span>和<span
class="math inline">\(Q\)</span>是离散数据集<span
class="math inline">\(\mathcal{X}\)</span>上定义的两个分布。它们两者之间的KL散度:
<span class="math display">\[
\begin{equation}
\begin{split}
  D_{kl}(P||Q)&amp;:=\sum\limits_{x \in \mathcal{X}} p(x) \log
\frac{p(x)}{q(x)}\\
  &amp;=E_p \log \frac{p(X)}{q(X)}.
\end{split}
\end{equation}
\]</span> 用凸函数性质(Jensen不等式)，看下面推导: <span
class="math display">\[\begin{equation}
\begin{split}
  D_{kl}(P||Q)&amp;=-E_p\bigg[ \log \frac{q(X)}{p(X)}\bigg] \geq -\log
E_p \bigg[ \frac{q(X)}{p(X)} \bigg]\\
  &amp;=-\log \bigg( \sum\limits_{x \in \mathcal{X}}
p(x)\frac{q(x)}{p(x)} \bigg)=-\log 1=0.
\end{split}
\end{equation}
\]</span> 这里因为<span
class="math inline">\(-\log\)</span>是严格凸的，所以就有<span
class="math inline">\(D_{kl}(P||Q) \geq 0\)</span>(当<span
class="math inline">\(P = Q\)</span>,取得等号).<br />
再有，<span class="math display">\[
\begin{equation}
  \label{entro-ineq}
  \begin{split}
    0 \leq D_{kl}(P||Q)&amp;=\sum\limits_{x \in \mathcal{X}} p(x)\log
\frac{p(x)}{q(x)}\\
    &amp;=-H(X)-\sum\limits_{x \in \mathcal{X}}p(x) \log q(x)\\
    &amp;=-H(X)+\log |\mathcal{X}|
  \end{split}
\end{equation}
\]</span> 所以<span class="math inline">\(H(X) \leq \log
|\mathcal{X}|\)</span></p>
<blockquote>
<p>使用常见不等式 <span class="math inline">\(\log x \leq
x-1(x&gt;0)\)</span> <span class="math display">\[
\begin{equation*}
-D(P||Q)=\sum\limits_{x \in \mathcal{X}} p(x) \log \frac{q(x)}{p(x)}
\leq \sum\limits_{x \in \mathcal{X}} p(x)(\frac{q(x)}{p(x)}-1) \le 0
\Rightarrow D(P||Q) \geq 0
\end{equation*}
\]</span></p>
</blockquote>
<h3 id="hx的性质范围"><span
class="math inline">\(H(X)\)</span>的性质范围</h3>
<p>对于定义在数据集<span
class="math inline">\(\mathcal{X}\)</span>上的离散型随机变量<span
class="math inline">\(X\)</span>，满足：<span
class="math display">\[\begin{equation}
    0 \leq H(X) \leq \log |\mathcal{X}|
\end{equation}\]</span><br />
证明：<br />
（1）由于<span class="math inline">\(p(x) \in [0,1]\)</span>，所以<span
class="math inline">\(H(X)=-E_p[\log p(x)] \geq 0\)</span>；
（2）给出证明<span class="math inline">\(H(X) \leq \log
|\mathcal{X}|\)</span>的两个方法：<br />
1. Jensen不等式<br />
<span class="math inline">\(f(x)=-x\log x, x \in (0,1].
f^{&#39;}(x)=-1-\log x, f^{&#39;&#39;}(x)=-\frac{1}{x} &lt;
0\)</span>。所以，由凸函数的二阶判定条件，知道<span
class="math inline">\(f(x)\)</span>为<span
class="math inline">\((0,1]\)</span>上的凹函数。由Jensen不等式，有<span
class="math inline">\(f[E(X)] \ge E[f(X)]\)</span>，故 <span
class="math display">\[
   \begin{split}
   \frac{1}{|\mathcal{X}|}H(X)=E[f(X)] \le -(E(X))\log
(E(X))&amp;=-\frac{\sum\limits_{x \in \mathcal{X}}p(x)}{|\mathcal{X}|}
\log \frac{\sum\limits_{x \in \mathcal{X}}p(x)}{|\mathcal{X}|}\\
   &amp;=\frac{1}{|\mathcal{X}|}\log |\mathcal{X}|
   \end{split}
   \]</span> <span class="math inline">\(\Rightarrow H(X)\le \log
|\mathcal{X}|\)</span>，当且仅当<span
class="math inline">\(p(x)=\frac{1}{|\mathcal{X}|}\)</span>取得等号。换言之，当随机变量服从均匀分布时，熵最大。<br />
2. 见公式(<span class="math inline">\(\ref{entro-ineq}\)</span>).</p>
<h3 id="联想">联想</h3>
<p>在解决长尾分布的视觉识别任务中，常见的几种缓解class-imbalance的方案——重采样，重加权和迁移学习，还有一种<a
href="https://arxiv.org/pdf/1910.09217.pdf">decoupled learning
scheme</a>。这种方案并不是典型的端到端的深度学习，具体的：将长尾分布下的训练集如同往常一样，作为orginal
input，之后再剥离(解耦)分类器classifier，对分类器进行instance-balance(实例平衡)下的参数重训练，其分类结果在绝大多数情形(Medium
shotting,Few shotting,All Samples)下，分类精度都得以提高。<br />
其实在instance-balance情形下，其样本点服从的就是均匀分布，也就说出现了信息熵最大的情形。这就是数理解释，也是我们希望得到的情形。</p>
<h3 id="general-roadmap">General Roadmap</h3>
<ol type="1">
<li>信息熵仅仅取决于随机变量的概率分布，和数据集无关<br />
Probability distribution <span
class="math inline">\(\Rightarrow\)</span> Entropy<br />
</li>
<li>设随机变量序列<span
class="math inline">\(X_1,X_2,\dots,X_n\)</span>的联合概率分布为<span
class="math inline">\(p(x_1,x_2,\dots,x_n)\)</span><br />
</li>
<li>概率论中基本准则
<ol type="1">
<li>链式法则(Chain rule)：<span class="math display">\[\begin{equation}
\begin{split}
&amp; p(x_1,x_2,\dots,x_n)  \\
=&amp; p(x_n)p(x_{n-1}|x_n)p(x_{n-2}|x_{n-1},x_n) \dotsb
p(x_1|x_2,x_3,\dots,x_{n})
\end{split}
\label{chain-rule}
\end{equation}\]</span></li>
<li>贝叶斯准则：<span
class="math inline">\(p(y)p(x|y)=p(x)p(y|x)\)</span></li>
</ol></li>
</ol>
<h3 id="联合熵">联合熵</h3>
<ul>
<li>The joint entropy <span class="math inline">\(H(X,Y)\)</span> of a
pair of discrete random variable <span
class="math inline">\((X,Y)\)</span> with joint distribution <span
class="math inline">\(p(x,y)\)</span> is defined as <span
class="math display">\[\begin{split}
H(X,Y)&amp;=-\sum\limits_{x \in \mathcal{X}} \sum\limits_{y \in
\mathcal{Y}} p(x,y)\log p(x,y)\\
&amp;=-E \log p(X,Y)
\end{split}\]</span><br />
</li>
<li><span class="math inline">\(H(X,X)=H(X)\)</span></li>
<li><span class="math inline">\(H(X,Y)=H(Y,X)\)</span></li>
<li>For a set of random variables <span
class="math inline">\(X_1,X_2,\dots,X_n\)</span> with joint distribution
<span class="math inline">\(p(x_1,\dots,x_n)\)</span>, its joint entropy
is defined as <span class="math display">\[
\begin{split}
H(X_1,X_2,\dots,X_n)&amp;=-\sum p(x_1,x_2,\dots,x_n)\log
p(x_1,x_2,\dots,x_n)\\
&amp;=-E \log p(X_1,X_2,\dots,X_n)
\end{split}
\]</span></li>
</ul>
<h3 id="条件熵">条件熵</h3>
<p>表示在观测到一个随机变量之后，另一个随机变量所持有的信息量。<span
class="math display">\[
\begin{equation}
    \label{condi-entro}
    \begin{split}
    H(X|Y=y)&amp;=-\sum\limits_{x \in \mathcal{X}} p(x|Y=y) \log
p(x|Y=y) \\
    and \quad H(X|Y)&amp;=\sum\limits_{y \in \mathcal{Y}} p(y) H(X|Y=y)
    \end{split}
\end{equation}
\]</span> 据公式(<span
class="math inline">\(\ref{condi-entro}\)</span>),另有 <span
class="math display">\[
\begin{equation}
    \begin{split}
    H(X|Y)&amp;=-\sum\limits_{y \in \mathcal{Y}} p(y) \sum\limits_{x \in
\mathcal{X}} p(x|y)\log p(x|y)\\
    &amp;=-\sum\limits_{x \in \mathcal{X}}\sum\limits_{y \in
\mathcal{Y}} p(x,y)\log p(x|y)\\
    &amp;=-E_{p(x,y)} \log p(X|Y)
    \end{split}
\end{equation}
\]</span> 不难发现，当随机变量Y的分布已知，则随机变量X的熵满足:<span
class="math inline">\(H(X|Y) \leq H(X)\)</span> <img data-src="/images/Information-Theory/p1.png" alt="conditional entropy" />
计算过程(这里<span class="math inline">\(\log\)</span>以2为底) <span
class="math display">\[
\begin{equation*}
\begin{split}
  H(X,Y)&amp;=2 \times \frac{1}{8} \times \log 8 +6 \times \frac{1}{16}
\times \log 16+4 \times \frac{1}{32}  \times \log 32+\frac{1}{4} \times
\log4 \\
  &amp;=\frac{3}{4}+\frac{6}{4}+\frac{5}{8}+\frac{1}{2}\\
  &amp;=\frac{27}{8}\\
  H(X)&amp;=\frac{1}{2}+\frac{1}{2}+2 \times 3 \times \frac{1}{8}\\
  &amp;=\frac{7}{4}\\
  H(Y)&amp;=4 \times \frac{1}{4} \times \log 4\\
  &amp;=2\\
  H(X|Y)&amp;=H(X,Y)-H(Y)=\frac{11}{8}\\
  H(Y|X)&amp;=H(X,Y)-H(X)=\frac{13}{8}
\end{split}
\end{equation*}
\]</span></p>
<h3 id="条件熵和联合熵">条件熵和联合熵</h3>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    p(x,y)&amp;=p(x|y)p(y)=p(y|x)p(x)\\
    \Rightarrow \log p(x,y)&amp;=\log p(x|y) + \log p(y)=\log
p(y|x)+\log p(x)\\
  \end{split}
\end{equation*}\]</span> 上式左右取期望得 <span class="math display">\[
\begin{equation}
  \begin{split}
  E[\log p(X,Y)]&amp;=E[\log p(X|Y)]+E[\log p(Y)]\\
  &amp;=E[\log p(Y|X)]+E[\log p(X)]\\
  \Rightarrow   -E[\log p(X,Y)]&amp;=-\bigg(E[\log p(X|Y)]+E[\log p(Y)]
\bigg)\\
  &amp;=-E[\log p(X|Y)]-E[\log p(Y)]\\
  &amp;=H(X|Y)+H(Y)\\
  \Rightarrow   -E[\log p(X,Y)]&amp;=-\bigg(E[\log p(Y|X)]+E[\log p(X)]
\bigg)\\
  &amp;=-E[\log p(Y|X)]-E[\log p(X)]\\
  &amp;=H(Y|X)+H(X)
  \end{split}
\end{equation}
\]</span></p>
<ul>
<li>若随机变量<span class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>相互独立，<span
class="math inline">\(H(X,Y)=H(X)+H(Y)\)</span><br />
</li>
<li>若随机变量X是关于Y的函数，即<span
class="math inline">\(X=\mathcal{F}(Y)\)</span>，则<span
class="math inline">\(H(X,Y)=H(Y)\)</span></li>
<li>由于<span class="math display">\[p(x,y,z)=p(z)p(y|z)p(x|y,z)
\Rightarrow p(x,y|z)=p(y|z)p(x|y,z)\]</span> 则<span
class="math inline">\(H(X,Y|Z)=H(Y|Z)+H(X|Y,Z)\)</span><br />
同理可得，<span class="math display">\[H(X,Y|Z)=H(X|Z)+H(Y|X,Z)\]</span>
<img data-src="/images/Information-Theory/p2.png" alt="ce&amp;ue" /></li>
</ul>
<h3 id="kl散度不是一个metric">KL散度不是一个metric</h3>
<blockquote>
<p>A metric(测度)<span class="math inline">\(d:X,Y \rightarrow
R^{+}\)</span>between two distributions should satisfy</p>
<ul>
<li><span class="math inline">\(d(X,Y) \geq 0\)</span></li>
<li><span class="math inline">\(d(X,Y)=d(Y,X)\)</span></li>
<li><span class="math inline">\(d(X,Y)=0\)</span> if and only if <span
class="math inline">\(X=Y\)</span></li>
<li><span class="math inline">\(d(X,Y) + d(Y,Z) \geq
d(X,Z)\)</span>(三角不等式)</li>
</ul>
</blockquote>
<ol type="1">
<li>欧氏距离是一个测度</li>
<li>Kl距离不是测度
<ol type="1">
<li><span class="math inline">\(D(p||p)=0\)</span></li>
<li><span class="math inline">\(D(p||q) \neq D(q||p)\)</span><br />
<span
class="math inline">\(p=(\frac{1}{2},\frac{1}{2}),q=(\frac{1}{4},\frac{3}{4}),D(p||q)=0.2,D(q||p)=0.18\)</span></li>
</ol></li>
<li><span class="math inline">\(D(p||q)=E_p(- \log q(x))-E_p(-\log
p(x))=\textcolor{red}{E_p(-\log q(x))}-H(p)\)</span><br />
红色部分被称为为交叉熵(cross entropy)，也能描述两个分布之间的距离</li>
<li>两个随机变量<span
class="math inline">\(p,q\)</span>之间的变分距离记作：<span
class="math display">\[V(p,q)=\sum\limits_{x \in \mathcal{X}}
|p(x)-q(x)|\]</span></li>
<li><span class="math inline">\(\textcolor{red}{Prinsker&#39;s
\hspace{1em} inequality}\)</span>(普林斯科尔不等式):<span
class="math display">\[D(p||q) \geq \frac{1}{2\ln
2}V^2(p,q)\]</span></li>
</ol>
<h3 id="互信息mutual-information">互信息(Mutual information)</h3>
<p>互信息描述两个随机分布之间的相互关联程度。已知随机变量<span
class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>，其联合概率质量函数为<span
class="math inline">\(p(x,y)\)</span>,边缘分布分别为<span
class="math inline">\(p(x)\)</span>和<span
class="math inline">\(p(y)\)</span>。其数学定义为: <span
class="math display">\[
\begin{equation}
\begin{split}
  I(X;Y)&amp;=\sum\limits_x \sum\limits_y p(x,y)\log
\frac{p(x,y)}{p(x)p(y)}\\
  &amp;=D(p(x,y)||p(x)p(y))\\
  &amp;=E_{p(x,y)}\log \frac{p(X,Y)}{p(X)p(Y)}
\end{split}
\end{equation}
\]</span></p>
<ul>
<li><span class="math inline">\(I(X;Y)=I(Y;X)\)</span></li>
<li><span class="math inline">\(I(X;X)=H(X)\)</span></li>
<li><span class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>相互独立，则<span
class="math inline">\(I(X;Y)=0\)</span> <img data-src="/images/Information-Theory/p3.png" alt="venn diagram" /></li>
</ul>
<p>由公式<span
class="math inline">\((\ref{chain-rule})\)</span>,对齐两侧取<span
class="math inline">\(-\log\)</span>后再取期望值<span
class="math inline">\(E\)</span>,得到 <span class="math display">\[
\begin{equation}
\label{chainrule4entro}
  \begin{split}
H(X_1,X_2,\dots,X_n)=H(X_1)+H(X_2|X_1)+\dotsb+H(X_n|X_1,X_2,\dots,X_{n-1})
  \end{split}
\end{equation}
\]</span></p>
<ul>
<li>若<span
class="math inline">\(X_1,X_2,\dots,X_n\)</span>相互独立，则<span
class="math inline">\(H(X_1,X_2,\dots,X_n)=\sum\limits_{i=1}^n
H(X_i)\)</span></li>
<li>对于两个相对独立的随机变量<span
class="math inline">\(X\)</span>和<span
class="math inline">\(Y\)</span>，则<span class="math display">\[
\begin{equation}
\begin{split}
H(X,Y)&amp;=H(X)+H(Y)\\
I(X;Y)&amp;=H(X)+H(Y)-H(X,Y)=0
\end{split}
\end{equation}
\]</span></li>
</ul>
<h3 id="chain-rule-for-information">Chain Rule for Information</h3>
<p>The conditional mutual information of random variables <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> given <span
class="math inline">\(Z\)</span> is defined by <span
class="math display">\[
\begin{equation}
\label{cr4info}
  \begin{split}
  I(X;Y|Z)&amp;=H(X|Z)-H(X|Y,Z)\\
  &amp;=E_{p(x,y,z)} \log \frac{p(X,Y|Z)}{p(X|Z)p(Y|Z)}
  \end{split}
\end{equation}
\]</span> 可以这么考虑,由于<span
class="math inline">\(I(X;Y)=H(X)-H(X|Y)\)</span>,所以得到公式<span
class="math inline">\((\ref{cr4info})\)</span><br />
链式法则： <span class="math display">\[
\begin{equation}
  I(X_1,X_2,\dots,X_n;Y)=\sum\limits_{i=1}^n
I(X_i;Y|X_{i-1},X_{i-2},\dots,X_1)
\end{equation}
\]</span> Proof Sketch: <span class="math display">\[
\begin{equation*}
  \begin{split}
  I(X_1,X_2,\dots,X_n;Y)=&amp;H(X_1,X_2,\dots,X_n)-H(X_1,X_2,\dots,X_n|Y)
   \xlongequal[对右侧的熵进行链式拆解]{依据公式(\ref{chainrule4entro})}
\\ &amp; H(X_1)+H(X_2|X_1)+H(X_3|X_1,X_2) \\ &amp;
+\dotsb+H(X_n|X_1,X_2,\dots,X_{n-1})\\ &amp;
-(H(X_1|Y)+H(X_2|X_1,Y)+H(X_3|X_1,X_2,Y)\\ &amp; + \dotsb +
H(X_n|X_1,X_2,\dots,X_{n-1},Y))\\
   =&amp;(H(X_1)-H(X_1|Y))+(H(X_2|X_1)-H(X_2|X_1,Y))  + \dotsb + \\
&amp;
   (H(X_n|X_1,X_2,\dots,X_{n-1})-H(X_n|X_1,X_2,\dots,X_{n-1},Y))\\
   =&amp; I(X_1;Y)+I(X_2;Y|X_1)+ \dotsb +
I(X_n;Y|X_1,X_2,\dots,X_{n-1})\\
   =&amp; \sum\limits_{i=1}^n I(X_i;Y|X_{i-1},X_{i-2},\dots,X_1)
  \end{split}
\end{equation*}
\]</span></p>
<h3 id="条件相对熵">条件相对熵</h3>
<figure>
<img data-src="/images/Information-Theory/p4.png" alt="con-rela-entro" />
<figcaption aria-hidden="true">con-rela-entro</figcaption>
</figure>
<h2 id="一些重要的不等式">一些重要的不等式</h2>
<h3 id="independence-bound-on-entropy">Independence Bound on
Entropy</h3>
<p><span class="math display">\[
\begin{equation}
  \label{iboe}
  H(X_1,X_2,\dots,X_n) = \sum\limits_{i=1}^n
H(X_i|X_{i-1},\dots,X_1)  \leq \sum\limits_{i=1}^n H(X_i)
\end{equation}
\]</span></p>
<ul>
<li>条件减少熵 <span class="math inline">\(H(Y|X) \leq
H(Y)\)</span>(白话：一旦增加了约束，不确定性将减小)</li>
<li>当且仅当<span
class="math inline">\((X_i,X_{i-1},\dots,X_1)\)</span>相互独立，(<span
class="math inline">\(\ref{iboe}\)</span>)两侧取等号</li>
</ul>
<h3 id="markov-chain">Markov Chain</h3>
<p>随机变量<span
class="math inline">\(X,Y,Z\)</span>形成一个马尔科夫链：<span
class="math inline">\(Z\)</span>的条件分布只依赖于<span
class="math inline">\(Y\)</span>,独立于随机变量<span
class="math inline">\(X\)</span>，记作<span class="math inline">\(X
\rightarrow Y \rightarrow Z\)</span>。<span class="math inline">\(X
\rightarrow Y \rightarrow
Z\)</span>这样的一个马尔科夫链，其联合概率质量函数可以写成 <span
class="math display">\[
\begin{equation*}
p(x,y,z)=p(x)p(y|x)p(z|y).  
\end{equation*}
\]</span>
从时序关系的角度来看，马尔科夫链说明当前时间点所发生事件的概率只依赖于前一个时间点的事件情况。马尔可夫链在现实世界中是十分重要的一个结构：</p>
<ul>
<li><span class="math inline">\(X \rightarrow Y \rightarrow
Z\)</span>当且仅当<span class="math inline">\(X\)</span>和<span
class="math inline">\(Z\)</span>条件依赖于给定的<span
class="math inline">\(Y\)</span>
<ul>
<li>即<span class="math inline">\(X \rightarrow Y \rightarrow Z
\Leftrightarrow I(X;Z|Y)=0\)</span></li>
</ul></li>
<li><span class="math inline">\(X \rightarrow Y \rightarrow
Z\)</span>指示着<span class="math inline">\(Z \rightarrow Y \rightarrow
X\)</span>,所以有时候MC(Markov Chain)写成<span class="math inline">\(X
\leftrightarrow Y \leftrightarrow Z\)</span></li>
<li>若<span class="math inline">\(Z=f(Y)\)</span>,则<span
class="math inline">\(X \rightarrow Y \rightarrow Z\)</span></li>
</ul>
<p>Proof Sketch:<br />
<span class="math display">\[
\begin{equation*}
\begin{split}
  I(X;Z|Y)&amp;=E_p(x,y,z) \log \frac{p(X,Z|Y)}{p(X|Y)p(Z|Y)}\\
  &amp; \xlongequal[故p(X,Z|Y)=p(X|Y)p(Z|Y)]{X,Z相互独立} 0
\end{split}
\end{equation*}
\]</span></p>
<h3 id="数据处理不等式">数据处理不等式</h3>
<p>若<span class="math inline">\(MC:X \rightarrow Y \rightarrow
Z\)</span>，则 <span class="math display">\[
\begin{equation}
  I(X;Y) \geq I(X;Z)
\end{equation}
\]</span></p>
<p>Proof Sketch: Expand <span class="math inline">\(I(X;Y,Z)\)</span> by
chain rule <span class="math display">\[
\begin{equation*}
  \begin{split}
    I(X;Y,Z)&amp;=H(Y,Z)-H(Y,Z|X)\\
    &amp;=H(Z)+H(Y|Z)-H(Y|Z,X)-H(Z|X)\\
    &amp;=[H(Z)-H(Z|X)]+[H(Y|Z)-H(Y|Z,X)]\\
    &amp;=I(X;Z)+I(X;Y|Z)\\
    I(X;Y,Z)&amp;=I(X;Y)+\textcolor{purple}{I(X;Z|Y)}
  \end{split}
\end{equation*}
\]</span> where <span
class="math inline">\(\textcolor{purple}{I(X;Z|Y)}=0\)</span></p>
<blockquote>
<p>注意：在互信息的运算中，条件运算符“|”优先级高于";"，比如<span
class="math inline">\(I(\underbrace{X}_{(1)};\underbrace{Z|Y}_{(2)})\)</span>需要看作这样的两部分.此外数据处理不等式说明，数据处理的步骤越多，信息丢失的也就越多。但是在熵运算中，"|"的优先级低于","，即<span
class="math inline">\(H(\underbrace{X}_{(1)}|\underbrace{Y,Z}_{(2)})\)</span></p>
</blockquote>
<ul>
<li>特别地，若<span class="math inline">\(Z=g(Y)\)</span>，则<span
class="math inline">\(I(X;Y) \geq I(X;g(Y))\)</span></li>
<li>若<span class="math inline">\(MC:X \rightarrow Y \rightarrow
Z\)</span>，则<span class="math inline">\(I(X;Y|Z) \le
I(X;Y)\)</span></li>
<li>假设<span
class="math inline">\(X,Y\)</span>是两个相互独立的随机变量，且在<span
class="math inline">\(\{0,1\}\)</span>服从均匀分布，随机变量<span
class="math inline">\(Z\)</span>满足<span
class="math inline">\(Z=X+Y(mod \hspace{1em} 2)\)</span>,计算<span
class="math inline">\(I(X;Y|Z)\)</span>和<span
class="math inline">\(I(X;Y)\)</span></li>
</ul>
<h3 id="ixyz"><span class="math inline">\(I(X;Y;Z)\)</span></h3>
<figure>
<img data-src="/images/Information-Theory/p5.png" alt="三元互信息" />
<figcaption aria-hidden="true">三元互信息</figcaption>
</figure>
<h3 id="information-diagram">Information Diagram</h3>
<p><img data-src="/images/Information-Theory/p6.png" alt="RVs2" /> <img data-src="/images/Information-Theory/p7.png" alt="RVs3" /> <img data-src="/images/Information-Theory/p8.png" alt="MC" /></p>
<blockquote>
<p>注意:互信息在使用Venn图表示时，其相交的部分不是简单的watch(表面),其所表示的互信息可以为负</p>
</blockquote>
<h3 id="熵相关的不等式">熵相关的不等式</h3>
]]></content>
      <categories>
        <category>信息论</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Information Theory</tag>
        <tag>信息熵</tag>
      </tags>
  </entry>
  <entry>
    <title>Masked Autoencoder</title>
    <url>/2023/07/13/Masked-Autoencoder/</url>
    <content><![CDATA[<h1 id="参考">参考</h1>
<ol type="1">
<li><a href="https://arxiv.org/pdf/2111.06377.pdf">paper</a></li>
<li><a
href="https://www.bilibili.com/video/BV1sq4y1q77t/?spm_id_from=333.788&amp;vd_source=03a1d15d9e2ed541e2d422d799de6c41">文献精读
by 李沐</a></li>
</ol>
<h1 id="prerequsitions">Prerequsitions</h1>
<ul>
<li>NLP
<ul>
<li>Transformer</li>
<li>BERT</li>
</ul></li>
<li>CV
<ul>
<li>ViT</li>
</ul></li>
<li>自监督
<ul>
<li>Encoder-Decoder</li>
<li>AutoEncoder</li>
<li>DAE(Denoising Autoencoder)
<ul>
<li>Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and PierreAntoine
Manzagol. Extracting and composing robust features with denoising
autoencoders. In ICML, 2008.</li>
</ul></li>
<li>VAE(Variational Autoencoder)</li>
</ul></li>
<li>迁移学习（预训练）</li>
<li>CNN</li>
</ul>
<h1 id="question">Question</h1>
<ol type="1">
<li>What is the Masked Autoencoder? Details about the architecture?</li>
<li>What is the main application scenario of Masked Autoencoder?
<ol type="1">
<li>数据去噪</li>
<li>特征学习</li>
<li>生成模型</li>
<li>数据压缩</li>
</ol></li>
</ol>
<h1 id="abstract">Abstract</h1>
<ol type="1">
<li>scalable self-supervised learners for CV;</li>
<li>操作：对输入的图片进行随机掩码，然后利用MAE重构打上掩码的patches(这就意味着，它是一个生成模型，且可以学得Latent
representation);</li>
<li>框架：非对称的encoder-decoder
<ol type="1">
<li>encoder:只在可视的patch进行操作</li>
<li>decoder:利用潜表示和打了掩码的patches重构原始图像</li>
</ol></li>
</ol>
<h1 id="introduction">Introduction</h1>
<h2 id="讨论">讨论</h2>
<p>在计算机视觉领域，自编码器方法的发展滞后于NLP领域。那么，对掩码自编码器(MAE)，是什么使得它在处理视觉任务和NLP任务时不同的呢？</p>
<ul>
<li>过去的10年内，CNN在视觉领域占据主导地位。CNN一般在利用局部感受野，在图像网格上进行局部采集特征表示。相比较于MAE，无法融合诸如mask
tokens和位置编码这样的"指示器"(indicators)。但是结构上的差异，可以由引入ViT解决。<br />
</li>
<li>语言和视觉图像之间的信息密度是不一样的
<ul>
<li>语言具有丰富的信息，可以包含高度语义化的特征</li>
<li>图像是具有严重空间冗余的自然信号，可以从相邻的patches中恢复，而对部分、物体和场景的高级理解很少</li>
</ul></li>
<li>解码器decoder可以将学到的潜表示(latent
representation)映射回输入时的模样，然而decoder在NLP和CV领域起到不同作用。</li>
</ul>
<h1 id="框架">框架</h1>
<figure>
<img data-src="/images/Masked-Autoencoder/p1.png" alt="MAE" />
<figcaption aria-hidden="true">MAE</figcaption>
</figure>
<p>注：预训练pre-training指的是在目标任务之前，在大规模的未标记数据上进行训练的过程。在这个过程中，模型通过学习数据中的统计规律和特征来提取有用的表示，从而为后续的具体任务提供更好的初始化参数或特征。</p>
<h1 id="approach">Approach</h1>
]]></content>
      <categories>
        <category>自监督学习</category>
      </categories>
      <tags>
        <tag>Encoder-Decoder</tag>
        <tag>Autoencoder</tag>
        <tag>MAE</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer &amp; ViT</title>
    <url>/2023/07/06/Transformer/</url>
    <content><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Transformer因为更好的并行计算能力，以及在大样本下更好的表现性能，已经在NLP领域得到了广泛运用，并取得了很多成效。随着ViT的出现，视觉领域也得到了重大突破。考虑到相关研究方法的主要背景然是深度学习和图像识别，所以主要是卷积神经网络做比较。</p>
<h1 id="提问">提问</h1>
<ol type="1">
<li>What is the Transformer？其基本结构是什么？
<ol type="1">
<li>什么是“平均注意力加权位置”(average attention-weighted position)</li>
<li>为什么平均注意力加权位置会导致有效分辨率降低？</li>
</ol></li>
<li>Transformer的主要特征(优缺点)？它是如何实现的(实现机制)？</li>
<li>注意力机制？自注意力机制？两者区别？</li>
<li>ViT是如何将Transformer移植到视觉领域的？</li>
</ol>
<h1 id="参考">参考</h1>
<ul>
<li><strong>Transformer</strong></li>
</ul>
<ol type="1">
<li>Attention Is All You Need, Neural Information Processing Systems
(NIPS 2017);</li>
<li>zhihu---<a
href="https://zhuanlan.zhihu.com/p/411311520">Transformer详解</a>;</li>
<li><a
href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0">Transformer精读-李沐</a></li>
<li><a
href="https://www.bilibili.com/video/BV1q3411U7Hi/?spm_id_from=333.999.0.0">白话教程1</a></li>
<li><a
href="https://www.bilibili.com/video/BV15v411W78M/?spm_id_from=333.999.0.0&amp;vd_source=03a1d15d9e2ed541e2d422d799de6c41">白话教程2</a></li>
<li><a
href="https://www.bilibili.com/video/BV1Qg4y1P7r4/?spm_id_from=333.788.recommend_more_video.1&amp;vd_source=03a1d15d9e2ed541e2d422d799de6c41">Transformer中的数据流动</a>---<font color="red">全场最佳</font></li>
</ol>
<ul>
<li><strong>Vision Transformer</strong></li>
</ul>
<ol type="1">
<li><a href="https://arxiv.org/pdf/2010.11929.pdf">AN IAMGE IS WORTH 16
<span class="math inline">\(\times\)</span> 16 TRANSFORMERS FOR IMAGE
RECOGNITION AT SCALE</a></li>
<li><a
href="https://www.bilibili.com/video/BV15P4y137jb/?spm_id_from=333.999.0.0&amp;vd_source=03a1d15d9e2ed541e2d422d799de6c41">ViT解读</a>--by<a
href="https://bryanyzhu.github.io/">朱毅</a></li>
</ol>
<h1 id="模型框架">模型框架</h1>
<ul>
<li>left:encoder <span class="math inline">\(\times N\)</span></li>
<li>right:decoder <span class="math inline">\(\times N\)</span></li>
<li>computing the loss compared with the GT after prediction by
softmax(Linear(decoder_output)) <img data-src="/images/Transformer/p1.png"
alt="Transformer Architecture" /></li>
</ul>
<h1 id="推断分类">推断分类</h1>
<h2 id="transduction-induction">transduction &amp; induction</h2>
<p>推断主要分为两类：<br />
1. 归纳推断(inductive inference): induction is reasoning from observed
training cases to general rules, which are then applied to the test
cases.
归纳是从观察到的训练案例推理出一般规则，然后将其应用于测试案例<br />
2.
转导推断：<font color="green">在逻辑、统计推断和有监督学习中，</font>transduction或transductive
inference指的是从观察到的特定（训练）案例到特定（测试）案例的推理</p>
<blockquote>
<p>简单说，一般的机器学习都是希望在训练数据集上学到使得期望风险最小的模型，当这类模型足够鲁棒时，其泛化能力足够强，即模型在相似数据上普遍适用；而转导学习的目标是学习一种在给定测试集上错误率最小的模型，在训练阶段可以利用测试集的信息</p>
</blockquote>
<h2 id="转导示例示意">转导示例/示意</h2>
<p>为了对比转导和归纳，在这里举例对比他们的一些特有的属性。 <img data-src="/images/Transformer/p2.png" alt="exa" />
在上图中，给出了一个点集，有些点已经被标注好了标签(A、B、C)，但是大部分点没有。目标：预测未被标注的标签信息</p>
<h3 id="inductive-inference">inductive inference</h3>
<p>解决此问题的归纳方法是使用标记点来训练监督学习算法，然后让它预测所有未标记点的标签。然而，对于这个问题，监督学习算法将只有五个标记点用作构建预测模型的基础。建立一个捕获这些数据结构的模型肯定会很困难。例如，如果使用最近邻算法，则中间附近的点将被标记为“A”或“C”，即使它们显然与标记为“B”的点属于同一簇。</p>
<h3 id="transductive-inference">transductive inference</h3>
<p>转导的优点是在执行标记任务时能够考虑所有点，而不仅仅是标记点。在这种情况下，传导算法将根据未标记点自然所属的簇来标记它们。因此，中间的点很可能被标记为“B”，因为它们非常靠近该簇。</p>
<h2 id="转导的适用场合和优缺点">转导的适用场合和优缺点</h2>
<p>转导的优点是它可以用更少的标记点做出更好的预测，因为它使用在未标记点中发现的自然中断。转导的一个缺点是它无法建立预测模型。如果将先前未知的点添加到集合中，则需要对所有点重复整个传导算法才能预测标签。如果数据在计算流中增量可用，则计算成本可能会很高。此外，这可能会导致一些旧点的预测发生变化（这可能是好是坏，具体取决于应用程序）。另一方面，监督学习算法可以立即标记新点，而计算成本非常低</p>
<h2 id="转导算法">转导算法</h2>
<p>转导算法可以大致分为两类：那些寻求将离散标签分配给未标记点的算法，以及那些寻求对未标记点回归连续标签的算法。寻求预测离散标签的算法往往是通过向聚类算法添加部分监督来导出的。可以使用两类算法：平面聚类和层次聚类。后者可以进一步细分为两类：通过划分进行聚类的聚类和通过聚集进行聚类的聚类。寻求预测连续标签的算法往往是通过向流形学习算法添加部分监督来导出的。</p>
<h3 id="partitioning-transduction">Partitioning transduction</h3>
<p>分区转导可以被认为是自上而下的转导。它是基于分区的聚类的半监督扩展。它通常按如下方式执行：将所有点的集合视为一个大分区。虽然任何分区
P 都包含两个具有冲突标签的点： 将 P 分区为更小的分区。对于每个分区 P：为
P 中的所有点分配相同的标签。</p>
<h3 id="agglomerative-transduction">Agglomerative transduction</h3>
<p>聚集转导可以被认为是自下而上的转导。它是凝聚聚类的半监督扩展。它通常按如下方式执行：计算所有点之间的成对距离
D。按升序对 D 进行排序。将每个点视为大小为 1 的簇。对于 D 中的每对点
{a,b}：如果（a 未标记）或（b 未标记）或（a 和 b
具有相同的标记），则合并包含 a 和 b
的两个簇。使用相同的标签标记合并簇中的所有点。</p>
<h3 id="manifold-transduction">Manifold transduction</h3>
<p>基于流形学习的转导仍然是一个非常年轻的研究领域。</p>
<h1 id="layer-normalization-batch-normalization">Layer Normalization
&amp; Batch Normalization</h1>
<h2 id="d情形">2D情形</h2>
<p>在统计推断基础部分得知，单个batch的样本可以表示为<span
class="math inline">\(X \in \mathbb{R}^{n \times
p}\)</span>，其中行向量组表示该batch内所有样本，列向量组表示所有特征。假定这里的batch都是预处理之后或某些layer的输出，则</p>
<ul>
<li><p>Batch normalization:假设<span class="math inline">\(X=\{f_1, f_2,
\dotsb, f_p\}, f_i = (x_{1i}, x_{2i}, \dotsb,
x_{ni})^T\)</span>，则特征均值<span
class="math inline">\(\bar{f_i}=\sum_{j=1}^n
x_{ji}\)</span>，特征标准差为<span
class="math inline">\(\sigma_i\)</span>,则batch
normalization所做的是<span
class="math inline">\(\frac{f_i-\bar{f_i}}{\sigma_i}\)</span>。更具体的，我们将该batch的均值和方差-协方差矩阵记作<span
class="math inline">\(\bar{X}\)</span>，<span
class="math inline">\(\frak{v}\)</span>，则<span
class="math inline">\(batch\_normal(X)=\frac{X-\bar{X}}{\frak{v}}\)</span>
<img data-src="/images/Transformer/p3.png" alt="2D-batch norm" /></p></li>
<li><p>Layer normalization:2D时的layer norm和batch
normal很相似，甚至计算如出一辙，只有细小区别，对比两图一目了然 <img data-src="/images/Transformer/p4.png" alt="2D-layer norm" /></p></li>
</ul>
<h2 id="时序3d情形">时序3D情形</h2>
<p>首先，由于文本之间拟存在时序关系，所以当每个短句抽象为一个seq，则在多个样本打包成一个batch时，其batch样本集结构如下图所示(假设每个短句，即seq的长度一样,当然现实是seq长度不一致时，可以选择补零策略)。这里，我们看到3D情形下的batch
norm差异性还是比较大的。一般情形下，考虑下面的情况，我们优先选择layer
norm:当一个长句成为该batch中的一个seq，显然在计算某个特征feature(q)的均值和方差时，其影响力就会比较大，则数据抖动就较大，这样是不好的；相反，在求单一样本的layer
norm时，则相对稳定。</p>
<figure>
<img data-src="/images/Transformer/p6.png" alt="时序3D-batch norm" />
<figcaption aria-hidden="true">时序3D-batch norm</figcaption>
</figure>
<figure>
<img data-src="/images/Transformer/p5.png" alt="时序3D-layer norm" />
<figcaption aria-hidden="true">时序3D-layer norm</figcaption>
</figure>
<h1 id="从论文出发">从论文出发</h1>
<p>鉴于本人的研究方向主要是图像识别，所以对NLP只能基于自己的粗浅理解和面向Google,XXXD</p>
<h2 id="背景">背景</h2>
<p>常见的并行计算模型(如，ByteNet和ConvS2S)中，使用了卷积神经网络作为基本的构建模块，在输入和输出位置并行计算隐藏特征表示，这确实解决了顺序计算/训练所带来的时间消耗，却带了这样的问题——学习远距离之间的特征依赖关系更加困难(操作/计算量激增)。相反，在Transformer中，这样的计算量被减少为恒定数量，<font color="red">代价是由于均值了注意力加权position而导致的有效分辨率降低</font>，在实现时，多头注意力机制可以抵消这种影响。</p>
<h2 id="自注意力机制">自注意力机制</h2>
<p>原文定义：self-attention, sometimes called intra-attention is an
attention mechanism relating different positions of a single sequence in
order to compute a representation of the sequence.
译作：自注意力，有时叫做内注意力，是一种注意力机制，用于单一序列中的不同位置，以计算序列的表示。</p>
<h2 id="不完全定义">不完全定义</h2>
<p>在这篇paper中，Transformer是一个<font color="red">完全依靠自注意力</font>进行输入和输出表示计算的<a
href="https://en.wikipedia.org/wiki/Transduction_(machine_learning)">转导模型</a>。在Transformer中，自注意力机制允许模型在编码和解码阶段同时对整个输入序列和输出序列进行注意力计算。这种并行性使得Transformer能够在整个数据集上进行推断，利用所有样本的信息来学习模型的表示能力。这种全局推断的性质使得Transformer更具转导模型的特点。</p>
<h2 id="框架解析">框架解析</h2>
<ul>
<li>转导模型</li>
<li>编码器-解码器结构</li>
<li>（自）注意力机制</li>
</ul>
<h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h3>
<ul>
<li>编码器：编码器由N个相同的层堆叠而成。每个层包含两个子层。第一个子层是一个多头自注意力机制，第二个子层是一个简单的position-wise的全连接前馈网络。在两个子层周围使用残差连接，并且每个子层的输出由于采用了残差连接和Layer
Normlization，所以当每层的输入是x时，其标准输出是LayerNorm(x+Sublayer(x)),这里的Sublayer指的是某个子层实现的函数功能；</li>
<li>解码器：解码器也由N个相同的层堆叠而成。除了每个编码器层中的两个子层外，解码器还插入第三个子层，该子层在编码器堆栈的输出上执行多头注意力。与编码器类似，每个子层周围使用残差连接，然后进行层归一化。此外还修改了解码器堆栈中的自注意力子层，以防止位置编码参与到后续的位置编码。</li>
</ul>
<blockquote>
<p>掩码：假如需要预测t+1时刻的分布或真值，需要依据t时刻所提供的真实数据对t+1时的分布（或真值）进行预测，而掩码使得在t时刻，预测者看不到t+1时刻的真值，从而保证训练和预测时的行为一致性（对真实情况未知）</p>
</blockquote>
<h3 id="attention">Attention</h3>
<p>注意力函数（Attention
function）可以描述成一个查询（query）和一组键-值（key-value）对映射到一个输出上，其中查询（query）、键（keys）、值（values）和输出都是向量。输出通过对值的加权求和来计算，其中分配给每个值的权重是通过查询与相应键的相似度（兼容性）函数计算得到的。即
<span class="math display">\[Attention(Q,K,V)=compatibility(Q,K) \cdot
V\]</span> 这里，compatibility是描述Q,K相似度的函数。</p>
<p>假设以2范距离来描述Q,K的相似度，即<span
class="math inline">\(compatibility(Q,K) \propto
\frac{1}{||Position_Q-Position_K||_2}\)</span>，则如下图所示，体现出了对向量V各分量的加权情形
<img data-src="/images/Transformer/p8.png" alt="compatibility function" />
可以看到，由于查询分量<span class="math inline">\(Q_i\)</span>离<span
class="math inline">\(k_1,k_2\)</span>比较近，所以对<span
class="math inline">\(v_1,v_2\)</span>加权比较大，相反对较远的V分量加权较小。</p>
<h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4>
<p>计算流程如下图所示 <img data-src="/images/Transformer/p7.png"
alt="Scaled-Dot Attention" /></p>
<p>基本形式： <span
class="math display">\[Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span></p>
<h4 id="multi-head-attention">Multi-Head Attention</h4>
<figure>
<img data-src="/images/Transformer/p9.png" alt="Multi-Head Attention" />
<figcaption aria-hidden="true">Multi-Head Attention</figcaption>
</figure>
<blockquote>
<p>为什么使用多头注意力机制？
类似于CNN的多通道卷积(每个通道都是图像的一个特征子空间，不过开始时不包含高级语义，需要通过多个filter【对应多头注意力里的多个线性投影下的关注】后，才能获得)。在Transformer中引入了h次注意力，由图可知，全连接的线性层将输入的(V,K,Q)进行降维（投影），然后通过h次的Scaled
Dot-Product
Attention获得h个模式Pattern，可以看到这样做可以挖掘更多的高级特征。从注意力的角度来说，就是多个头关注不同的position
dot，然后挖掘全局之于此处position的关联度。</p>
</blockquote>
<p>多头注意力的一般形式： <span class="math display">\[\begin{split}
MultiHead(Q,K,V)&amp;=Concat(head_1,\dots,head_h)W^O \\
where head_i &amp;= Attention(QW_i^Q,KW^K_i,VW^V_i)
\end{split}
\]</span> 这里，<span class="math inline">\(Q,K,V \in
\mathbb{R}^{d\_model},W_i^Q \in \mathbb{R}^{d\_model \times d_k}, W^K_i
\in \mathbb{R}^{d\_model \times d_k},W^V_i \in \mathbb{R}^{d\_model
\times d_v},i=\{1,2,\dots,h\}\)</span> and <span
class="math inline">\(W^O \in \mathbb{R}^{hd_v \times
d\_model}\)</span>。 可以看到，这里将Q,K,V通过h种投影矩阵<span
class="math inline">\((W_i^Q,W^K_i,W^V_i)\)</span>投影到了不同的子空间，从而在不同的子空间上进行关注。</p>
<p>多头注意力使模型能够同时关注不同位置的不同表示子空间中的信息。使用单个注意力头，平均操作会抑制这种能力。通过引入多个注意力头，模型可以并行地学习多个不同的查询、键和值的投影，从而在不同的表示子空间上进行关注，避免了信息的平均化。这样可以增强模型对输入的表征能力，提高模型处理复杂关系和学习长距离依赖的能力。通过多头注意力，模型可以捕捉更丰富的上下文信息，并且能够更好地处理输入中的多样性和复杂性。</p>
<h3 id="position-wise-feed-forward-networksmlp">Position-wise
Feed-Forward Networks(MLP)</h3>
<h4 id="基本形式">基本形式</h4>
<p><span class="math display">\[FFN(x)=max(0,xW_1+b_1)W_2+b_2\]</span>
实质：将来自Multi-Attention的输入交给一个线性层<span
class="math inline">\(Linear_1(x)=xW_1 +
b_1\)</span>,然后交给激活函数ReLU，最后在做线性变换<span
class="math inline">\(Linear_2(x)=xW_2+b_2\)</span></p>
<h4 id="position-encoding">Position Encoding</h4>
<p>RNN是逐步处理embedding的过程，是将t时刻的输出作为t时刻的输入的一部分考量。而Transformer则不同，他是在一开始就利用注意力机制考量不同embedding
vectors之间的相关度，所以在输入时位置编码显得比较重要，它会提供RNN中“逐步处理”这样的时序关系，来告诉模型每个embedding的位置(position)。</p>
<ul>
<li>论文中的位置编码： <span class="math display">\[
\begin{split}
  PE_{(pos,2i)} &amp;= sin(pos/10000^{2i/d\_model})\\
  PE_{(pos,2i+1)} &amp;= cos(pos/10000^{2i/d\_model})
\end{split}
\]</span></li>
</ul>
<h1 id="从代码出发">从代码出发</h1>
<p>前面的章节，已经介绍了Transformer的主要框架结构，包含了(自)注意力机制、（掩码）多头注意力机制、MLP以及位置编码和残差连接，但是若没有接触过自回归结构的模型，其实仍然很难明白，当一个embedding进到Encoder之后的具体流程如何？当然可以参考这个<a
href="https://www.bilibili.com/video/BV15v411W78M/?spm_id_from=333.999.0.0&amp;vd_source=03a1d15d9e2ed541e2d422d799de6c41">教程</a>，介绍了Attention的张量计算过程，并举实例将训练流程讲清楚了。</p>
<figure>
<img data-src="/images/Transformer/p10.png" alt="实例" />
<figcaption aria-hidden="true">实例</figcaption>
</figure>
<p>其中，&lt;bos&gt; 和 &lt;eos&gt;分别表示beginning of sentence和end of
sentence。但即便这样，依旧会错过很多细枝末节，不妨来看看代码实现。</p>
<h2 id="机翻代码示例">机翻代码示例</h2>
<p>代码链接：<a
href="https://github.com/wmathor/nlp-tutorial">github-code</a></p>
<p>代码框架：<img data-src="/images/Transformer/p11.png"
alt="code-struct" /></p>
<h2 id="transformer">Transformer</h2>
<p>Transfomer可以抽象为3部分——Encoders,Decoders和(Linear+Softmax)。其学习过程可以由function
“forward”得出,可以看到在最初</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        self.encoder = Encoder().cuda()</span><br><span class="line">        self.decoder = Decoder().cuda()</span><br><span class="line">        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=<span class="literal">False</span>).cuda()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, dec_inputs</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># tensor to store decoder outputs</span></span><br><span class="line">        <span class="comment"># outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">        enc_outputs, enc_self_attns = self.encoder(enc_inputs)</span><br><span class="line">        <span class="comment"># dec_outpus: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]</span></span><br><span class="line">        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)</span><br><span class="line">        dec_logits = self.projection(dec_outputs) <span class="comment"># dec_logits: [batch_size, tgt_len, tgt_vocab_size]</span></span><br><span class="line">        <span class="keyword">return</span> dec_logits.view(-<span class="number">1</span>, dec_logits.size(-<span class="number">1</span>)), enc_self_attns, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure>
<h2 id="encoder">Encoder</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.src_emb = nn.Embedding(src_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs) <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        enc_outputs = self.pos_emb(enc_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) <span class="comment"># [batch_size, src_len, src_len]</span></span><br><span class="line">        enc_self_attns = []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)</span><br><span class="line">            enc_self_attns.append(enc_self_attn)</span><br><span class="line">        <span class="keyword">return</span> enc_outputs, enc_self_attns</span><br></pre></td></tr></table></figure>
<h2 id="decoder">Decoder</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        enc_intpus: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        enc_outputs: [batsh_size, src_len, d_model]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs) <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_outputs = self.pos_emb(dec_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>).cuda() <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda() <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda() <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), <span class="number">0</span>).cuda() <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"></span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) <span class="comment"># [batc_size, tgt_len, src_len]</span></span><br><span class="line"></span><br><span class="line">        dec_self_attns, dec_enc_attns = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span></span><br><span class="line">            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)</span><br><span class="line">            dec_self_attns.append(dec_self_attn)</span><br><span class="line">            dec_enc_attns.append(dec_enc_attn)</span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure>
<h2 id="positional-encoding">Positional Encoding</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        x: [seq_len, batch_size, d_model]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<p>当然，位置编码这里的自由度比较高，很多论文也是引入了新的位置编码改善性能。</p>
<h1 id="vit">ViT</h1>
<p>从<a
href="https://arxiv.org/pdf/2010.11929.pdf">paper</a>的标题--TRANSFORMERS
FOR IMAGE RECOGNITION AT
SCALE，我们就清楚ViT仍然更适用于大数据集的视觉任务。当然，如果使用预训练模型，对下游任务进行参数微调，性能也会很好。</p>
<h2 id="cnn-transformer">CNN &amp; Transformer</h2>
<ul>
<li>当在数据规模相近的不带强制性正则的中小型数据集上(比如，ImageNet)采用ViT和ResNet,那么ViT的性能会劣于ResNet,主要原因：ViT缺乏CNN内在固有的Inductive
bias(简单理解为合理的先验知识)——translation
equivariance和locality。</li>
<li>CNN利用局部性原理挖掘高级特征，Transformer关注的是全局，通过多头实现基于全局的关联度的特征挖掘。</li>
</ul>
<h2 id="overview-of-vit">Overview of ViT</h2>
<figure>
<img data-src="/images/Transformer/ViT-model.png" alt="ViT Overview" />
<figcaption aria-hidden="true">ViT Overview</figcaption>
</figure>
<p>可以看到其结构相对于Transfomer的架构相对简单，原来的encoder-decoder结构将decoder抽象为了简单的MLP，且加入了可以学习的<code>classfication token</code>
(简称<code>cls token</code>)，当然也可以不加，但是需要在最后进行全局平均池化。</p>
<h2 id="method">Method</h2>
<ul>
<li>input:图像<span class="math inline">\({x} \in \mathbb{R}^{H \times W
\times C}\)</span></li>
<li>设定patch_size=P,将多通道图像打成2D,此时图像变为<span
class="math inline">\({x}_p \in \mathbb{R}^{N \times (P^2 \cdot C)
}\)</span>,其中<span
class="math inline">\(P^2\)</span>是每个patch的分辨率(图像大小)，C为通道数，则每个patch
token的大小为<span class="math inline">\(P^2 \cdot
C\)</span>，N为每个通道切割出的patch数，即<span
class="math inline">\(N=\frac{HW}{P^2}\)</span>,然后将多个patch
token打包成一个batch</li>
<li>假定batch size为N,则单个embedded batch可以描述为<span
class="math inline">\([x_p^1, x_p^2, \dotsb,
x_p^N]\)</span>,将其进行线性投影，投影矩阵为<span
class="math inline">\(E\)</span>，即作<span
class="math inline">\([x_p^1\textbf{E}, x_p^2\textbf{E}, \dotsb,
x_p^N\textbf{E}]\)</span></li>
<li>concat拼接<code>cls token</code>,<span
class="math inline">\([x_{class}, x_p^1\textbf{E}, x_p^2\textbf{E},
\dotsb, x_p^N\textbf{E}]\)</span></li>
<li>加上位置编码<span class="math inline">\(E_{pos}\)</span></li>
</ul>
<h1 id="code-of-vitrgb">Code of ViTRGB</h1>
<ol type="1">
<li><p>导包和shape定型 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, repeat</span><br><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pair</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t <span class="keyword">if</span> <span class="built_in">isinstance</span>(t, <span class="built_in">tuple</span>) <span class="keyword">else</span> (t, t)</span><br></pre></td></tr></table></figure></p></li>
<li><p>工具类</p></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; LayerNorm &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PreNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)</span><br><span class="line">        self.fn = fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, **kwargs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fn(self.norm(x), **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; 引入dropout和GELU的FFN(MLP) &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(hidden_dim, dim),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot; 注意力机制 &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="comment"># dim--输入特征x的维度，heads为头数，dim_head等于每个头的维度，dropout的引入防止过拟合</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads=<span class="number">8</span>, dim_head=<span class="number">64</span>, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head * heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dim_head == dim)</span><br><span class="line"></span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dim_head**-<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.attend = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.to_out = (nn.Sequential(nn.Linear(inner_dim, dim),</span><br><span class="line">                                     nn.Dropout(dropout))</span><br><span class="line">                       <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x.shape:[batch_size, seq_length, dim]</span></span><br><span class="line">        <span class="comment"># 输入特征x映射为q，k，v</span></span><br><span class="line">        <span class="comment"># to_qkv(x)之后的shape:[batch_size, seq_length, inner_dim x 3]</span></span><br><span class="line">        <span class="comment"># chunk均分之后shape:[batch_size, seq_length, inner_dim]</span></span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># rearrange重塑输入的张量结构，将lamnda表达式的输入张量t(这里是可迭代对象qkv)进行shape reorder</span></span><br><span class="line">        <span class="comment"># 通过map的迭代reorder之后，q,k,v的shape是由[batch_size, seq_length, inner_dim]变为[batch_size, heads, seq_length, dim_head]</span></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(</span><br><span class="line">            <span class="keyword">lambda</span> t: rearrange(t, <span class="string">&quot;b n (h d) -&gt; b h n d&quot;</span>, h=self.heads), qkv)</span><br><span class="line">        <span class="comment"># 标准化注意力矩阵，shape:[batch_size, heads, seq_length, seq_length]</span></span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * self.scale</span><br><span class="line">        <span class="comment"># 取得注意力机制中相关性系数</span></span><br><span class="line">        attn = self.attend(dots)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [batch_size, heads, seq_length, dim_head]</span></span><br><span class="line">        out = torch.matmul(attn, v)</span><br><span class="line">        <span class="comment"># [batch_size, seq_length, dim]</span></span><br><span class="line">        out = rearrange(out, <span class="string">&quot;b h n d -&gt; b n (h d)&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="comment"># dim--embedding/feature,depth--depth of encoder stack</span></span><br><span class="line">    <span class="comment"># dim_head--dimension of each head, mlp_dim--dimension of hidden layer</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            self.layers.append(</span><br><span class="line">                nn.ModuleList([</span><br><span class="line">                    PreNorm(</span><br><span class="line">                        dim,</span><br><span class="line">                        Attention(dim,</span><br><span class="line">                                  heads=heads,</span><br><span class="line">                                  dim_head=dim_head,</span><br><span class="line">                                  dropout=dropout),</span><br><span class="line">                    ),</span><br><span class="line">                    PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)),</span><br><span class="line">                ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># 残差结构</span></span><br><span class="line">            x = attn(x) + x</span><br><span class="line">            x = ff(x) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>ViTRGB</li>
</ol>
<p>这里，position embedding为随机初始化的张量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; Vision Transformer &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ViTRGB</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        *,</span></span><br><span class="line"><span class="params">        image_size,</span></span><br><span class="line"><span class="params">        patch_size,</span></span><br><span class="line"><span class="params">        num_classes,</span></span><br><span class="line"><span class="params">        dim,  <span class="comment"># 特征维度/token size</span></span></span><br><span class="line"><span class="params">        depth,  <span class="comment"># depth of encoder stack</span></span></span><br><span class="line"><span class="params">        heads,</span></span><br><span class="line"><span class="params">        mlp_dim,</span></span><br><span class="line"><span class="params">        pool=<span class="string">&quot;cls&quot;</span>,  <span class="comment"># option:(&#x27;cls&#x27;, &#x27;mean&#x27;),池化类型:&#x27;cls&#x27;表示选择classification token, &#x27;mean&#x27;表示选择全局平均池化 </span></span></span><br><span class="line"><span class="params">        channels=<span class="number">3</span>,  <span class="comment"># 图像通道数，默认RGB-3</span></span></span><br><span class="line"><span class="params">        dim_head=<span class="number">64</span>,  <span class="comment"># dimension of each head</span></span></span><br><span class="line"><span class="params">        dropout=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        emb_dropout=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        pixelwise=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 1D --&gt; 2D shape</span></span><br><span class="line">        image_height, image_width = pair(image_size)</span><br><span class="line">        patch_height, patch_width = pair(patch_size)</span><br><span class="line"></span><br><span class="line">        self.pixelwise = pixelwise</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 裁剪为同样大小的patches</span></span><br><span class="line">        self.image_height = image_height</span><br><span class="line">        self.image_width = image_width</span><br><span class="line">        self.patch_height = patch_height</span><br><span class="line">        self.patch_width = patch_width</span><br><span class="line">        self.num_patches_height = image_height // patch_height</span><br><span class="line">        self.num_patches_width = image_width // patch_width</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> (image_height % patch_height == <span class="number">0</span> <span class="keyword">and</span> image_width % patch_width</span><br><span class="line">                == <span class="number">0</span>), <span class="string">&quot;Image dimensions must be divisible by the patch size.&quot;</span></span><br><span class="line"></span><br><span class="line">        num_patches = (image_height // patch_height) * (image_width //</span><br><span class="line">                                                        patch_width)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># patch_dim 其实就是变为token之前的维度，之后经过MLP,将patch_dim变成token size(即下面dim)</span></span><br><span class="line">        patch_dim = channels * patch_height * patch_width</span><br><span class="line">        <span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;</span><br><span class="line">            <span class="string">&quot;cls&quot;</span>,</span><br><span class="line">            <span class="string">&quot;mean&quot;</span>,</span><br><span class="line">        &#125;, <span class="string">&quot;pool type must be either cls (cls token) or mean (mean pooling)&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># reorder the tensor corresponding to the shape of image</span></span><br><span class="line">        <span class="comment"># image shape:[batch_size, channels, height, width]</span></span><br><span class="line">        <span class="comment"># --&gt; image shape:[batch_size, channels, h x patch_height, w x patch_width]</span></span><br><span class="line">        <span class="comment"># --&gt; iamge shape:[batch_size, h x w, patch_height x patch_width x channels]</span></span><br><span class="line">        self.to_patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(</span><br><span class="line">                <span class="string">&quot;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&quot;</span>,</span><br><span class="line">                p1=patch_height,</span><br><span class="line">                p2=patch_width,</span><br><span class="line">            ),</span><br><span class="line">            nn.LayerNorm(patch_dim),</span><br><span class="line">            nn.Linear(patch_dim, dim),</span><br><span class="line">            nn.LayerNorm(dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 注意这里是num_patches + 1，所以考虑到cls token的位置编码</span></span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))</span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))</span><br><span class="line">        self.dropout = nn.Dropout(emb_dropout)</span><br><span class="line"></span><br><span class="line">        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim,</span><br><span class="line">                                       dropout)</span><br><span class="line"></span><br><span class="line">        self.pool = pool</span><br><span class="line">        self.to_latent = nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.mlp_head = nn.Sequential(nn.LayerNorm(dim),</span><br><span class="line">                                      nn.Linear(dim, num_classes))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 像素级</span></span><br><span class="line">        <span class="keyword">if</span> self.pixelwise:</span><br><span class="line">            self.mlp_head = nn.Sequential(</span><br><span class="line">                nn.LayerNorm(dim),</span><br><span class="line">                nn.Linear(dim, num_classes * patch_height * patch_width),</span><br><span class="line">                Rearrange(</span><br><span class="line">                    <span class="string">&quot;b h w (p1 p2 num_classes) -&gt; b (h p1) (w p2) num_classes&quot;</span>,</span><br><span class="line">                    p1=patch_height,</span><br><span class="line">                    p2=patch_width,</span><br><span class="line">                    num_classes=num_classes,</span><br><span class="line">                ),</span><br><span class="line">                <span class="comment"># reorder shape,交换dim=1和最后一维</span></span><br><span class="line">                MoveAxis((-<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;patch_height=&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;patch_width=&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;num_classes=&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="comment"># patch embedding, x.shape:[batch_size, h x w, patch_height x patch_width x channels]</span></span><br><span class="line">        x = self.to_patch_embedding(img)</span><br><span class="line">        <span class="comment"># b:batch size, n: number of samples(n = h x w)</span></span><br><span class="line">        b, n, _ = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个batch需要补上cls token，所以复制b份</span></span><br><span class="line">        cls_tokens = repeat(self.cls_token, <span class="string">&quot;1 1 d -&gt; b 1 d&quot;</span>, b=b)</span><br><span class="line">        <span class="comment"># x.shape after concat:[batch_size, h x w + 1, patch_height x patch_width x channels]</span></span><br><span class="line">        <span class="comment"># 注意这里和x = torch.cat((x,cls_token), dim=1)有区别，谁在前，谁就在低维</span></span><br><span class="line">        x = torch.cat(</span><br><span class="line">            (cls_tokens, x), dim=<span class="number">1</span></span><br><span class="line">        )  <span class="comment"># cls_token在前，所以cls token在[:,0]的位置，patch embedding的位置是[:,1:]</span></span><br><span class="line">        <span class="comment"># 加上posistion embedding</span></span><br><span class="line">        x += self.pos_embedding[:, :(n + <span class="number">1</span>)]</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># x抛进Transfomer</span></span><br><span class="line">        x = self.transformer(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># x = x.mean(dim = 1) if self.pool == &#x27;mean&#x27; else x[:, 0]</span></span><br><span class="line">        x = x[:, <span class="number">1</span>:, :]  <span class="comment"># remove cls..</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 去除cls token之后的shape:[batch_size, h x w, patch_height x patch_width x channels]</span></span><br><span class="line">        x = self.to_latent(x)</span><br><span class="line">        x = rearrange(</span><br><span class="line">            x,</span><br><span class="line">            <span class="string">&quot;b (h w) d -&gt; b h w d&quot;</span>,</span><br><span class="line">            h=self.num_patches_height,</span><br><span class="line">            w=self.num_patches_width,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> self.mlp_head(x)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Model</tag>
      </tags>
  </entry>
</search>
