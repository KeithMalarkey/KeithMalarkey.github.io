<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"keithmalarkey.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.16.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Introduction Transformer因为更好的并行计算能力，以及在大样本下更好的表现性能，已经在NLP领域得到了广泛运用，并取得了很多成效。随着ViT的出现，视觉领域也得到了重大突破。考虑到相关研究方法的主要背景然是深度学习和图像识别，所以主要是卷积神经网络做比较。 提问  What is the Transformer？其基本结构是什么？  什么是“平均注意力加权位置”(">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer &amp; ViT">
<meta property="og:url" content="https://keithmalarkey.github.io/2023/07/06/Transformer/index.html">
<meta property="og:site_name" content="Keith&#39;s Blog">
<meta property="og:description" content="Introduction Transformer因为更好的并行计算能力，以及在大样本下更好的表现性能，已经在NLP领域得到了广泛运用，并取得了很多成效。随着ViT的出现，视觉领域也得到了重大突破。考虑到相关研究方法的主要背景然是深度学习和图像识别，所以主要是卷积神经网络做比较。 提问  What is the Transformer？其基本结构是什么？  什么是“平均注意力加权位置”(">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://keithmalarkey.github.io/images/Transformer/p1.png">
<meta property="og:image" content="https://keithmalarkey.github.io/images/Transformer/p3.png">
<meta property="og:image" content="https://keithmalarkey.github.io/images/Transformer/p6.png">
<meta property="og:image" content="https://keithmalarkey.github.io/images/Transformer/p5.png">
<meta property="og:image" content="https://keithmalarkey.github.io/images/Transformer/p8.png">
<meta property="og:image" content="https://keithmalarkey.github.io/images/Transformer/p7.png">
<meta property="og:image" content="https://keithmalarkey.github.io/images/Transformer/p9.png">
<meta property="og:image" content="https://keithmalarkey.github.io/images/Transformer/p10.png">
<meta property="og:image" content="https://keithmalarkey.github.io/images/Transformer/p11.png">
<meta property="og:image" content="https://keithmalarkey.github.io/images/Transformer/ViT-model.png">
<meta property="article:published_time" content="2023-07-06T06:58:44.051Z">
<meta property="article:modified_time" content="2023-08-06T12:55:57.241Z">
<meta property="article:author" content="Keith Malarkey">
<meta property="article:tag" content="Model">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://keithmalarkey.github.io/images/Transformer/p1.png">


<link rel="canonical" href="https://keithmalarkey.github.io/2023/07/06/Transformer/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://keithmalarkey.github.io/2023/07/06/Transformer/","path":"2023/07/06/Transformer/","title":"Transformer & ViT"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Transformer & ViT | Keith's Blog</title>
  







<link rel="dns-prefetch" href="waline4test-af5tulsko-keithmalarkey.vercel.app">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Keith's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">欲买桂花同载酒，终不似，少年游</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8F%90%E9%97%AE"><span class="nav-number">2.</span> <span class="nav-text">提问</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">3.</span> <span class="nav-text">参考</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6"><span class="nav-number">4.</span> <span class="nav-text">模型框架</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8E%A8%E6%96%AD%E5%88%86%E7%B1%BB"><span class="nav-number">5.</span> <span class="nav-text">推断分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#transduction-induction"><span class="nav-number">5.1.</span> <span class="nav-text">transduction &amp; induction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AC%E5%AF%BC%E7%A4%BA%E4%BE%8B%E7%A4%BA%E6%84%8F"><span class="nav-number">5.2.</span> <span class="nav-text">转导示例&#x2F;示意</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#inductive-inference"><span class="nav-number">5.2.1.</span> <span class="nav-text">inductive inference</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transductive-inference"><span class="nav-number">5.2.2.</span> <span class="nav-text">transductive inference</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AC%E5%AF%BC%E7%9A%84%E9%80%82%E7%94%A8%E5%9C%BA%E5%90%88%E5%92%8C%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-number">5.3.</span> <span class="nav-text">转导的适用场合和优缺点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AC%E5%AF%BC%E7%AE%97%E6%B3%95"><span class="nav-number">5.4.</span> <span class="nav-text">转导算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#partitioning-transduction"><span class="nav-number">5.4.1.</span> <span class="nav-text">Partitioning transduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#agglomerative-transduction"><span class="nav-number">5.4.2.</span> <span class="nav-text">Agglomerative transduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#manifold-transduction"><span class="nav-number">5.4.3.</span> <span class="nav-text">Manifold transduction</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#layer-normalization-batch-normalization"><span class="nav-number">6.</span> <span class="nav-text">Layer Normalization
&amp; Batch Normalization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#d%E6%83%85%E5%BD%A2"><span class="nav-number">6.1.</span> <span class="nav-text">2D情形</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F3d%E6%83%85%E5%BD%A2"><span class="nav-number">6.2.</span> <span class="nav-text">时序3D情形</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%8E%E8%AE%BA%E6%96%87%E5%87%BA%E5%8F%91"><span class="nav-number">7.</span> <span class="nav-text">从论文出发</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">7.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">7.2.</span> <span class="nav-text">自注意力机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E5%AE%8C%E5%85%A8%E5%AE%9A%E4%B9%89"><span class="nav-number">7.3.</span> <span class="nav-text">不完全定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90"><span class="nav-number">7.4.</span> <span class="nav-text">框架解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#encoder-and-decoder-stacks"><span class="nav-number">7.4.1.</span> <span class="nav-text">Encoder and Decoder Stacks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention"><span class="nav-number">7.4.2.</span> <span class="nav-text">Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#scaled-dot-product-attention"><span class="nav-number">7.4.2.1.</span> <span class="nav-text">Scaled Dot-Product Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#multi-head-attention"><span class="nav-number">7.4.2.2.</span> <span class="nav-text">Multi-Head Attention</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#position-wise-feed-forward-networksmlp"><span class="nav-number">7.4.3.</span> <span class="nav-text">Position-wise
Feed-Forward Networks(MLP)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%BD%A2%E5%BC%8F"><span class="nav-number">7.4.3.1.</span> <span class="nav-text">基本形式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#position-encoding"><span class="nav-number">7.4.3.2.</span> <span class="nav-text">Position Encoding</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%8E%E4%BB%A3%E7%A0%81%E5%87%BA%E5%8F%91"><span class="nav-number">8.</span> <span class="nav-text">从代码出发</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E7%BF%BB%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="nav-number">8.1.</span> <span class="nav-text">机翻代码示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformer"><span class="nav-number">8.2.</span> <span class="nav-text">Transformer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#encoder"><span class="nav-number">8.3.</span> <span class="nav-text">Encoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decoder"><span class="nav-number">8.4.</span> <span class="nav-text">Decoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#positional-encoding"><span class="nav-number">8.5.</span> <span class="nav-text">Positional Encoding</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#vit"><span class="nav-number">9.</span> <span class="nav-text">ViT</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#cnn-transformer"><span class="nav-number">9.1.</span> <span class="nav-text">CNN &amp; Transformer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#overview-of-vit"><span class="nav-number">9.2.</span> <span class="nav-text">Overview of ViT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#method"><span class="nav-number">9.3.</span> <span class="nav-text">Method</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#code-of-vitrgb"><span class="nav-number">10.</span> <span class="nav-text">Code of ViTRGB</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%A7%92%E5%BA%A6"><span class="nav-number">11.</span> <span class="nav-text">总结(计算机视觉角度)</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Keith Malarkey"
      src="/images/avatar.jpeg">
  <p class="site-author-name" itemprop="name">Keith Malarkey</p>
  <div class="site-description" itemprop="description">技术贴 + 数学/算法笔记 + vim/neovim初级爱好者</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/KeithMalarkey" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;KeithMalarkey" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/1173351470?spm_id_from=333.1007.0.0" title="bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;1173351470?spm_id_from&#x3D;333.1007.0.0" rel="noopener me" target="_blank"><i class="fa bilibili fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/keithmalarkey" title="zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;keithmalarkey" rel="noopener me" target="_blank"><i class="fa zhihu fa-fw"></i></a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://keithmalarkey.github.io/2023/07/06/Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpeg">
      <meta itemprop="name" content="Keith Malarkey">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Keith's Blog">
      <meta itemprop="description" content="技术贴 + 数学/算法笔记 + vim/neovim初级爱好者">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Transformer & ViT | Keith's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Transformer & ViT
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-07-06 14:58:44" itemprop="dateCreated datePublished" datetime="2023-07-06T14:58:44+08:00">2023-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-08-06 20:55:57" itemprop="dateModified" datetime="2023-08-06T20:55:57+08:00">2023-08-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline：</span>
  
    <a title="waline" href="/2023/07/06/Transformer/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2023/07/06/Transformer/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>6.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>22 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="introduction">Introduction</h1>
<p>Transformer因为更好的并行计算能力，以及在大样本下更好的表现性能，已经在NLP领域得到了广泛运用，并取得了很多成效。随着ViT的出现，视觉领域也得到了重大突破。考虑到相关研究方法的主要背景然是深度学习和图像识别，所以主要是卷积神经网络做比较。</p>
<h1 id="提问">提问</h1>
<ol type="1">
<li>What is the Transformer？其基本结构是什么？
<ol type="1">
<li>什么是“平均注意力加权位置”(average attention-weighted position)</li>
<li>为什么平均注意力加权位置会导致有效分辨率降低？</li>
</ol></li>
<li>注意力机制？自注意力机制？两者区别？</li>
<li>ViT是如何将Transformer移植到视觉领域的？</li>
<li>ViT的特点(优缺点)？它和CNN的区别和各自的优势</li>
</ol>
<h1 id="参考">参考</h1>
<ul>
<li><strong>Transformer</strong></li>
</ul>
<ol type="1">
<li>Attention Is All You Need, Neural Information Processing Systems
(Neural IPS 2017);</li>
<li>zhihu---<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/411311520">Transformer详解</a>;</li>
<li><a
target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0">Transformer精读-李沐</a></li>
<li><a
target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1q3411U7Hi/?spm_id_from=333.999.0.0">白话教程1</a></li>
<li><a
target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV15v411W78M/?spm_id_from=333.999.0.0&amp;vd_source=03a1d15d9e2ed541e2d422d799de6c41">白话教程2</a></li>
<li><a
target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Qg4y1P7r4/?spm_id_from=333.788.recommend_more_video.1&amp;vd_source=03a1d15d9e2ed541e2d422d799de6c41">Transformer中的数据流动(前向过程)</a>---<font color="red">全场最佳</font></li>
</ol>
<ul>
<li><strong>Vision Transformer</strong></li>
</ul>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929.pdf">AN IAMGE IS WORTH 16
<span class="math inline">\(\times\)</span> 16 TRANSFORMERS FOR IMAGE
RECOGNITION AT SCALE</a></li>
<li><a
target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV15P4y137jb/?spm_id_from=333.999.0.0&amp;vd_source=03a1d15d9e2ed541e2d422d799de6c41">ViT解读</a>--by<a
target="_blank" rel="noopener" href="https://bryanyzhu.github.io/">朱毅</a></li>
</ol>
<h1 id="模型框架">模型框架</h1>
<ul>
<li>left:encoder <span class="math inline">\(\times N\)</span></li>
<li>right:decoder <span class="math inline">\(\times N\)</span></li>
<li>computing the loss compared with the GT after prediction by
softmax(Linear(decoder_output)) <img data-src="/images/Transformer/p1.png"
alt="Transformer Architecture" /></li>
</ul>
<h1 id="推断分类">推断分类</h1>
<h2 id="transduction-induction">transduction &amp; induction</h2>
<p>推断主要分为两类：<br />
1. 归纳推断(inductive inference): induction is reasoning from observed
training cases to general rules, which are then applied to the test
cases.
归纳是从观察到的训练案例推理出一般规则，然后将其应用于测试案例<br />
2.
转导推断：<font color="green">在逻辑、统计推断和有监督学习中，</font>transduction或transductive
inference指的是从观察到的特定（训练）案例到特定（测试）案例的推理</p>
<blockquote>
<p>简单说，一般的机器学习都是希望在训练数据集上学到使得期望风险最小的模型，当这类模型足够鲁棒时，其泛化能力足够强，即模型在相似数据上普遍适用；而转导学习的目标是学习一种在给定测试集上错误率最小的模型，在训练阶段可以利用测试集的信息</p>
</blockquote>
<h2 id="转导示例示意">转导示例/示意</h2>
<p>为了对比转导和归纳，在这里举例对比他们的一些特有的属性。 <img data-src="/images/Transformer/p2.png" alt="exa" />
在上图中，给出了一个点集，有些点已经被标注好了标签(A、B、C)，但是大部分点没有。目标：预测未被标注的标签信息</p>
<h3 id="inductive-inference">inductive inference</h3>
<p>解决此问题的归纳方法是使用标记点来训练监督学习算法，然后让它预测所有未标记点的标签。然而，对于这个问题，监督学习算法将只有五个标记点用作构建预测模型的基础。建立一个捕获这些数据结构的模型肯定会很困难。例如，如果使用最近邻算法，则中间附近的点将被标记为“A”或“C”，即使它们显然与标记为“B”的点属于同一簇。</p>
<h3 id="transductive-inference">transductive inference</h3>
<p>转导的优点是在执行标记任务时能够考虑所有点，而不仅仅是标记点。在这种情况下，传导算法将根据未标记点自然所属的簇来标记它们。因此，中间的点很可能被标记为“B”，因为它们非常靠近该簇。</p>
<h2 id="转导的适用场合和优缺点">转导的适用场合和优缺点</h2>
<p>转导的优点是它可以用更少的标记点做出更好的预测，因为它使用在未标记点中发现的自然中断。转导的一个缺点是它无法建立预测模型。如果将先前未知的点添加到集合中，则需要对所有点重复整个传导算法才能预测标签。如果数据在计算流中增量可用，则计算成本可能会很高。此外，这可能会导致一些旧点的预测发生变化（这可能是好是坏，具体取决于应用程序）。另一方面，监督学习算法可以立即标记新点，而计算成本非常低</p>
<h2 id="转导算法">转导算法</h2>
<p>转导算法可以大致分为两类：那些寻求将离散标签分配给未标记点的算法，以及那些寻求对未标记点回归连续标签的算法。寻求预测离散标签的算法往往是通过向聚类算法添加部分监督来导出的。可以使用两类算法：平面聚类和层次聚类。后者可以进一步细分为两类：通过划分进行聚类的聚类和通过聚集进行聚类的聚类。寻求预测连续标签的算法往往是通过向流形学习算法添加部分监督来导出的。</p>
<h3 id="partitioning-transduction">Partitioning transduction</h3>
<p>分区转导可以被认为是自上而下的转导。它是基于分区的聚类的半监督扩展。它通常按如下方式执行：将所有点的集合视为一个大分区。虽然任何分区
P 都包含两个具有冲突标签的点： 将 P 分区为更小的分区。对于每个分区 P：为
P 中的所有点分配相同的标签。</p>
<h3 id="agglomerative-transduction">Agglomerative transduction</h3>
<p>聚集转导可以被认为是自下而上的转导。它是凝聚聚类的半监督扩展。它通常按如下方式执行：计算所有点之间的成对距离
D。按升序对 D 进行排序。将每个点视为大小为 1 的簇。对于 D 中的每对点
{a,b}：如果（a 未标记）或（b 未标记）或（a 和 b
具有相同的标记），则合并包含 a 和 b
的两个簇。使用相同的标签标记合并簇中的所有点。</p>
<h3 id="manifold-transduction">Manifold transduction</h3>
<p>基于流形学习的转导仍然是一个非常年轻的研究领域。</p>
<h1 id="layer-normalization-batch-normalization">Layer Normalization
&amp; Batch Normalization</h1>
<h2 id="d情形">2D情形</h2>
<p>在统计推断基础部分得知，单个batch的样本可以表示为<span
class="math inline">\(X \in \mathbb{R}^{n \times
p}\)</span>，其中行向量组表示该batch内所有样本，列向量组表示所有特征。假定这里的batch都是预处理之后或某些layer的输出，则</p>
<ul>
<li><p>Batch normalization:假设<span class="math inline">\(X=\{f_1, f_2,
\dotsb, f_p\}, f_i = (x_{1i}, x_{2i}, \dotsb,
x_{ni})^T\)</span>，则特征均值<span
class="math inline">\(\bar{f_i}=\sum_{j=1}^n
x_{ji}\)</span>，特征标准差为<span
class="math inline">\(\sigma_i\)</span>,则batch
normalization所做的是<span
class="math inline">\(\frac{f_i-\bar{f_i}}{\sigma_i}\)</span>。更具体的，我们将该batch的均值和方差-协方差矩阵记作<span
class="math inline">\(\bar{X}\)</span>，<span
class="math inline">\(\frak{v}\)</span>，则<span
class="math inline">\(batch\_normal(X)=\frac{X-\bar{X}}{\frak{v}}\)</span>
<img data-src="/images/Transformer/p3.png" alt="2D-batch norm" /></p></li>
<li><p>Layer normalization:2D时的layer norm和batch
normal很相似，甚至计算如出一辙，只有细小区别，对比两图一目了然 <img data-src="/images/Transformer/p4.png" alt="2D-layer norm" /></p></li>
</ul>
<h2 id="时序3d情形">时序3D情形</h2>
<p>首先，由于文本之间拟存在时序关系，所以当每个短句抽象为一个seq，则在多个样本打包成一个batch时，其batch样本集结构如下图所示(假设每个短句，即seq的长度一样,当然现实是seq长度不一致时，可以选择补零策略)。这里，我们看到3D情形下的batch
norm差异性还是比较大的。一般情形下，考虑下面的情况，我们优先选择layer
norm:当一个长句成为该batch中的一个seq，显然在计算某个特征feature(q)的均值和方差时，其影响力就会比较大，则数据抖动就较大，这样是不好的；相反，在求单一样本的layer
norm时，则相对稳定。</p>
<figure>
<img data-src="/images/Transformer/p6.png" alt="时序3D-batch norm" />
<figcaption aria-hidden="true">时序3D-batch norm</figcaption>
</figure>
<figure>
<img data-src="/images/Transformer/p5.png" alt="时序3D-layer norm" />
<figcaption aria-hidden="true">时序3D-layer norm</figcaption>
</figure>
<h1 id="从论文出发">从论文出发</h1>
<p>鉴于本人的研究方向主要是图像识别，所以对NLP只能基于自己的粗浅理解和面向Google,XXXD</p>
<h2 id="背景">背景</h2>
<p>常见的并行计算模型(如，ByteNet和ConvS2S)中，使用了卷积神经网络作为基本的构建模块，在输入和输出位置并行计算隐藏特征表示，这确实解决了顺序计算/训练所带来的时间消耗，却带了这样的问题——学习远距离之间的特征依赖关系更加困难(操作/计算量激增)。相反，在Transformer中，这样的计算量被减少为恒定数量，<font color="red">代价是由于均值了注意力加权position而导致的有效分辨率降低</font>，在实现时，多头注意力机制可以抵消这种影响。</p>
<h2 id="自注意力机制">自注意力机制</h2>
<p>原文定义：self-attention, sometimes called intra-attention is an
attention mechanism relating different positions of a single sequence in
order to compute a representation of the sequence.
译作：自注意力，有时叫做内注意力，是一种注意力机制，用于单一序列中的不同位置，以计算序列的表示。</p>
<h2 id="不完全定义">不完全定义</h2>
<p>在这篇paper中，Transformer是一个<font color="red">完全依靠自注意力</font>进行输入和输出表示计算的<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Transduction_(machine_learning)">转导模型</a>。在Transformer中，自注意力机制允许模型在编码和解码阶段同时对整个输入序列和输出序列进行注意力计算。这种并行性使得Transformer能够在整个数据集上进行推断，利用所有样本的信息来学习模型的表示能力。这种全局推断的性质使得Transformer更具转导模型的特点。</p>
<h2 id="框架解析">框架解析</h2>
<ul>
<li>转导模型</li>
<li>编码器-解码器结构</li>
<li>（自）注意力机制</li>
</ul>
<h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h3>
<ul>
<li>编码器：编码器由N个相同的层堆叠而成。每个层包含两个子层。第一个子层是一个多头自注意力机制，第二个子层是一个简单的position-wise的全连接前馈网络。在两个子层周围使用残差连接，并且每个子层的输出由于采用了残差连接和Layer
Normlization，所以当每层的输入是x时，其标准输出是LayerNorm(x+Sublayer(x)),这里的Sublayer指的是某个子层实现的函数功能；</li>
<li>解码器：解码器也由N个相同的层堆叠而成。除了每个编码器层中的两个子层外，解码器还插入第三个子层，该子层在编码器堆栈的输出上执行多头注意力。与编码器类似，每个子层周围使用残差连接，然后进行层归一化。此外还修改了解码器堆栈中的自注意力子层，以防止位置编码参与到后续的位置编码。</li>
</ul>
<blockquote>
<p>掩码：假如需要预测t+1时刻的分布或真值，需要依据t时刻所提供的真实数据对t+1时的分布（或真值）进行预测，而掩码使得在t时刻，预测者看不到t+1时刻的真值，从而保证训练和预测时的行为一致性（对真实情况未知）</p>
</blockquote>
<h3 id="attention">Attention</h3>
<p>注意力函数（Attention
function）可以描述成一个查询（query）和一组键-值（key-value）对映射到一个输出上，其中查询（query）、键（keys）、值（values）和输出都是向量。输出通过对值的加权求和来计算，其中分配给每个值的权重是通过查询与相应键的相似度（兼容性）函数计算得到的。即
<span class="math display">\[Attention(Q,K,V)=compatibility(Q,K) \cdot
V\]</span> 这里，compatibility是描述Q,K相似度的函数。</p>
<p>假设以2范距离来描述Q,K的相似度，即<span
class="math inline">\(compatibility(Q,K) \propto
\frac{1}{||Position_Q-Position_K||_2}\)</span>，则如下图所示，体现出了对向量V各分量的加权情形
<img data-src="/images/Transformer/p8.png" alt="compatibility function" />
可以看到，由于查询分量<span class="math inline">\(Q_i\)</span>离<span
class="math inline">\(k_1,k_2\)</span>比较近，所以对<span
class="math inline">\(v_1,v_2\)</span>加权比较大，相反对较远的V分量加权较小。</p>
<h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4>
<p>计算流程如下图所示 <img data-src="/images/Transformer/p7.png"
alt="Scaled-Dot Attention" /></p>
<p>基本形式： <span
class="math display">\[Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span></p>
<h4 id="multi-head-attention">Multi-Head Attention</h4>
<figure>
<img data-src="/images/Transformer/p9.png" alt="Multi-Head Attention" />
<figcaption aria-hidden="true">Multi-Head Attention</figcaption>
</figure>
<blockquote>
<p>为什么使用多头注意力机制？
类似于CNN的多通道卷积(每个通道都是图像的一个特征子空间，不过开始时不包含高级语义，需要通过多个filter【对应多头注意力里的多个线性投影下的关注】后，才能获得)。在Transformer中引入了h次注意力，由图可知，全连接的线性层将输入的(V,K,Q)进行降维（投影），然后通过h次的Scaled
Dot-Product
Attention获得h个模式Pattern，可以看到这样做可以挖掘更多的高级特征。从注意力的角度来说，就是多个头关注不同的position
dot，然后挖掘全局之于此处position的关联度。</p>
</blockquote>
<p>多头注意力的一般形式： <span class="math display">\[\begin{split}
MultiHead(Q,K,V)&amp;=Concat(head_1,\dots,head_h)W^O \\
where head_i &amp;= Attention(QW_i^Q,KW^K_i,VW^V_i)
\end{split}
\]</span> 这里，<span class="math inline">\(Q,K,V \in
\mathbb{R}^{d\_model},W_i^Q \in \mathbb{R}^{d\_model \times d_k}, W^K_i
\in \mathbb{R}^{d\_model \times d_k},W^V_i \in \mathbb{R}^{d\_model
\times d_v},i=\{1,2,\dots,h\}\)</span> and <span
class="math inline">\(W^O \in \mathbb{R}^{hd_v \times
d\_model}\)</span>。 可以看到，这里将Q,K,V通过h种投影矩阵<span
class="math inline">\((W_i^Q,W^K_i,W^V_i)\)</span>投影到了不同的子空间，从而在不同的子空间上进行关注。</p>
<p>多头注意力使模型能够同时关注不同位置的不同表示子空间中的信息。使用单个注意力头，平均操作会抑制这种能力。通过引入多个注意力头，模型可以并行地学习多个不同的查询、键和值的投影，从而在不同的表示子空间上进行关注，避免了信息的平均化。这样可以增强模型对输入的表征能力，提高模型处理复杂关系和学习长距离依赖的能力。通过多头注意力，模型可以捕捉更丰富的上下文信息，并且能够更好地处理输入中的多样性和复杂性。</p>
<h3 id="position-wise-feed-forward-networksmlp">Position-wise
Feed-Forward Networks(MLP)</h3>
<h4 id="基本形式">基本形式</h4>
<p><span class="math display">\[FFN(x)=max(0,xW_1+b_1)W_2+b_2\]</span>
实质：将来自Multi-Attention的输入交给一个线性层<span
class="math inline">\(Linear_1(x)=xW_1 +
b_1\)</span>,然后交给激活函数ReLU，最后在做线性变换<span
class="math inline">\(Linear_2(x)=xW_2+b_2\)</span></p>
<h4 id="position-encoding">Position Encoding</h4>
<p>RNN是逐步处理embedding的过程，是将t时刻的输出作为t时刻的输入的一部分考量。而Transformer则不同，他是在一开始就利用注意力机制考量不同embedding
vectors之间的相关度，所以在输入时位置编码显得比较重要，它会提供RNN中“逐步处理”这样的时序关系，来告诉模型每个embedding的位置(position)。</p>
<ul>
<li>论文中的位置编码： <span class="math display">\[
\begin{split}
  PE_{(pos,2i)} &amp;= sin(pos/10000^{2i/d\_model})\\
  PE_{(pos,2i+1)} &amp;= cos(pos/10000^{2i/d\_model})
\end{split}
\]</span></li>
</ul>
<h1 id="从代码出发">从代码出发</h1>
<p>前面的章节，已经介绍了Transformer的主要框架结构，包含了(自)注意力机制、（掩码）多头注意力机制、MLP以及位置编码和残差连接，但是若没有接触过自回归结构的模型，其实仍然很难明白，当一个embedding进到Encoder之后的具体流程如何？当然可以参考这个<a
target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV15v411W78M/?spm_id_from=333.999.0.0&amp;vd_source=03a1d15d9e2ed541e2d422d799de6c41">教程</a>，介绍了Attention的张量计算过程，并举实例将训练流程讲清楚了。</p>
<figure>
<img data-src="/images/Transformer/p10.png" alt="实例" />
<figcaption aria-hidden="true">实例</figcaption>
</figure>
<p>其中，&lt;bos&gt; 和 &lt;eos&gt;分别表示beginning of sentence和end of
sentence。但即便这样，依旧会错过很多细枝末节，不妨来看看代码实现。</p>
<h2 id="机翻代码示例">机翻代码示例</h2>
<p>代码链接：<a
target="_blank" rel="noopener" href="https://github.com/wmathor/nlp-tutorial">github-code</a></p>
<p>代码框架：<img data-src="/images/Transformer/p11.png"
alt="code-struct" /></p>
<h2 id="transformer">Transformer</h2>
<p>Transfomer可以抽象为3部分——Encoders,Decoders和(Linear+Softmax)。其学习过程可以由function
“forward”得出,可以看到在最初</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        self.encoder = Encoder().cuda()</span><br><span class="line">        self.decoder = Decoder().cuda()</span><br><span class="line">        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=<span class="literal">False</span>).cuda()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, dec_inputs</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># tensor to store decoder outputs</span></span><br><span class="line">        <span class="comment"># outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">        enc_outputs, enc_self_attns = self.encoder(enc_inputs)</span><br><span class="line">        <span class="comment"># dec_outpus: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]</span></span><br><span class="line">        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)</span><br><span class="line">        dec_logits = self.projection(dec_outputs) <span class="comment"># dec_logits: [batch_size, tgt_len, tgt_vocab_size]</span></span><br><span class="line">        <span class="keyword">return</span> dec_logits.view(-<span class="number">1</span>, dec_logits.size(-<span class="number">1</span>)), enc_self_attns, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure>
<h2 id="encoder">Encoder</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.src_emb = nn.Embedding(src_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs) <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        enc_outputs = self.pos_emb(enc_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) <span class="comment"># [batch_size, src_len, src_len]</span></span><br><span class="line">        enc_self_attns = []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)</span><br><span class="line">            enc_self_attns.append(enc_self_attn)</span><br><span class="line">        <span class="keyword">return</span> enc_outputs, enc_self_attns</span><br></pre></td></tr></table></figure>
<h2 id="decoder">Decoder</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        enc_intpus: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        enc_outputs: [batsh_size, src_len, d_model]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs) <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_outputs = self.pos_emb(dec_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>).cuda() <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda() <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda() <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), <span class="number">0</span>).cuda() <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"></span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) <span class="comment"># [batc_size, tgt_len, src_len]</span></span><br><span class="line"></span><br><span class="line">        dec_self_attns, dec_enc_attns = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span></span><br><span class="line">            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)</span><br><span class="line">            dec_self_attns.append(dec_self_attn)</span><br><span class="line">            dec_enc_attns.append(dec_enc_attn)</span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure>
<h2 id="positional-encoding">Positional Encoding</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        x: [seq_len, batch_size, d_model]</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<p>当然，位置编码这里的自由度比较高，很多论文也是引入了新的位置编码改善性能。</p>
<h1 id="vit">ViT</h1>
<p>从<a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929.pdf">paper</a>的标题--TRANSFORMERS
FOR IMAGE RECOGNITION AT
SCALE，我们就清楚ViT仍然更适用于大数据集的视觉任务。当然，如果使用预训练模型，对下游任务进行参数微调，性能也会很好。</p>
<h2 id="cnn-transformer">CNN &amp; Transformer</h2>
<ul>
<li>当在数据规模相近的不带强制性正则的中小型数据集上(比如，ImageNet)采用ViT和ResNet,那么ViT的性能会劣于ResNet,主要原因：ViT缺乏CNN内在固有的Inductive
bias(简单理解为合理的先验知识)——translation
equivariance和locality。</li>
<li>CNN利用局部性原理挖掘高级特征，Transformer关注的是全局，通过多头实现基于全局的关联度的特征挖掘。</li>
</ul>
<h2 id="overview-of-vit">Overview of ViT</h2>
<figure>
<img data-src="/images/Transformer/ViT-model.png" alt="ViT Overview" />
<figcaption aria-hidden="true">ViT Overview</figcaption>
</figure>
<p>可以看到其结构相对于Transfomer的架构相对简单，原来的encoder-decoder结构将decoder抽象为了简单的MLP(或者其他的下游任务的头结构)，且加入了可以学习的<code>classfication token</code>
(简称<code>cls token</code>)，当然也可以不加，但是需要在最后进行全局平均池化。</p>
<h2 id="method">Method</h2>
<ul>
<li>input:图像<span class="math inline">\({x} \in \mathbb{R}^{H \times W
\times C}\)</span></li>
<li>设定patch_size=P,将多通道图像打成2D,此时图像变为<span
class="math inline">\({x}_p \in \mathbb{R}^{N \times (P^2 \cdot C)
}\)</span>,其中<span
class="math inline">\(P^2\)</span>是每个patch的分辨率(图像大小)，C为通道数，则每个patch
token的大小为<span class="math inline">\(P^2 \cdot
C\)</span>，N为每个通道切割出的patch数，即<span
class="math inline">\(N=\frac{HW}{P^2}\)</span>,然后将多个patch
token打包成一个batch</li>
<li>假定batch size为N,则单个embedded batch可以描述为<span
class="math inline">\([x_p^1, x_p^2, \dotsb,
x_p^N]\)</span>,将其进行线性投影，投影矩阵为<span
class="math inline">\(E\)</span>，即作<span
class="math inline">\([x_p^1\textbf{E}, x_p^2\textbf{E}, \dotsb,
x_p^N\textbf{E}]\)</span></li>
<li>concat拼接<code>cls token</code>,<span
class="math inline">\([x_{class}, x_p^1\textbf{E}, x_p^2\textbf{E},
\dotsb, x_p^N\textbf{E}]\)</span></li>
<li>加上位置编码<span class="math inline">\(E_{pos}\)</span></li>
</ul>
<h1 id="code-of-vitrgb">Code of ViTRGB</h1>
<ol type="1">
<li><p>导包和shape定型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, repeat</span><br><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pair</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t <span class="keyword">if</span> <span class="built_in">isinstance</span>(t, <span class="built_in">tuple</span>) <span class="keyword">else</span> (t, t)</span><br></pre></td></tr></table></figure></p></li>
<li><p>工具类</p></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PreNorm</span>(nn.Module):</span><br><span class="line"><span class="string">&quot;&quot;&quot; LayerNorm &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(dim)</span><br><span class="line">        self.fn = fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, **kwargs</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fn(self.norm(x), **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line"><span class="string">&quot;&quot;&quot; 引入dropout和GELU的FFN(MLP) &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(hidden_dim, dim),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line"><span class="string">&quot;&quot;&quot; 注意力机制 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># dim--输入特征x的维度，heads为头数，dim_head等于每个头的维度，dropout的引入防止过拟合</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads=<span class="number">8</span>, dim_head=<span class="number">64</span>, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        inner_dim = dim_head * heads</span><br><span class="line">        project_out = <span class="keyword">not</span> (heads == <span class="number">1</span> <span class="keyword">and</span> dim_head == dim)</span><br><span class="line"></span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = dim_head**-<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.attend = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.to_qkv = nn.Linear(dim, inner_dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.to_out = (nn.Sequential(nn.Linear(inner_dim, dim),</span><br><span class="line">                                     nn.Dropout(dropout))</span><br><span class="line">                       <span class="keyword">if</span> project_out <span class="keyword">else</span> nn.Identity())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x.shape:[batch_size, seq_length, dim]</span></span><br><span class="line">        <span class="comment"># 输入特征x映射为q，k，v</span></span><br><span class="line">        <span class="comment"># to_qkv(x)之后的shape:[batch_size, seq_length, inner_dim x 3]</span></span><br><span class="line">        <span class="comment"># chunk均分之后shape:[batch_size, seq_length, inner_dim]</span></span><br><span class="line">        qkv = self.to_qkv(x).chunk(<span class="number">3</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># rearrange重塑输入的张量结构，将lamnda表达式的输入张量t(这里是可迭代对象qkv)进行shape reorder</span></span><br><span class="line">        <span class="comment"># 通过map的迭代reorder之后，q,k,v的shape是由[batch_size, seq_length, inner_dim]变为[batch_size, heads, seq_length, dim_head]</span></span><br><span class="line">        q, k, v = <span class="built_in">map</span>(</span><br><span class="line">            <span class="keyword">lambda</span> t: rearrange(t, <span class="string">&quot;b n (h d) -&gt; b h n d&quot;</span>, h=self.heads), qkv)</span><br><span class="line">        <span class="comment"># 标准化注意力矩阵，shape:[batch_size, heads, seq_length, seq_length]</span></span><br><span class="line">        dots = torch.matmul(q, k.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) * self.scale</span><br><span class="line">        <span class="comment"># 取得注意力机制中相关性系数</span></span><br><span class="line">        attn = self.attend(dots)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [batch_size, heads, seq_length, dim_head]</span></span><br><span class="line">        out = torch.matmul(attn, v)</span><br><span class="line">        <span class="comment"># [batch_size, seq_length, dim]</span></span><br><span class="line">        out = rearrange(out, <span class="string">&quot;b h n d -&gt; b n (h d)&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.to_out(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="comment"># dim--embedding/feature,depth--depth of encoder stack</span></span><br><span class="line">    <span class="comment"># dim_head--dimension of each head, mlp_dim--dimension of hidden layer</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, heads, dim_head, mlp_dim, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layers = nn.ModuleList([])</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(depth):</span><br><span class="line">            self.layers.append(</span><br><span class="line">                nn.ModuleList([</span><br><span class="line">                    PreNorm(</span><br><span class="line">                        dim,</span><br><span class="line">                        Attention(dim,</span><br><span class="line">                                  heads=heads,</span><br><span class="line">                                  dim_head=dim_head,</span><br><span class="line">                                  dropout=dropout),</span><br><span class="line">                    ),</span><br><span class="line">                    PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout)),</span><br><span class="line">                ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> attn, ff <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># 残差结构</span></span><br><span class="line">            x = attn(x) + x</span><br><span class="line">            x = ff(x) + x</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>ViTRGB</li>
</ol>
<p>这里，position embedding为随机初始化的张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ViTRGB</span>(nn.Module):</span><br><span class="line"><span class="string">&quot;&quot;&quot; Vision Transformer &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        *,</span></span><br><span class="line"><span class="params">        image_size,</span></span><br><span class="line"><span class="params">        patch_size,</span></span><br><span class="line"><span class="params">        num_classes,</span></span><br><span class="line"><span class="params">        dim,  <span class="comment"># 特征维度/token size</span></span></span><br><span class="line"><span class="params">        depth,  <span class="comment"># depth of encoder stack</span></span></span><br><span class="line"><span class="params">        heads,</span></span><br><span class="line"><span class="params">        mlp_dim,</span></span><br><span class="line"><span class="params">        pool=<span class="string">&quot;cls&quot;</span>,  <span class="comment"># option:(&#x27;cls&#x27;, &#x27;mean&#x27;),池化类型:&#x27;cls&#x27;表示选择classification token, &#x27;mean&#x27;表示选择全局平均池化 </span></span></span><br><span class="line"><span class="params">        channels=<span class="number">3</span>,  <span class="comment"># 图像通道数，默认RGB-3</span></span></span><br><span class="line"><span class="params">        dim_head=<span class="number">64</span>,  <span class="comment"># dimension of each head</span></span></span><br><span class="line"><span class="params">        dropout=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        emb_dropout=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        pixelwise=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 1D --&gt; 2D shape</span></span><br><span class="line">        image_height, image_width = pair(image_size)</span><br><span class="line">        patch_height, patch_width = pair(patch_size)</span><br><span class="line"></span><br><span class="line">        self.pixelwise = pixelwise</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 裁剪为同样大小的patches</span></span><br><span class="line">        self.image_height = image_height</span><br><span class="line">        self.image_width = image_width</span><br><span class="line">        self.patch_height = patch_height</span><br><span class="line">        self.patch_width = patch_width</span><br><span class="line">        self.num_patches_height = image_height // patch_height</span><br><span class="line">        self.num_patches_width = image_width // patch_width</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> (image_height % patch_height == <span class="number">0</span> <span class="keyword">and</span> image_width % patch_width</span><br><span class="line">                == <span class="number">0</span>), <span class="string">&quot;Image dimensions must be divisible by the patch size.&quot;</span></span><br><span class="line"></span><br><span class="line">        num_patches = (image_height // patch_height) * (image_width //</span><br><span class="line">                                                        patch_width)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># patch_dim 其实就是变为token之前的维度，之后经过MLP,将patch_dim变成token size(即下面dim)</span></span><br><span class="line">        patch_dim = channels * patch_height * patch_width</span><br><span class="line">        <span class="keyword">assert</span> pool <span class="keyword">in</span> &#123;</span><br><span class="line">            <span class="string">&quot;cls&quot;</span>,</span><br><span class="line">            <span class="string">&quot;mean&quot;</span>,</span><br><span class="line">        &#125;, <span class="string">&quot;pool type must be either cls (cls token) or mean (mean pooling)&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># reorder the tensor corresponding to the shape of image</span></span><br><span class="line">        <span class="comment"># image shape:[batch_size, channels, height, width]</span></span><br><span class="line">        <span class="comment"># --&gt; image shape:[batch_size, channels, h x patch_height, w x patch_width]</span></span><br><span class="line">        <span class="comment"># --&gt; iamge shape:[batch_size, h x w, patch_height x patch_width x channels]</span></span><br><span class="line">        self.to_patch_embedding = nn.Sequential(</span><br><span class="line">            Rearrange(</span><br><span class="line">                <span class="string">&quot;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&quot;</span>,</span><br><span class="line">                p1=patch_height,</span><br><span class="line">                p2=patch_width,</span><br><span class="line">            ),</span><br><span class="line">            nn.LayerNorm(patch_dim),</span><br><span class="line">            nn.Linear(patch_dim, dim),</span><br><span class="line">            nn.LayerNorm(dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 注意这里是num_patches + 1，所以考虑到cls token的位置编码</span></span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))</span><br><span class="line">        self.cls_token = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">1</span>, dim))</span><br><span class="line">        self.dropout = nn.Dropout(emb_dropout)</span><br><span class="line"></span><br><span class="line">        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim,</span><br><span class="line">                                       dropout)</span><br><span class="line"></span><br><span class="line">        self.pool = pool</span><br><span class="line">        self.to_latent = nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.mlp_head = nn.Sequential(nn.LayerNorm(dim),</span><br><span class="line">                                      nn.Linear(dim, num_classes))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 像素级</span></span><br><span class="line">        <span class="keyword">if</span> self.pixelwise:</span><br><span class="line">            self.mlp_head = nn.Sequential(</span><br><span class="line">                nn.LayerNorm(dim),</span><br><span class="line">                nn.Linear(dim, num_classes * patch_height * patch_width),</span><br><span class="line">                Rearrange(</span><br><span class="line">                    <span class="string">&quot;b h w (p1 p2 num_classes) -&gt; b (h p1) (w p2) num_classes&quot;</span>,</span><br><span class="line">                    p1=patch_height,</span><br><span class="line">                    p2=patch_width,</span><br><span class="line">                    num_classes=num_classes,</span><br><span class="line">                ),</span><br><span class="line">                <span class="comment"># reorder shape,交换dim=1和最后一维</span></span><br><span class="line">                MoveAxis((-<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;patch_height=&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;patch_width=&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;num_classes=&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="comment"># patch embedding, x.shape:[batch_size, h x w, patch_height x patch_width x channels]</span></span><br><span class="line">        x = self.to_patch_embedding(img)</span><br><span class="line">        <span class="comment"># b:batch size, n: number of samples(n = h x w)</span></span><br><span class="line">        b, n, _ = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个batch需要补上cls token，所以复制b份</span></span><br><span class="line">        cls_tokens = repeat(self.cls_token, <span class="string">&quot;1 1 d -&gt; b 1 d&quot;</span>, b=b)</span><br><span class="line">        <span class="comment"># x.shape after concat:[batch_size, h x w + 1, patch_height x patch_width x channels]</span></span><br><span class="line">        <span class="comment"># 注意这里和x = torch.cat((x,cls_token), dim=1)有区别，谁在前，谁就在低维</span></span><br><span class="line">        x = torch.cat(</span><br><span class="line">            (cls_tokens, x), dim=<span class="number">1</span></span><br><span class="line">        )  <span class="comment"># cls_token在前，所以cls token在[:,0]的位置，patch embedding的位置是[:,1:]</span></span><br><span class="line">        <span class="comment"># 加上posistion embedding</span></span><br><span class="line">        x += self.pos_embedding[:, :(n + <span class="number">1</span>)]</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># x抛进Transfomer</span></span><br><span class="line">        x = self.transformer(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># x = x.mean(dim = 1) if self.pool == &#x27;mean&#x27; else x[:, 0]</span></span><br><span class="line">        x = x[:, <span class="number">1</span>:, :]  <span class="comment"># remove cls..</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 去除cls token之后的shape:[batch_size, h x w, patch_height x patch_width x channels]</span></span><br><span class="line">        x = self.to_latent(x)</span><br><span class="line">        x = rearrange(</span><br><span class="line">            x,</span><br><span class="line">            <span class="string">&quot;b (h w) d -&gt; b h w d&quot;</span>,</span><br><span class="line">            h=self.num_patches_height,</span><br><span class="line">            w=self.num_patches_width,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> self.mlp_head(x)</span><br></pre></td></tr></table></figure>
<h1 id="总结计算机视觉角度">总结(计算机视觉角度)</h1>
<ol type="1">
<li>CNN引入的inductive
bias使得他们在处理局部特征时更加高效，但是无法捕获长程(long-range)的依赖关系；</li>
<li>从这两点来看，其实CNN和attention机制各有优劣势。CNN因为可以捕捉感受野内的特征，但是无法跳脱出这样的局限，使得他无法捕获长程依赖，即便是在引入池化操作，那也只是在临近的感受野范围内进行特征融合，说明这种能力很weak；注意力机制，其实就是在每个patch上进行“特征提取”，这点同卷积操作的目的是一致的。</li>
<li>attention确实可以捕获长程以来，但是它需要将整幅图分成多个patch进行embedding，然后进行多头注意力机制，这需要消耗大量的内存，这就是缺点之一；</li>
<li>注意力机制常常在pretext task中出现，所以它是一个large
scale模型。</li>
</ol>

    </div>

    
    
    
    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------已经到底啦！<i class="fa fa-paw"></i>-------------</div>
    
</div>

      
    </div>



    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Model/" rel="tag"># Model</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/06/24/Combinatorial-Mathematics/" rel="prev" title="Combinatorics">
                  <i class="fa fa-chevron-left"></i> Combinatorics
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/07/13/Masked-Autoencoder/" rel="next" title="Masked Autoencoder">
                  Masked Autoencoder <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Keith Malarkey</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">21k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:18</span>
  </span>
</div>
<div class="busuanzi-count">
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

<!-- <br /> -->
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<!-- <span id="times">载入时分秒...</span> -->
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("11/17/2022 8:00:00");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); 
        if(String(snum).length ==1 ){snum = "0" + snum;}
        // var times = document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "+hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/pace.js"></script>


  
  <script async src="/js/busuanzi.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-cn","enable":true,"serverURL":"waline4test-af5tulsko-keithmalarkey.vercel.app","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":false,"placeholder":"请文明评论鸭~","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"visitor":true,"comment_count":true,"requiredFields":["nick","mail"],"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","el":"#waline","comment":true,"path":"/2023/07/06/Transformer/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>


  
   <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
   <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
   <script type="text/javascript" src="/js/fireworks.js"></script>
  

  
  <script type="text/javascript"
    count="150"
    opacity: 1
    src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
  </script>
  
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right"},"mobile":{"show":true}});</script></body>


<a target="_blank" rel="noopener" href="https://github.com/KeithMalarkey" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
</html>

